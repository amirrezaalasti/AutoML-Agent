[
  {
    "title": "Multiprocessing best practices — PyTorch 2.7 documentation",
    "content": "Multiprocessing best practices\n¶\ntorch.multiprocessing\nis a drop in replacement for Python’s\nmultiprocessing\nmodule. It supports the exact same operations,\nbut extends it, so that all tensors sent through a\nmultiprocessing.Queue\n, will have their data moved into shared\nmemory and will only send a handle to another process.\nNote\nWhen a\nTensor\nis sent to another process, the\nTensor\ndata is shared. If\ntorch.Tensor.grad\nis\nnot\nNone\n, it is also shared. After a\nTensor\nwithout\na\ntorch.Tensor.grad\nfield is sent to the other process, it\ncreates a standard process-specific\n.grad\nTensor\nthat\nis not automatically shared across all processes, unlike how the\nTensor\n’s data has been shared.\nThis allows to implement various training methods, like Hogwild, A3C, or any\nothers that require asynchronous operation.\nCUDA in multiprocessing\n¶\nThe CUDA runtime does not support the\nfork\nstart method; either the\nspawn\nor\nforkserver\nstart method are\nrequired to use CUDA in subprocesses.\nNote\nThe start method can be set via either creating a context with\nmultiprocessing.get_context(...)\nor directly using\nmultiprocessing.set_start_method(...)\n.\nUnlike CPU tensors, the sending process is required to keep the original tensor\nas long as the receiving process retains a copy of the tensor. It is implemented\nunder the hood but requires users to follow the best practices for the program\nto run correctly. For example, the sending process must stay alive as long as\nthe consumer process has references to the tensor, and the refcounting can not\nsave you if the consumer process exits abnormally via a fatal signal. See\nthis section\n.\nSee also:\nUse nn.parallel.DistributedDataParallel instead of multiprocessing or nn.DataParallel\nBest practices and tips\n¶\nAvoiding and fighting deadlocks\n¶\nThere are a lot of things that can go wrong when a new process is spawned, with\nthe most common cause of deadlocks being background threads. If there’s any\nthread that holds a lock or imports a module, and\nfork\nis called, it’s very\nlikely that the subprocess will be in a corrupted state and will deadlock or\nfail in a different way. Note that even if you don’t, Python built in\nlibraries do - no need to look further than\nmultiprocessing\n.\nmultiprocessing.Queue\nis actually a very complex class, that\nspawns multiple threads used to serialize, send and receive objects, and they\ncan cause aforementioned problems too. If you find yourself in such situation\ntry using a\nSimpleQueue\n, that doesn’t\nuse any additional threads.\nWe’re trying our best to make it easy for you and ensure these deadlocks don’t\nhappen but some things are out of our control. If you have any issues you can’t\ncope with for a while, try reaching out on forums, and we’ll see if it’s an\nissue we can fix.\nReuse buffers passed through a Queue\n¶\nRemember that each time you put a\nTensor\ninto a\nmultiprocessing.Queue\n, it has to be moved into shared memory.\nIf it’s already shared, it is a no-op, otherwise it will incur an additional\nmemory copy that can slow down the whole process. Even if you have a pool of\nprocesses sending data to a single one, make it send the buffers back - this\nis nearly free and will let you avoid a copy when sending next batch.\nAsynchronous multiprocess training (e.g. Hogwild)\n¶\nUsing\ntorch.multiprocessing\n, it is possible to train a model\nasynchronously, with parameters either shared all the time, or being\nperiodically synchronized. In the first case, we recommend sending over the whole\nmodel object, while in the latter, we advise to only send the\nstate_dict()\n.\nWe recommend using\nmultiprocessing.Queue\nfor passing all kinds\nof PyTorch objects between processes. It is possible to e.g. inherit the tensors\nand storages already in shared memory, when using the\nfork\nstart method,\nhowever it is very bug prone and should be used with care, and only by advanced\nusers. Queues, even though they’re sometimes a less elegant solution, will work\nproperly in all cases.\nWarning\nYou should be careful about having global statements, that are not guarded\nwith an\nif\n__name__\n==\n'__main__'\n. If a different start method than\nfork\nis used, they will be executed in all subprocesses.\nHogwild\n¶\nA concrete Hogwild implementation can be found in the\nexamples repository\n,\nbut to showcase the overall structure of the code, there’s also a minimal\nexample below as well:\nimport\ntorch.multiprocessing\nas\nmp\nfrom\nmodel\nimport\nMyModel\ndef\ntrain\n(\nmodel\n):\n# Construct data_loader, optimizer, etc.\nfor\ndata\n,\nlabels\nin\ndata_loader\n:\noptimizer\n.\nzero_grad\n()\nloss_fn\n(\nmodel\n(\ndata\n),\nlabels\n)\n.\nbackward\n()\noptimizer\n.\nstep\n()\n# This will update the shared parameters\nif\n__name__\n==\n'__main__'\n:\nnum_processes\n=\n4\nmodel\n=\nMyModel\n()\n# NOTE: this is required for the ``fork`` method to work\nmodel\n.\nshare_memory\n()\nprocesses\n=\n[]\nfor\nrank\nin\nrange\n(\nnum_processes\n):\np\n=\nmp\n.\nProcess\n(\ntarget\n=\ntrain\n,\nargs\n=\n(\nmodel\n,))\np\n.\nstart\n()\nprocesses\n.\nappend\n(\np\n)\nfor\np\nin\nprocesses\n:\np\n.\njoin\n()\nCPU in multiprocessing\n¶\nInappropriate multiprocessing can lead to CPU oversubscription, causing\ndifferent processes to compete for CPU resources, resulting in low\nefficiency.\nThis tutorial will explain what CPU oversubscription is and how to\navoid it.\nCPU oversubscription\n¶\nCPU oversubscription is a technical term that refers to a situation\nwhere the total number of vCPUs allocated to a system exceeds the total\nnumber of vCPUs available on the hardware.\nThis leads to severe contention for CPU resources. In such cases, there\nis frequent switching between processes, which increases processes\nswitching overhead and decreases overall system efficiency.\nSee CPU oversubscription with the code examples in the Hogwild\nimplementation found in the\nexample\nrepository\n.\nWhen running the training example with the following command on CPU\nusing 4 processes:\npython\nmain.py\n--num-processes\n4\nAssuming there are N vCPUs available on the machine, executing the above\ncommand will generate 4 subprocesses. Each subprocess will allocate N\nvCPUs for itself, resulting in a requirement of 4*N vCPUs. However, the\nmachine only has N vCPUs available. Consequently, the different\nprocesses will compete for resources, leading to frequent process\nswitching.\nThe following observations indicate the presence of CPU over\nsubscription:\nHigh CPU Utilization: By using the\nhtop\ncommand, you can observe\nthat the CPU utilization is consistently high, often reaching or\nexceeding its maximum capacity. This indicates that the demand for\nCPU resources exceeds the available physical cores, causing\ncontention and competition among processes for CPU time.\nFrequent Context Switching with Low System Efficiency: In an\noversubscribed CPU scenario, processes compete for CPU time, and the\noperating system needs to rapidly switch between different processes\nto allocate resources fairly. This frequent context switching adds\noverhead and reduces the overall system efficiency.\nAvoid CPU oversubscription\n¶\nA good way to avoid CPU oversubscription is proper resource allocation.\nEnsure that the number of processes or threads running concurrently does\nnot exceed the available CPU resources.\nIn this case, a solution would be to specify the appropriate number of\nthreads in the subprocesses. This can be achieved by setting the number\nof threads for each process using the\ntorch.set_num_threads(int)\nfunction in subprocess.\nAssuming there are N vCPUs on the machine and M processes will be\ngenerated, the maximum\nnum_threads\nvalue used by each process would\nbe\nfloor(N/M)\n. To avoid CPU oversubscription in the mnist_hogwild\nexample, the following changes are needed for the file\ntrain.py\nin\nexample\nrepository\n.\ndef\ntrain\n(\nrank\n,\nargs\n,\nmodel\n,\ndevice\n,\ndataset\n,\ndataloader_kwargs\n):\ntorch\n.\nmanual_seed\n(\nargs\n.\nseed\n+\nrank\n)\n#### define the num threads used in current sub-processes\ntorch\n.\nset_num_threads\n(\nfloor\n(\nN\n/\nM\n))\ntrain_loader\n=\ntorch\n.\nutils\n.\ndata\n.\nDataLoader\n(\ndataset\n,\n**\ndataloader_kwargs\n)\noptimizer\n=\noptim\n.\nSGD\n(\nmodel\n.\nparameters\n(),\nlr\n=\nargs\n.\nlr\n,\nmomentum\n=\nargs\n.\nmomentum\n)\nfor\nepoch\nin\nrange\n(\n1\n,\nargs\n.\nepochs\n+\n1\n):\ntrain_epoch\n(\nepoch\n,\nargs\n,\nmodel\n,\ndevice\n,\ntrain_loader\n,\noptimizer\n)\nSet\nnum_thread\nfor each process using\ntorch.set_num_threads(floor(N/M))\n. where you replace N with the\nnumber of vCPUs available and M with the chosen number of processes. The\nappropriate\nnum_thread\nvalue will vary depending on the specific\ntask at hand. However, as a general guideline, the maximum value for the\nnum_thread\nshould be\nfloor(N/M)\nto avoid CPU oversubscription.\nIn the\nmnist_hogwild\ntraining example, after avoiding CPU over\nsubscription, you can achieve a 30x performance boost.",
    "url": "https://pytorch.org/docs/stable/notes/multiprocessing.html"
  },
  {
    "title": "Automatic Mixed Precision examples — PyTorch 2.7 documentation",
    "content": "Automatic Mixed Precision examples\n¶\nOrdinarily, “automatic mixed precision training” means training with\ntorch.autocast\nand\ntorch.amp.GradScaler\ntogether.\nInstances of\ntorch.autocast\nenable autocasting for chosen regions.\nAutocasting automatically chooses the precision for operations to improve performance\nwhile maintaining accuracy.\nInstances of\ntorch.amp.GradScaler\nhelp perform the steps of\ngradient scaling conveniently.  Gradient scaling improves convergence for networks with\nfloat16\n(by default on CUDA and XPU)\ngradients by minimizing gradient underflow, as explained\nhere\n.\ntorch.autocast\nand\ntorch.amp.GradScaler\nare modular.\nIn the samples below, each is used as its individual documentation suggests.\n(Samples here are illustrative.  See the\nAutomatic Mixed Precision recipe\nfor a runnable walkthrough.)\nTypical Mixed Precision Training\nWorking with Unscaled Gradients\nGradient clipping\nWorking with Scaled Gradients\nGradient accumulation\nGradient penalty\nWorking with Multiple Models, Losses, and Optimizers\nWorking with Multiple GPUs\nDataParallel in a single process\nDistributedDataParallel, one GPU per process\nDistributedDataParallel, multiple GPUs per process\nAutocast and Custom Autograd Functions\nFunctions with multiple inputs or autocastable ops\nFunctions that need a particular\ndtype\nTypical Mixed Precision Training\n¶\n# Creates model and optimizer in default precision\nmodel\n=\nNet\n()\n.\ncuda\n()\noptimizer\n=\noptim\n.\nSGD\n(\nmodel\n.\nparameters\n(),\n...\n)\n# Creates a GradScaler once at the beginning of training.\nscaler\n=\nGradScaler\n()\nfor\nepoch\nin\nepochs\n:\nfor\ninput\n,\ntarget\nin\ndata\n:\noptimizer\n.\nzero_grad\n()\n# Runs the forward pass with autocasting.\nwith\nautocast\n(\ndevice_type\n=\n'cuda'\n,\ndtype\n=\ntorch\n.\nfloat16\n):\noutput\n=\nmodel\n(\ninput\n)\nloss\n=\nloss_fn\n(\noutput\n,\ntarget\n)\n# Scales loss.  Calls backward() on scaled loss to create scaled gradients.\n# Backward passes under autocast are not recommended.\n# Backward ops run in the same dtype autocast chose for corresponding forward ops.\nscaler\n.\nscale\n(\nloss\n)\n.\nbackward\n()\n# scaler.step() first unscales the gradients of the optimizer's assigned params.\n# If these gradients do not contain infs or NaNs, optimizer.step() is then called,\n# otherwise, optimizer.step() is skipped.\nscaler\n.\nstep\n(\noptimizer\n)\n# Updates the scale for next iteration.\nscaler\n.\nupdate\n()\nWorking with Unscaled Gradients\n¶\nAll gradients produced by\nscaler.scale(loss).backward()\nare scaled.  If you wish to modify or inspect\nthe parameters’\n.grad\nattributes between\nbackward()\nand\nscaler.step(optimizer)\n,  you should\nunscale them first.  For example, gradient clipping manipulates a set of gradients such that their global norm\n(see\ntorch.nn.utils.clip_grad_norm_()\n) or maximum magnitude (see\ntorch.nn.utils.clip_grad_value_()\n)\nis\n<\n=\n<=\n<=\nsome user-imposed threshold.  If you attempted to clip\nwithout\nunscaling, the gradients’ norm/maximum\nmagnitude would also be scaled, so your requested threshold (which was meant to be the threshold for\nunscaled\ngradients) would be invalid.\nscaler.unscale_(optimizer)\nunscales gradients held by\noptimizer\n’s assigned parameters.\nIf your model or models contain other parameters that were assigned to another optimizer\n(say\noptimizer2\n), you may call\nscaler.unscale_(optimizer2)\nseparately to unscale those\nparameters’ gradients as well.\nGradient clipping\n¶\nCalling\nscaler.unscale_(optimizer)\nbefore clipping enables you to clip unscaled gradients as usual:\nscaler\n=\nGradScaler\n()\nfor\nepoch\nin\nepochs\n:\nfor\ninput\n,\ntarget\nin\ndata\n:\noptimizer\n.\nzero_grad\n()\nwith\nautocast\n(\ndevice_type\n=\n'cuda'\n,\ndtype\n=\ntorch\n.\nfloat16\n):\noutput\n=\nmodel\n(\ninput\n)\nloss\n=\nloss_fn\n(\noutput\n,\ntarget\n)\nscaler\n.\nscale\n(\nloss\n)\n.\nbackward\n()\n# Unscales the gradients of optimizer's assigned params in-place\nscaler\n.\nunscale_\n(\noptimizer\n)\n# Since the gradients of optimizer's assigned params are unscaled, clips as usual:\ntorch\n.\nnn\n.\nutils\n.\nclip_grad_norm_\n(\nmodel\n.\nparameters\n(),\nmax_norm\n)\n# optimizer's gradients are already unscaled, so scaler.step does not unscale them,\n# although it still skips optimizer.step() if the gradients contain infs or NaNs.\nscaler\n.\nstep\n(\noptimizer\n)\n# Updates the scale for next iteration.\nscaler\n.\nupdate\n()\nscaler\nrecords that\nscaler.unscale_(optimizer)\nwas already called for this optimizer\nthis iteration, so\nscaler.step(optimizer)\nknows not to redundantly unscale gradients before\n(internally) calling\noptimizer.step()\n.\nWarning\nunscale_\nshould only be called once per optimizer per\nstep\ncall,\nand only after all gradients for that optimizer’s assigned parameters have been accumulated.\nCalling\nunscale_\ntwice for a given optimizer between each\nstep\ntriggers a RuntimeError.\nWorking with Scaled Gradients\n¶\nGradient accumulation\n¶\nGradient accumulation adds gradients over an effective batch of size\nbatch_per_iter\n*\niters_to_accumulate\n(\n*\nnum_procs\nif distributed).  The scale should be calibrated for the effective batch, which means inf/NaN checking,\nstep skipping if inf/NaN grads are found, and scale updates should occur at effective-batch granularity.\nAlso, grads should remain scaled, and the scale factor should remain constant, while grads for a given effective\nbatch are accumulated.  If grads are unscaled (or the scale factor changes) before accumulation is complete,\nthe next backward pass will add scaled grads to unscaled grads (or grads scaled by a different factor)\nafter which it’s impossible to recover the accumulated unscaled grads\nstep\nmust apply.\nTherefore, if you want to\nunscale_\ngrads (e.g., to allow clipping unscaled grads),\ncall\nunscale_\njust before\nstep\n, after all (scaled) grads for the upcoming\nstep\nhave been accumulated.  Also, only call\nupdate\nat the end of iterations\nwhere you called\nstep\nfor a full effective batch:\nscaler\n=\nGradScaler\n()\nfor\nepoch\nin\nepochs\n:\nfor\ni\n,\n(\ninput\n,\ntarget\n)\nin\nenumerate\n(\ndata\n):\nwith\nautocast\n(\ndevice_type\n=\n'cuda'\n,\ndtype\n=\ntorch\n.\nfloat16\n):\noutput\n=\nmodel\n(\ninput\n)\nloss\n=\nloss_fn\n(\noutput\n,\ntarget\n)\nloss\n=\nloss\n/\niters_to_accumulate\n# Accumulates scaled gradients.\nscaler\n.\nscale\n(\nloss\n)\n.\nbackward\n()\nif\n(\ni\n+\n1\n)\n%\niters_to_accumulate\n==\n0\n:\n# may unscale_ here if desired (e.g., to allow clipping unscaled gradients)\nscaler\n.\nstep\n(\noptimizer\n)\nscaler\n.\nupdate\n()\noptimizer\n.\nzero_grad\n()\nGradient penalty\n¶\nA gradient penalty implementation commonly creates gradients using\ntorch.autograd.grad()\n, combines them to create the penalty value,\nand adds the penalty value to the loss.\nHere’s an ordinary example of an L2 penalty without gradient scaling or autocasting:\nfor\nepoch\nin\nepochs\n:\nfor\ninput\n,\ntarget\nin\ndata\n:\noptimizer\n.\nzero_grad\n()\noutput\n=\nmodel\n(\ninput\n)\nloss\n=\nloss_fn\n(\noutput\n,\ntarget\n)\n# Creates gradients\ngrad_params\n=\ntorch\n.\nautograd\n.\ngrad\n(\noutputs\n=\nloss\n,\ninputs\n=\nmodel\n.\nparameters\n(),\ncreate_graph\n=\nTrue\n)\n# Computes the penalty term and adds it to the loss\ngrad_norm\n=\n0\nfor\ngrad\nin\ngrad_params\n:\ngrad_norm\n+=\ngrad\n.\npow\n(\n2\n)\n.\nsum\n()\ngrad_norm\n=\ngrad_norm\n.\nsqrt\n()\nloss\n=\nloss\n+\ngrad_norm\nloss\n.\nbackward\n()\n# clip gradients here, if desired\noptimizer\n.\nstep\n()\nTo implement a gradient penalty\nwith\ngradient scaling, the\noutputs\nTensor(s)\npassed to\ntorch.autograd.grad()\nshould be scaled.  The resulting gradients\nwill therefore be scaled, and should be unscaled before being combined to create the\npenalty value.\nAlso, the penalty term computation is part of the forward pass, and therefore should be\ninside an\nautocast\ncontext.\nHere’s how that looks for the same L2 penalty:\nscaler\n=\nGradScaler\n()\nfor\nepoch\nin\nepochs\n:\nfor\ninput\n,\ntarget\nin\ndata\n:\noptimizer\n.\nzero_grad\n()\nwith\nautocast\n(\ndevice_type\n=\n'cuda'\n,\ndtype\n=\ntorch\n.\nfloat16\n):\noutput\n=\nmodel\n(\ninput\n)\nloss\n=\nloss_fn\n(\noutput\n,\ntarget\n)\n# Scales the loss for autograd.grad's backward pass, producing scaled_grad_params\nscaled_grad_params\n=\ntorch\n.\nautograd\n.\ngrad\n(\noutputs\n=\nscaler\n.\nscale\n(\nloss\n),\ninputs\n=\nmodel\n.\nparameters\n(),\ncreate_graph\n=\nTrue\n)\n# Creates unscaled grad_params before computing the penalty. scaled_grad_params are\n# not owned by any optimizer, so ordinary division is used instead of scaler.unscale_:\ninv_scale\n=\n1.\n/\nscaler\n.\nget_scale\n()\ngrad_params\n=\n[\np\n*\ninv_scale\nfor\np\nin\nscaled_grad_params\n]\n# Computes the penalty term and adds it to the loss\nwith\nautocast\n(\ndevice_type\n=\n'cuda'\n,\ndtype\n=\ntorch\n.\nfloat16\n):\ngrad_norm\n=\n0\nfor\ngrad\nin\ngrad_params\n:\ngrad_norm\n+=\ngrad\n.\npow\n(\n2\n)\n.\nsum\n()\ngrad_norm\n=\ngrad_norm\n.\nsqrt\n()\nloss\n=\nloss\n+\ngrad_norm\n# Applies scaling to the backward call as usual.\n# Accumulates leaf gradients that are correctly scaled.\nscaler\n.\nscale\n(\nloss\n)\n.\nbackward\n()\n# may unscale_ here if desired (e.g., to allow clipping unscaled gradients)\n# step() and update() proceed as usual.\nscaler\n.\nstep\n(\noptimizer\n)\nscaler\n.\nupdate\n()\nWorking with Multiple Models, Losses, and Optimizers\n¶\nIf your network has multiple losses, you must call\nscaler.scale\non each of them individually.\nIf your network has multiple optimizers, you may call\nscaler.unscale_\non any of them individually,\nand you must call\nscaler.step\non each of them individually.\nHowever,\nscaler.update\nshould only be called once,\nafter all optimizers used this iteration have been stepped:\nscaler\n=\ntorch\n.\namp\n.\nGradScaler\n()\nfor\nepoch\nin\nepochs\n:\nfor\ninput\n,\ntarget\nin\ndata\n:\noptimizer0\n.\nzero_grad\n()\noptimizer1\n.\nzero_grad\n()\nwith\nautocast\n(\ndevice_type\n=\n'cuda'\n,\ndtype\n=\ntorch\n.\nfloat16\n):\noutput0\n=\nmodel0\n(\ninput\n)\noutput1\n=\nmodel1\n(\ninput\n)\nloss0\n=\nloss_fn\n(\n2\n*\noutput0\n+\n3\n*\noutput1\n,\ntarget\n)\nloss1\n=\nloss_fn\n(\n3\n*\noutput0\n-\n5\n*\noutput1\n,\ntarget\n)\n# (retain_graph here is unrelated to amp, it's present because in this\n# example, both backward() calls share some sections of graph.)\nscaler\n.\nscale\n(\nloss0\n)\n.\nbackward\n(\nretain_graph\n=\nTrue\n)\nscaler\n.\nscale\n(\nloss1\n)\n.\nbackward\n()\n# You can choose which optimizers receive explicit unscaling, if you\n# want to inspect or modify the gradients of the params they own.\nscaler\n.\nunscale_\n(\noptimizer0\n)\nscaler\n.\nstep\n(\noptimizer0\n)\nscaler\n.\nstep\n(\noptimizer1\n)\nscaler\n.\nupdate\n()\nEach optimizer checks its gradients for infs/NaNs and makes an independent decision\nwhether or not to skip the step.  This may result in one optimizer skipping the step\nwhile the other one does not.  Since step skipping occurs rarely (every several hundred iterations)\nthis should not impede convergence.  If you observe poor convergence after adding gradient scaling\nto a multiple-optimizer model, please report a bug.\nWorking with Multiple GPUs\n¶\nThe issues described here only affect\nautocast\n.\nGradScaler\n‘s usage is unchanged.\nDataParallel in a single process\n¶\nEven if\ntorch.nn.DataParallel\nspawns threads to run the forward pass on each device.\nThe autocast state is propagated in each one and the following will work:\nmodel\n=\nMyModel\n()\ndp_model\n=\nnn\n.\nDataParallel\n(\nmodel\n)\n# Sets autocast in the main thread\nwith\nautocast\n(\ndevice_type\n=\n'cuda'\n,\ndtype\n=\ntorch\n.\nfloat16\n):\n# dp_model's internal threads will autocast.\noutput\n=\ndp_model\n(\ninput\n)\n# loss_fn also autocast\nloss\n=\nloss_fn\n(\noutput\n)\nDistributedDataParallel, one GPU per process\n¶\ntorch.nn.parallel.DistributedDataParallel\n’s documentation recommends one GPU per process for best\nperformance.  In this case,\nDistributedDataParallel\ndoes not spawn threads internally,\nso usages of\nautocast\nand\nGradScaler\nare not affected.\nDistributedDataParallel, multiple GPUs per process\n¶\nHere\ntorch.nn.parallel.DistributedDataParallel\nmay spawn a side thread to run the forward pass on each\ndevice, like\ntorch.nn.DataParallel\n.\nThe fix is the same\n:\napply autocast as part of your model’s\nforward\nmethod to ensure it’s enabled in side threads.\nAutocast and Custom Autograd Functions\n¶\nIf your network uses\ncustom autograd functions\n(subclasses of\ntorch.autograd.Function\n), changes are required for\nautocast compatibility if any function\ntakes multiple floating-point Tensor inputs,\nwraps any autocastable op (see the\nAutocast Op Reference\n), or\nrequires a particular\ndtype\n(for example, if it wraps\nCUDA extensions\nthat were only compiled for\ndtype\n).\nIn all cases, if you’re importing the function and can’t alter its definition, a safe fallback\nis to disable autocast and force execution in\nfloat32\n( or\ndtype\n) at any points of use where errors occur:\nwith\nautocast\n(\ndevice_type\n=\n'cuda'\n,\ndtype\n=\ntorch\n.\nfloat16\n):\n...\nwith\nautocast\n(\ndevice_type\n=\n'cuda'\n,\ndtype\n=\ntorch\n.\nfloat16\n,\nenabled\n=\nFalse\n):\noutput\n=\nimported_function\n(\ninput1\n.\nfloat\n(),\ninput2\n.\nfloat\n())\nIf you’re the function’s author (or can alter its definition) a better solution is to use the\ntorch.amp.custom_fwd()\nand\ntorch.amp.custom_bwd()\ndecorators as shown in\nthe relevant case below.\nFunctions with multiple inputs or autocastable ops\n¶\nApply\ncustom_fwd\nand\ncustom_bwd\n(with no arguments) to\nforward\nand\nbackward\nrespectively.  These ensure\nforward\nexecutes with the current autocast state and\nbackward\nexecutes with the same autocast state as\nforward\n(which can prevent type mismatch errors):\nclass\nMyMM\n(\ntorch\n.\nautograd\n.\nFunction\n):\n@staticmethod\n@custom_fwd\ndef\nforward\n(\nctx\n,\na\n,\nb\n):\nctx\n.\nsave_for_backward\n(\na\n,\nb\n)\nreturn\na\n.\nmm\n(\nb\n)\n@staticmethod\n@custom_bwd\ndef\nbackward\n(\nctx\n,\ngrad\n):\na\n,\nb\n=\nctx\n.\nsaved_tensors\nreturn\ngrad\n.\nmm\n(\nb\n.\nt\n()),\na\n.\nt\n()\n.\nmm\n(\ngrad\n)\nNow\nMyMM\ncan be invoked anywhere, without disabling autocast or manually casting inputs:\nmymm\n=\nMyMM\n.\napply\nwith\nautocast\n(\ndevice_type\n=\n'cuda'\n,\ndtype\n=\ntorch\n.\nfloat16\n):\noutput\n=\nmymm\n(\ninput1\n,\ninput2\n)\nFunctions that need a particular\ndtype\n¶\nConsider a custom function that requires\ntorch.float32\ninputs.\nApply\ncustom_fwd(device_type='cuda',\ncast_inputs=torch.float32)\nto\nforward\nand\ncustom_bwd(device_type='cuda')\nto\nbackward\n.\nIf\nforward\nruns in an autocast-enabled region, the decorators cast floating-point Tensor\ninputs to\nfloat32\non designated device assigned by the argument\ndevice_type\n,\nCUDA\nin this example, and locally disable autocast during\nforward\nand\nbackward\n:\nclass\nMyFloat32Func\n(\ntorch\n.\nautograd\n.\nFunction\n):\n@staticmethod\n@custom_fwd\n(\ndevice_type\n=\n'cuda'\n,\ncast_inputs\n=\ntorch\n.\nfloat32\n)\ndef\nforward\n(\nctx\n,\ninput\n):\nctx\n.\nsave_for_backward\n(\ninput\n)\n...\nreturn\nfwd_output\n@staticmethod\n@custom_bwd\n(\ndevice_type\n=\n'cuda'\n)\ndef\nbackward\n(\nctx\n,\ngrad\n):\n...\nNow\nMyFloat32Func\ncan be invoked anywhere, without manually disabling autocast or casting inputs:\nfunc\n=\nMyFloat32Func\n.\napply\nwith\nautocast\n(\ndevice_type\n=\n'cuda'\n,\ndtype\n=\ntorch\n.\nfloat16\n):\n# func will run in float32, regardless of the surrounding autocast state\noutput\n=\nfunc\n(\ninput\n)",
    "url": "https://pytorch.org/docs/stable/notes/amp_examples.html"
  }
]