[
  {
    "title": "scikit-learn: machine learning in Python",
    "content": "scikit-learn: machine learning in Python\nYou will be automatically redirected to the\nnew location of this page\n.",
    "url": "https://scikit-learn.org/stable/modules/model_persistence.html"
  },
  {
    "title": "3.4. Metrics and scoring: quantifying the quality of predictions — scikit-learn 1.7.0 documentation",
    "content": "3.4.\nMetrics and scoring: quantifying the quality of predictions\n#\n3.4.1.\nWhich scoring function should I use?\n#\nBefore we take a closer look into the details of the many scores and\nevaluation metrics\n, we want to give some guidance, inspired by statistical\ndecision theory, on the choice of\nscoring functions\nfor\nsupervised learning\n,\nsee\n[Gneiting2009]\n:\nWhich scoring function should I use?\nWhich scoring function is a good one for my task?\nIn a nutshell, if the scoring function is given, e.g. in a kaggle competition\nor in a business context, use that one.\nIf you are free to choose, it starts by considering the ultimate goal and application\nof the prediction. It is useful to distinguish two steps:\nPredicting\nDecision making\nPredicting:\nUsually, the response variable\n\\(Y\\)\nis a random variable, in the sense that there\nis\nno deterministic\nfunction\n\\(Y = g(X)\\)\nof the features\n\\(X\\)\n.\nInstead, there is a probability distribution\n\\(F\\)\nof\n\\(Y\\)\n.\nOne can aim to predict the whole distribution, known as\nprobabilistic prediction\n,\nor—more the focus of scikit-learn—issue a\npoint prediction\n(or point forecast)\nby choosing a property or functional of that distribution\n\\(F\\)\n.\nTypical examples are the mean (expected value), the median or a quantile of the\nresponse variable\n\\(Y\\)\n(conditionally on\n\\(X\\)\n).\nOnce that is settled, use a\nstrictly consistent\nscoring function for that\n(target) functional, see\n[Gneiting2009]\n.\nThis means using a scoring function that is aligned with\nmeasuring the distance\nbetween predictions\ny_pred\nand the true target functional using observations of\n\\(Y\\)\n, i.e.\ny_true\n.\nFor classification\nstrictly proper scoring rules\n, see\nWikipedia entry for Scoring rule\nand\n[Gneiting2007]\n, coincide with strictly consistent scoring functions.\nThe table further below provides examples.\nOne could say that consistent scoring functions act as\ntruth serum\nin that\nthey guarantee\n“that truth telling […] is an optimal strategy in\nexpectation”\n[Gneiting2014]\n.\nOnce a strictly consistent scoring function is chosen, it is best used for both: as\nloss function for model training and as metric/score in model evaluation and model\ncomparison.\nNote that for regressors, the prediction is done with\npredict\nwhile for\nclassifiers it is usually\npredict_proba\n.\nDecision Making:\nThe most common decisions are done on binary classification tasks, where the result of\npredict_proba\nis turned into a single outcome, e.g., from the predicted\nprobability of rain a decision is made on how to act (whether to take mitigating\nmeasures like an umbrella or not).\nFor classifiers, this is what\npredict\nreturns.\nSee also\nTuning the decision threshold for class prediction\n.\nThere are many scoring functions which measure different aspects of such a\ndecision, most of them are covered with or derived from the\nmetrics.confusion_matrix\n.\nList of strictly consistent scoring functions:\nHere, we list some of the most relevant statistical functionals and corresponding\nstrictly consistent scoring functions for tasks in practice. Note that the list is not\ncomplete and that there are more of them.\nFor further criteria on how to select a specific one, see\n[Fissler2022]\n.\nfunctional\nscoring or loss function\nresponse\ny\nprediction\nClassification\nmean\nBrier score\n1\nmulti-class\npredict_proba\nmean\nlog loss\nmulti-class\npredict_proba\nmode\nzero-one loss\n2\nmulti-class\npredict\n, categorical\nRegression\nmean\nsquared error\n3\nall reals\npredict\n, all reals\nmean\nPoisson deviance\nnon-negative\npredict\n, strictly positive\nmean\nGamma deviance\nstrictly positive\npredict\n, strictly positive\nmean\nTweedie deviance\ndepends on\npower\npredict\n, depends on\npower\nmedian\nabsolute error\nall reals\npredict\n, all reals\nquantile\npinball loss\nall reals\npredict\n, all reals\nmode\nno consistent one exists\nreals\n1\nThe Brier score is just a different name for the squared error in case of\nclassification.\n2\nThe zero-one loss is only consistent but not strictly consistent for the mode.\nThe zero-one loss is equivalent to one minus the accuracy score, meaning it gives\ndifferent score values but the same ranking.\n3\nR² gives the same ranking as squared error.\nFictitious Example:\nLet’s make the above arguments more tangible. Consider a setting in network reliability\nengineering, such as maintaining stable internet or Wi-Fi connections.\nAs provider of the network, you have access to the dataset of log entries of network\nconnections containing network load over time and many interesting features.\nYour goal is to improve the reliability of the connections.\nIn fact, you promise your customers that on at least 99% of all days there are no\nconnection discontinuities larger than 1 minute.\nTherefore, you are interested in a prediction of the 99% quantile (of longest\nconnection interruption duration per day) in order to know in advance when to add\nmore bandwidth and thereby satisfy your customers. So the\ntarget functional\nis the\n99% quantile. From the table above, you choose the pinball loss as scoring function\n(fair enough, not much choice given), for model training (e.g.\nHistGradientBoostingRegressor(loss=\"quantile\",\nquantile=0.99)\n) as well as model\nevaluation (\nmean_pinball_loss(...,\nalpha=0.99)\n- we apologize for the different\nargument names,\nquantile\nand\nalpha\n) be it in grid search for finding\nhyperparameters or in comparing to other models like\nQuantileRegressor(quantile=0.99)\n.\nReferences\n[\nGneiting2007\n]\nT. Gneiting and A. E. Raftery.\nStrictly Proper\nScoring Rules, Prediction, and Estimation\nIn: Journal of the American Statistical Association 102 (2007),\npp. 359– 378.\nlink to pdf\n[\nGneiting2009\n]\n(\n1\n,\n2\n)\nT. Gneiting.\nMaking and Evaluating Point Forecasts\nJournal of the American Statistical Association 106 (2009): 746 - 762.\n[\nGneiting2014\n]\nT. Gneiting and M. Katzfuss.\nProbabilistic Forecasting\n. In: Annual Review of Statistics and Its Application 1.1 (2014), pp. 125–151.\n[\nFissler2022\n]\nT. Fissler, C. Lorentzen and M. Mayer.\nModel\nComparison and Calibration Assessment: User Guide for Consistent Scoring\nFunctions in Machine Learning and Actuarial Practice.\n3.4.2.\nScoring API overview\n#\nThere are 3 different APIs for evaluating the quality of a model’s\npredictions:\nEstimator score method\n: Estimators have a\nscore\nmethod providing a\ndefault evaluation criterion for the problem they are designed to solve.\nMost commonly this is\naccuracy\nfor classifiers and the\ncoefficient of determination\n(\n\\(R^2\\)\n) for regressors.\nDetails for each estimator can be found in its documentation.\nScoring parameter\n: Model-evaluation tools that use\ncross-validation\n(such as\nmodel_selection.GridSearchCV\n,\nmodel_selection.validation_curve\nand\nlinear_model.LogisticRegressionCV\n) rely on an internal\nscoring\nstrategy.\nThis can be specified using the\nscoring\nparameter of that tool and is discussed\nin the section\nThe scoring parameter: defining model evaluation rules\n.\nMetric functions\n: The\nsklearn.metrics\nmodule implements functions\nassessing prediction error for specific purposes. These metrics are detailed\nin sections on\nClassification metrics\n,\nMultilabel ranking metrics\n,\nRegression metrics\nand\nClustering metrics\n.\nFinally,\nDummy estimators\nare useful to get a baseline\nvalue of those metrics for random predictions.\nSee also\nFor “pairwise” metrics, between\nsamples\nand not estimators or\npredictions, see the\nPairwise metrics, Affinities and Kernels\nsection.\n3.4.3.\nThe\nscoring\nparameter: defining model evaluation rules\n#\nModel selection and evaluation tools that internally use\ncross-validation\n(such as\nmodel_selection.GridSearchCV\n,\nmodel_selection.validation_curve\nand\nlinear_model.LogisticRegressionCV\n) take a\nscoring\nparameter that\ncontrols what metric they apply to the estimators evaluated.\nThey can be specified in several ways:\nNone\n: the estimator’s default evaluation criterion (i.e., the metric used in the\nestimator’s\nscore\nmethod) is used.\nString name\n: common metrics can be passed via a string\nname.\nCallable\n: more complex metrics can be passed via a custom\nmetric callable (e.g., function).\nSome tools do also accept multiple metric evaluation. See\nUsing multiple metric evaluation\nfor details.\n3.4.3.1.\nString name scorers\n#\nFor the most common use cases, you can designate a scorer object with the\nscoring\nparameter via a string name; the table below shows all possible values.\nAll scorer objects follow the convention that\nhigher return values are better\nthan lower return values\n. Thus metrics which measure the distance between\nthe model and the data, like\nmetrics.mean_squared_error\n, are\navailable as ‘neg_mean_squared_error’ which return the negated value\nof the metric.\nScoring string name\nFunction\nComment\nClassification\n‘accuracy’\nmetrics.accuracy_score\n‘balanced_accuracy’\nmetrics.balanced_accuracy_score\n‘top_k_accuracy’\nmetrics.top_k_accuracy_score\n‘average_precision’\nmetrics.average_precision_score\n‘neg_brier_score’\nmetrics.brier_score_loss\n‘f1’\nmetrics.f1_score\nfor binary targets\n‘f1_micro’\nmetrics.f1_score\nmicro-averaged\n‘f1_macro’\nmetrics.f1_score\nmacro-averaged\n‘f1_weighted’\nmetrics.f1_score\nweighted average\n‘f1_samples’\nmetrics.f1_score\nby multilabel sample\n‘neg_log_loss’\nmetrics.log_loss\nrequires\npredict_proba\nsupport\n‘precision’ etc.\nmetrics.precision_score\nsuffixes apply as with ‘f1’\n‘recall’ etc.\nmetrics.recall_score\nsuffixes apply as with ‘f1’\n‘jaccard’ etc.\nmetrics.jaccard_score\nsuffixes apply as with ‘f1’\n‘roc_auc’\nmetrics.roc_auc_score\n‘roc_auc_ovr’\nmetrics.roc_auc_score\n‘roc_auc_ovo’\nmetrics.roc_auc_score\n‘roc_auc_ovr_weighted’\nmetrics.roc_auc_score\n‘roc_auc_ovo_weighted’\nmetrics.roc_auc_score\n‘d2_log_loss_score’\nmetrics.d2_log_loss_score\nClustering\n‘adjusted_mutual_info_score’\nmetrics.adjusted_mutual_info_score\n‘adjusted_rand_score’\nmetrics.adjusted_rand_score\n‘completeness_score’\nmetrics.completeness_score\n‘fowlkes_mallows_score’\nmetrics.fowlkes_mallows_score\n‘homogeneity_score’\nmetrics.homogeneity_score\n‘mutual_info_score’\nmetrics.mutual_info_score\n‘normalized_mutual_info_score’\nmetrics.normalized_mutual_info_score\n‘rand_score’\nmetrics.rand_score\n‘v_measure_score’\nmetrics.v_measure_score\nRegression\n‘explained_variance’\nmetrics.explained_variance_score\n‘neg_max_error’\nmetrics.max_error\n‘neg_mean_absolute_error’\nmetrics.mean_absolute_error\n‘neg_mean_squared_error’\nmetrics.mean_squared_error\n‘neg_root_mean_squared_error’\nmetrics.root_mean_squared_error\n‘neg_mean_squared_log_error’\nmetrics.mean_squared_log_error\n‘neg_root_mean_squared_log_error’\nmetrics.root_mean_squared_log_error\n‘neg_median_absolute_error’\nmetrics.median_absolute_error\n‘r2’\nmetrics.r2_score\n‘neg_mean_poisson_deviance’\nmetrics.mean_poisson_deviance\n‘neg_mean_gamma_deviance’\nmetrics.mean_gamma_deviance\n‘neg_mean_absolute_percentage_error’\nmetrics.mean_absolute_percentage_error\n‘d2_absolute_error_score’\nmetrics.d2_absolute_error_score\nUsage examples:\n>>>\nfrom\nsklearn\nimport\nsvm\n,\ndatasets\n>>>\nfrom\nsklearn.model_selection\nimport\ncross_val_score\n>>>\nX\n,\ny\n=\ndatasets\n.\nload_iris\n(\nreturn_X_y\n=\nTrue\n)\n>>>\nclf\n=\nsvm\n.\nSVC\n(\nrandom_state\n=\n0\n)\n>>>\ncross_val_score\n(\nclf\n,\nX\n,\ny\n,\ncv\n=\n5\n,\nscoring\n=\n'recall_macro'\n)\narray([0.96, 0.96, 0.96, 0.93, 1.        ])\nNote\nIf a wrong scoring name is passed, an\nInvalidParameterError\nis raised.\nYou can retrieve the names of all available scorers by calling\nget_scorer_names\n.\n3.4.3.2.\nCallable scorers\n#\nFor more complex use cases and more flexibility, you can pass a callable to\nthe\nscoring\nparameter. This can be done by:\nAdapting predefined metrics via make_scorer\nCreating a custom scorer object\n(most flexible)\n3.4.3.2.1.\nAdapting predefined metrics via\nmake_scorer\n#\nThe following metric functions are not implemented as named scorers,\nsometimes because they require additional parameters, such as\nfbeta_score\n. They cannot be passed to the\nscoring\nparameters; instead their callable needs to be passed to\nmake_scorer\ntogether with the value of the user-settable\nparameters.\nFunction\nParameter\nExample usage\nClassification\nmetrics.fbeta_score\nbeta\nmake_scorer(fbeta_score,\nbeta=2)\nRegression\nmetrics.mean_tweedie_deviance\npower\nmake_scorer(mean_tweedie_deviance,\npower=1.5)\nmetrics.mean_pinball_loss\nalpha\nmake_scorer(mean_pinball_loss,\nalpha=0.95)\nmetrics.d2_tweedie_score\npower\nmake_scorer(d2_tweedie_score,\npower=1.5)\nmetrics.d2_pinball_score\nalpha\nmake_scorer(d2_pinball_score,\nalpha=0.95)\nOne typical use case is to wrap an existing metric function from the library\nwith non-default values for its parameters, such as the\nbeta\nparameter for\nthe\nfbeta_score\nfunction:\n>>>\nfrom\nsklearn.metrics\nimport\nfbeta_score\n,\nmake_scorer\n>>>\nftwo_scorer\n=\nmake_scorer\n(\nfbeta_score\n,\nbeta\n=\n2\n)\n>>>\nfrom\nsklearn.model_selection\nimport\nGridSearchCV\n>>>\nfrom\nsklearn.svm\nimport\nLinearSVC\n>>>\ngrid\n=\nGridSearchCV\n(\nLinearSVC\n(),\nparam_grid\n=\n{\n'C'\n:\n[\n1\n,\n10\n]},\n...\nscoring\n=\nftwo_scorer\n,\ncv\n=\n5\n)\nThe module\nsklearn.metrics\nalso exposes a set of simple functions\nmeasuring a prediction error given ground truth and prediction:\nfunctions ending with\n_score\nreturn a value to\nmaximize, the higher the better.\nfunctions ending with\n_error\n,\n_loss\n, or\n_deviance\nreturn a\nvalue to minimize, the lower the better. When converting\ninto a scorer object using\nmake_scorer\n, set\nthe\ngreater_is_better\nparameter to\nFalse\n(\nTrue\nby default; see the\nparameter description below).\n3.4.3.2.2.\nCreating a custom scorer object\n#\nYou can create your own custom scorer object using\nmake_scorer\nor for the most flexibility, from scratch. See below for details.\nCustom scorer objects using\nmake_scorer\n#\nYou can build a completely custom scorer object\nfrom a simple python function using\nmake_scorer\n, which can\ntake several parameters:\nthe python function you want to use (\nmy_custom_loss_func\nin the example below)\nwhether the python function returns a score (\ngreater_is_better=True\n,\nthe default) or a loss (\ngreater_is_better=False\n). If a loss, the output\nof the python function is negated by the scorer object, conforming to\nthe cross validation convention that scorers return higher values for better models.\nfor classification metrics only: whether the python function you provided requires\ncontinuous decision certainties. If the scoring function only accepts probability\nestimates (e.g.\nmetrics.log_loss\n), then one needs to set the parameter\nresponse_method=\"predict_proba\"\n. Some scoring\nfunctions do not necessarily require probability estimates but rather non-thresholded\ndecision values (e.g.\nmetrics.roc_auc_score\n). In this case, one can provide a\nlist (e.g.,\nresponse_method=[\"decision_function\",\n\"predict_proba\"]\n),\nand scorer will use the first available method, in the order given in the list,\nto compute the scores.\nany additional parameters of the scoring function, such as\nbeta\nor\nlabels\n.\nHere is an example of building custom scorers, and of using the\ngreater_is_better\nparameter:\n>>>\nimport\nnumpy\nas\nnp\n>>>\ndef\nmy_custom_loss_func\n(\ny_true\n,\ny_pred\n):\n...\ndiff\n=\nnp\n.\nabs\n(\ny_true\n-\ny_pred\n)\n.\nmax\n()\n...\nreturn\nfloat\n(\nnp\n.\nlog1p\n(\ndiff\n))\n...\n>>>\n# score will negate the return value of my_custom_loss_func,\n>>>\n# which will be np.log(2), 0.693, given the values for X\n>>>\n# and y defined below.\n>>>\nscore\n=\nmake_scorer\n(\nmy_custom_loss_func\n,\ngreater_is_better\n=\nFalse\n)\n>>>\nX\n=\n[[\n1\n],\n[\n1\n]]\n>>>\ny\n=\n[\n0\n,\n1\n]\n>>>\nfrom\nsklearn.dummy\nimport\nDummyClassifier\n>>>\nclf\n=\nDummyClassifier\n(\nstrategy\n=\n'most_frequent'\n,\nrandom_state\n=\n0\n)\n>>>\nclf\n=\nclf\n.\nfit\n(\nX\n,\ny\n)\n>>>\nmy_custom_loss_func\n(\ny\n,\nclf\n.\npredict\n(\nX\n))\n0.69\n>>>\nscore\n(\nclf\n,\nX\n,\ny\n)\n-0.69\nCustom scorer objects from scratch\n#\nYou can generate even more flexible model scorers by constructing your own\nscoring object from scratch, without using the\nmake_scorer\nfactory.\nFor a callable to be a scorer, it needs to meet the protocol specified by\nthe following two rules:\nIt can be called with parameters\n(estimator,\nX,\ny)\n, where\nestimator\nis the model that should be evaluated,\nX\nis validation data, and\ny\nis\nthe ground truth target for\nX\n(in the supervised case) or\nNone\n(in the\nunsupervised case).\nIt returns a floating point number that quantifies the\nestimator\nprediction quality on\nX\n, with reference to\ny\n.\nAgain, by convention higher numbers are better, so if your scorer\nreturns loss, that value should be negated.\nAdvanced: If it requires extra metadata to be passed to it, it should expose\na\nget_metadata_routing\nmethod returning the requested metadata. The user\nshould be able to set the requested metadata via a\nset_score_request\nmethod. Please see\nUser Guide\nand\nDeveloper\nGuide\nfor\nmore details.\nUsing custom scorers in functions where n_jobs > 1\n#\nWhile defining the custom scoring function alongside the calling function\nshould work out of the box with the default joblib backend (loky),\nimporting it from another module will be a more robust approach and work\nindependently of the joblib backend.\nFor example, to use\nn_jobs\ngreater than 1 in the example below,\ncustom_scoring_function\nfunction is saved in a user-created module\n(\ncustom_scorer_module.py\n) and imported:\n>>>\nfrom\ncustom_scorer_module\nimport\ncustom_scoring_function\n>>>\ncross_val_score\n(\nmodel\n,\n...\nX_train\n,\n...\ny_train\n,\n...\nscoring\n=\nmake_scorer\n(\ncustom_scoring_function\n,\ngreater_is_better\n=\nFalse\n),\n...\ncv\n=\n5\n,\n...\nn_jobs\n=-\n1\n)\n3.4.3.3.\nUsing multiple metric evaluation\n#\nScikit-learn also permits evaluation of multiple metrics in\nGridSearchCV\n,\nRandomizedSearchCV\nand\ncross_validate\n.\nThere are three ways to specify multiple scoring metrics for the\nscoring\nparameter:\nAs an iterable of string metrics:\n>>>\nscoring\n=\n[\n'accuracy'\n,\n'precision'\n]\nAs a\ndict\nmapping the scorer name to the scoring function:\n>>>\nfrom\nsklearn.metrics\nimport\naccuracy_score\n>>>\nfrom\nsklearn.metrics\nimport\nmake_scorer\n>>>\nscoring\n=\n{\n'accuracy'\n:\nmake_scorer\n(\naccuracy_score\n),\n...\n'prec'\n:\n'precision'\n}\nNote that the dict values can either be scorer functions or one of the\npredefined metric strings.\nAs a callable that returns a dictionary of scores:\n>>>\nfrom\nsklearn.model_selection\nimport\ncross_validate\n>>>\nfrom\nsklearn.metrics\nimport\nconfusion_matrix\n>>>\n# A sample toy binary classification dataset\n>>>\nX\n,\ny\n=\ndatasets\n.\nmake_classification\n(\nn_classes\n=\n2\n,\nrandom_state\n=\n0\n)\n>>>\nsvm\n=\nLinearSVC\n(\nrandom_state\n=\n0\n)\n>>>\ndef\nconfusion_matrix_scorer\n(\nclf\n,\nX\n,\ny\n):\n...\ny_pred\n=\nclf\n.\npredict\n(\nX\n)\n...\ncm\n=\nconfusion_matrix\n(\ny\n,\ny_pred\n)\n...\nreturn\n{\n'tn'\n:\ncm\n[\n0\n,\n0\n],\n'fp'\n:\ncm\n[\n0\n,\n1\n],\n...\n'fn'\n:\ncm\n[\n1\n,\n0\n],\n'tp'\n:\ncm\n[\n1\n,\n1\n]}\n>>>\ncv_results\n=\ncross_validate\n(\nsvm\n,\nX\n,\ny\n,\ncv\n=\n5\n,\n...\nscoring\n=\nconfusion_matrix_scorer\n)\n>>>\n# Getting the test set true positive scores\n>>>\nprint\n(\ncv_results\n[\n'test_tp'\n])\n[10  9  8  7  8]\n>>>\n# Getting the test set false negative scores\n>>>\nprint\n(\ncv_results\n[\n'test_fn'\n])\n[0 1 2 3 2]\n3.4.4.\nClassification metrics\n#\nThe\nsklearn.metrics\nmodule implements several loss, score, and utility\nfunctions to measure classification performance.\nSome metrics might require probability estimates of the positive class,\nconfidence values, or binary decisions values.\nMost implementations allow each sample to provide a weighted contribution\nto the overall score, through the\nsample_weight\nparameter.\nSome of these are restricted to the binary classification case:\nprecision_recall_curve\n(y_true, y_score, *[, ...])\nCompute precision-recall pairs for different probability thresholds.\nroc_curve\n(y_true, y_score, *[, pos_label, ...])\nCompute Receiver operating characteristic (ROC).\nclass_likelihood_ratios\n(y_true, y_pred, *[, ...])\nCompute binary classification positive and negative likelihood ratios.\ndet_curve\n(y_true, y_score[, pos_label, ...])\nCompute Detection Error Tradeoff (DET) for different probability thresholds.\nOthers also work in the multiclass case:\nbalanced_accuracy_score\n(y_true, y_pred, *[, ...])\nCompute the balanced accuracy.\ncohen_kappa_score\n(y1, y2, *[, labels, ...])\nCompute Cohen's kappa: a statistic that measures inter-annotator agreement.\nconfusion_matrix\n(y_true, y_pred, *[, ...])\nCompute confusion matrix to evaluate the accuracy of a classification.\nhinge_loss\n(y_true, pred_decision, *[, ...])\nAverage hinge loss (non-regularized).\nmatthews_corrcoef\n(y_true, y_pred, *[, ...])\nCompute the Matthews correlation coefficient (MCC).\nroc_auc_score\n(y_true, y_score, *[, average, ...])\nCompute Area Under the Receiver Operating Characteristic Curve (ROC AUC)     from prediction scores.\ntop_k_accuracy_score\n(y_true, y_score, *[, ...])\nTop-k Accuracy classification score.\nSome also work in the multilabel case:\naccuracy_score\n(y_true, y_pred, *[, ...])\nAccuracy classification score.\nclassification_report\n(y_true, y_pred, *[, ...])\nBuild a text report showing the main classification metrics.\nf1_score\n(y_true, y_pred, *[, labels, ...])\nCompute the F1 score, also known as balanced F-score or F-measure.\nfbeta_score\n(y_true, y_pred, *, beta[, ...])\nCompute the F-beta score.\nhamming_loss\n(y_true, y_pred, *[, sample_weight])\nCompute the average Hamming loss.\njaccard_score\n(y_true, y_pred, *[, labels, ...])\nJaccard similarity coefficient score.\nlog_loss\n(y_true, y_pred, *[, normalize, ...])\nLog loss, aka logistic loss or cross-entropy loss.\nmultilabel_confusion_matrix\n(y_true, y_pred, *)\nCompute a confusion matrix for each class or sample.\nprecision_recall_fscore_support\n(y_true, ...)\nCompute precision, recall, F-measure and support for each class.\nprecision_score\n(y_true, y_pred, *[, labels, ...])\nCompute the precision.\nrecall_score\n(y_true, y_pred, *[, labels, ...])\nCompute the recall.\nroc_auc_score\n(y_true, y_score, *[, average, ...])\nCompute Area Under the Receiver Operating Characteristic Curve (ROC AUC)     from prediction scores.\nzero_one_loss\n(y_true, y_pred, *[, ...])\nZero-one classification loss.\nd2_log_loss_score\n(y_true, y_pred, *[, ...])\n\\(D^2\\)\nscore function, fraction of log loss explained.\nAnd some work with binary and multilabel (but not multiclass) problems:\naverage_precision_score\n(y_true, y_score, *)\nCompute average precision (AP) from prediction scores.\nIn the following sub-sections, we will describe each of those functions,\npreceded by some notes on common API and metric definition.\n3.4.4.1.\nFrom binary to multiclass and multilabel\n#\nSome metrics are essentially defined for binary classification tasks (e.g.\nf1_score\n,\nroc_auc_score\n). In these cases, by default\nonly the positive label is evaluated, assuming by default that the positive\nclass is labelled\n1\n(though this may be configurable through the\npos_label\nparameter).\nIn extending a binary metric to multiclass or multilabel problems, the data\nis treated as a collection of binary problems, one for each class.\nThere are then a number of ways to average binary metric calculations across\nthe set of classes, each of which may be useful in some scenario.\nWhere available, you should select among these using the\naverage\nparameter.\n\"macro\"\nsimply calculates the mean of the binary metrics,\ngiving equal weight to each class.  In problems where infrequent classes\nare nonetheless important, macro-averaging may be a means of highlighting\ntheir performance. On the other hand, the assumption that all classes are\nequally important is often untrue, such that macro-averaging will\nover-emphasize the typically low performance on an infrequent class.\n\"weighted\"\naccounts for class imbalance by computing the average of\nbinary metrics in which each class’s score is weighted by its presence in the\ntrue data sample.\n\"micro\"\ngives each sample-class pair an equal contribution to the overall\nmetric (except as a result of sample-weight). Rather than summing the\nmetric per class, this sums the dividends and divisors that make up the\nper-class metrics to calculate an overall quotient.\nMicro-averaging may be preferred in multilabel settings, including\nmulticlass classification where a majority class is to be ignored.\n\"samples\"\napplies only to multilabel problems. It does not calculate a\nper-class measure, instead calculating the metric over the true and predicted\nclasses for each sample in the evaluation data, and returning their\n(\nsample_weight\n-weighted) average.\nSelecting\naverage=None\nwill return an array with the score for each\nclass.\nWhile multiclass data is provided to the metric, like binary targets, as an\narray of class labels, multilabel data is specified as an indicator matrix,\nin which cell\n[i,\nj]\nhas value 1 if sample\ni\nhas label\nj\nand value\n0 otherwise.\n3.4.4.2.\nAccuracy score\n#\nThe\naccuracy_score\nfunction computes the\naccuracy\n, either the fraction\n(default) or the count (normalize=False) of correct predictions.\nIn multilabel classification, the function returns the subset accuracy. If\nthe entire set of predicted labels for a sample strictly match with the true\nset of labels, then the subset accuracy is 1.0; otherwise it is 0.0.\nIf\n\\(\\hat{y}_i\\)\nis the predicted value of\nthe\n\\(i\\)\n-th sample and\n\\(y_i\\)\nis the corresponding true value,\nthen the fraction of correct predictions over\n\\(n_\\text{samples}\\)\nis\ndefined as\n\\[\\texttt{accuracy}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples}-1} 1(\\hat{y}_i = y_i)\\]\nwhere\n\\(1(x)\\)\nis the\nindicator function\n.\n>>>\nimport\nnumpy\nas\nnp\n>>>\nfrom\nsklearn.metrics\nimport\naccuracy_score\n>>>\ny_pred\n=\n[\n0\n,\n2\n,\n1\n,\n3\n]\n>>>\ny_true\n=\n[\n0\n,\n1\n,\n2\n,\n3\n]\n>>>\naccuracy_score\n(\ny_true\n,\ny_pred\n)\n0.5\n>>>\naccuracy_score\n(\ny_true\n,\ny_pred\n,\nnormalize\n=\nFalse\n)\n2.0\nIn the multilabel case with binary label indicators:\n>>>\naccuracy_score\n(\nnp\n.\narray\n([[\n0\n,\n1\n],\n[\n1\n,\n1\n]]),\nnp\n.\nones\n((\n2\n,\n2\n)))\n0.5\nExamples\nSee\nTest with permutations the significance of a classification score\nfor an example of accuracy score usage using permutations of\nthe dataset.\n3.4.4.3.\nTop-k accuracy score\n#\nThe\ntop_k_accuracy_score\nfunction is a generalization of\naccuracy_score\n. The difference is that a prediction is considered\ncorrect as long as the true label is associated with one of the\nk\nhighest\npredicted scores.\naccuracy_score\nis the special case of\nk\n=\n1\n.\nThe function covers the binary and multiclass classification cases but not the\nmultilabel case.\nIf\n\\(\\hat{f}_{i,j}\\)\nis the predicted class for the\n\\(i\\)\n-th sample\ncorresponding to the\n\\(j\\)\n-th largest predicted score and\n\\(y_i\\)\nis the\ncorresponding true value, then the fraction of correct predictions over\n\\(n_\\text{samples}\\)\nis defined as\n\\[\\texttt{top-k accuracy}(y, \\hat{f}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples}-1} \\sum_{j=1}^{k} 1(\\hat{f}_{i,j} = y_i)\\]\nwhere\n\\(k\\)\nis the number of guesses allowed and\n\\(1(x)\\)\nis the\nindicator function\n.\n>>>\nimport\nnumpy\nas\nnp\n>>>\nfrom\nsklearn.metrics\nimport\ntop_k_accuracy_score\n>>>\ny_true\n=\nnp\n.\narray\n([\n0\n,\n1\n,\n2\n,\n2\n])\n>>>\ny_score\n=\nnp\n.\narray\n([[\n0.5\n,\n0.2\n,\n0.2\n],\n...\n[\n0.3\n,\n0.4\n,\n0.2\n],\n...\n[\n0.2\n,\n0.4\n,\n0.3\n],\n...\n[\n0.7\n,\n0.2\n,\n0.1\n]])\n>>>\ntop_k_accuracy_score\n(\ny_true\n,\ny_score\n,\nk\n=\n2\n)\n0.75\n>>>\n# Not normalizing gives the number of \"correctly\" classified samples\n>>>\ntop_k_accuracy_score\n(\ny_true\n,\ny_score\n,\nk\n=\n2\n,\nnormalize\n=\nFalse\n)\n3.0\n3.4.4.4.\nBalanced accuracy score\n#\nThe\nbalanced_accuracy_score\nfunction computes the\nbalanced accuracy\n, which avoids inflated\nperformance estimates on imbalanced datasets. It is the macro-average of recall\nscores per class or, equivalently, raw accuracy where each sample is weighted\naccording to the inverse prevalence of its true class.\nThus for balanced datasets, the score is equal to accuracy.\nIn the binary case, balanced accuracy is equal to the arithmetic mean of\nsensitivity\n(true positive rate) and\nspecificity\n(true negative\nrate), or the area under the ROC curve with binary predictions rather than\nscores:\n\\[\\texttt{balanced-accuracy} = \\frac{1}{2}\\left( \\frac{TP}{TP + FN} + \\frac{TN}{TN + FP}\\right )\\]\nIf the classifier performs equally well on either class, this term reduces to\nthe conventional accuracy (i.e., the number of correct predictions divided by\nthe total number of predictions).\nIn contrast, if the conventional accuracy is above chance only because the\nclassifier takes advantage of an imbalanced test set, then the balanced\naccuracy, as appropriate, will drop to\n\\(\\frac{1}{n\\_classes}\\)\n.\nThe score ranges from 0 to 1, or when\nadjusted=True\nis used, it is rescaled to\nthe range\n\\(\\frac{1}{1 - n\\_classes}\\)\nto 1, inclusive, with\nperformance at random scoring 0.\nIf\n\\(y_i\\)\nis the true value of the\n\\(i\\)\n-th sample, and\n\\(w_i\\)\nis the corresponding sample weight, then we adjust the sample weight to:\n\\[\\hat{w}_i = \\frac{w_i}{\\sum_j{1(y_j = y_i) w_j}}\\]\nwhere\n\\(1(x)\\)\nis the\nindicator function\n.\nGiven predicted\n\\(\\hat{y}_i\\)\nfor sample\n\\(i\\)\n, balanced accuracy is\ndefined as:\n\\[\\texttt{balanced-accuracy}(y, \\hat{y}, w) = \\frac{1}{\\sum{\\hat{w}_i}} \\sum_i 1(\\hat{y}_i = y_i) \\hat{w}_i\\]\nWith\nadjusted=True\n, balanced accuracy reports the relative increase from\n\\(\\texttt{balanced-accuracy}(y, \\mathbf{0}, w) =\n\\frac{1}{n\\_classes}\\)\n.  In the binary case, this is also known as\n*Youden’s J statistic*\n,\nor\ninformedness\n.\nNote\nThe multiclass definition here seems the most reasonable extension of the\nmetric used in binary classification, though there is no certain consensus\nin the literature:\nOur definition:\n[Mosley2013]\n,\n[Kelleher2015]\nand\n[Guyon2015]\n, where\n[Guyon2015]\nadopt the adjusted version to ensure that random predictions\nhave a score of\n\\(0\\)\nand perfect predictions have a score of\n\\(1\\)\n..\nClass balanced accuracy as described in\n[Mosley2013]\n: the minimum between the precision\nand the recall for each class is computed. Those values are then averaged over the total\nnumber of classes to get the balanced accuracy.\nBalanced Accuracy as described in\n[Urbanowicz2015]\n: the average of sensitivity and specificity\nis computed for each class and then averaged over total number of classes.\nReferences\n[\nGuyon2015\n]\n(\n1\n,\n2\n)\nI. Guyon, K. Bennett, G. Cawley, H.J. Escalante, S. Escalera, T.K. Ho, N. Macià,\nB. Ray, M. Saeed, A.R. Statnikov, E. Viegas,\nDesign of the 2015 ChaLearn AutoML Challenge\n, IJCNN 2015.\n[\nMosley2013\n]\n(\n1\n,\n2\n)\nL. Mosley,\nA balanced approach to the multi-class imbalance problem\n, IJCV 2010.\n[\nKelleher2015\n]\nJohn. D. Kelleher, Brian Mac Namee, Aoife D’Arcy,\nFundamentals of\nMachine Learning for Predictive Data Analytics: Algorithms, Worked Examples,\nand Case Studies\n,\n2015.\n[\nUrbanowicz2015\n]\nUrbanowicz R.J.,  Moore, J.H.\nExSTraCS 2.0: description\nand evaluation of a scalable learning classifier\nsystem\n, Evol. Intel. (2015) 8: 89.\n3.4.4.5.\nCohen’s kappa\n#\nThe function\ncohen_kappa_score\ncomputes\nCohen’s kappa\nstatistic.\nThis measure is intended to compare labelings by different human annotators,\nnot a classifier versus a ground truth.\nThe kappa score is a number between -1 and 1.\nScores above .8 are generally considered good agreement;\nzero or lower means no agreement (practically random labels).\nKappa scores can be computed for binary or multiclass problems,\nbut not for multilabel problems (except by manually computing a per-label score)\nand not for more than two annotators.\n>>>\nfrom\nsklearn.metrics\nimport\ncohen_kappa_score\n>>>\nlabeling1\n=\n[\n2\n,\n0\n,\n2\n,\n2\n,\n0\n,\n1\n]\n>>>\nlabeling2\n=\n[\n0\n,\n0\n,\n2\n,\n2\n,\n0\n,\n2\n]\n>>>\ncohen_kappa_score\n(\nlabeling1\n,\nlabeling2\n)\n0.4285714285714286\n3.4.4.6.\nConfusion matrix\n#\nThe\nconfusion_matrix\nfunction evaluates\nclassification accuracy by computing the\nconfusion matrix\nwith each row corresponding\nto the true class (Wikipedia and other references may use different convention\nfor axes).\nBy definition, entry\n\\(i, j\\)\nin a confusion matrix is\nthe number of observations actually in group\n\\(i\\)\n, but\npredicted to be in group\n\\(j\\)\n. Here is an example:\n>>>\nfrom\nsklearn.metrics\nimport\nconfusion_matrix\n>>>\ny_true\n=\n[\n2\n,\n0\n,\n2\n,\n2\n,\n0\n,\n1\n]\n>>>\ny_pred\n=\n[\n0\n,\n0\n,\n2\n,\n2\n,\n0\n,\n2\n]\n>>>\nconfusion_matrix\n(\ny_true\n,\ny_pred\n)\narray([[2, 0, 0],\n[0, 0, 1],\n[1, 0, 2]])\nConfusionMatrixDisplay\ncan be used to visually represent a confusion\nmatrix as shown in the\nConfusion matrix\nexample, which creates the following figure:\nThe parameter\nnormalize\nallows to report ratios instead of counts. The\nconfusion matrix can be normalized in 3 different ways:\n'pred'\n,\n'true'\n,\nand\n'all'\nwhich will divide the counts by the sum of each columns, rows, or\nthe entire matrix, respectively.\n>>>\ny_true\n=\n[\n0\n,\n0\n,\n0\n,\n1\n,\n1\n,\n1\n,\n1\n,\n1\n]\n>>>\ny_pred\n=\n[\n0\n,\n1\n,\n0\n,\n1\n,\n0\n,\n1\n,\n0\n,\n1\n]\n>>>\nconfusion_matrix\n(\ny_true\n,\ny_pred\n,\nnormalize\n=\n'all'\n)\narray([[0.25 , 0.125],\n[0.25 , 0.375]])\nFor binary problems, we can get counts of true negatives, false positives,\nfalse negatives and true positives as follows:\n>>>\ny_true\n=\n[\n0\n,\n0\n,\n0\n,\n1\n,\n1\n,\n1\n,\n1\n,\n1\n]\n>>>\ny_pred\n=\n[\n0\n,\n1\n,\n0\n,\n1\n,\n0\n,\n1\n,\n0\n,\n1\n]\n>>>\ntn\n,\nfp\n,\nfn\n,\ntp\n=\nconfusion_matrix\n(\ny_true\n,\ny_pred\n)\n.\nravel\n()\n.\ntolist\n()\n>>>\ntn\n,\nfp\n,\nfn\n,\ntp\n(2, 1, 2, 3)\nExamples\nSee\nConfusion matrix\nfor an example of using a confusion matrix to evaluate classifier output\nquality.\nSee\nRecognizing hand-written digits\nfor an example of using a confusion matrix to classify\nhand-written digits.\nSee\nClassification of text documents using sparse features\nfor an example of using a confusion matrix to classify text\ndocuments.\n3.4.4.7.\nClassification report\n#\nThe\nclassification_report\nfunction builds a text report showing the\nmain classification metrics. Here is a small example with custom\ntarget_names\nand inferred labels:\n>>>\nfrom\nsklearn.metrics\nimport\nclassification_report\n>>>\ny_true\n=\n[\n0\n,\n1\n,\n2\n,\n2\n,\n0\n]\n>>>\ny_pred\n=\n[\n0\n,\n0\n,\n2\n,\n1\n,\n0\n]\n>>>\ntarget_names\n=\n[\n'class 0'\n,\n'class 1'\n,\n'class 2'\n]\n>>>\nprint\n(\nclassification_report\n(\ny_true\n,\ny_pred\n,\ntarget_names\n=\ntarget_names\n))\nprecision    recall  f1-score   support\nclass 0       0.67      1.00      0.80         2\nclass 1       0.00      0.00      0.00         1\nclass 2       1.00      0.50      0.67         2\naccuracy                           0.60         5\nmacro avg       0.56      0.50      0.49         5\nweighted avg       0.67      0.60      0.59         5\nExamples\nSee\nRecognizing hand-written digits\nfor an example of classification report usage for\nhand-written digits.\nSee\nCustom refit strategy of a grid search with cross-validation\nfor an example of classification report usage for\ngrid search with nested cross-validation.\n3.4.4.8.\nHamming loss\n#\nThe\nhamming_loss\ncomputes the average Hamming loss or\nHamming\ndistance\nbetween two sets\nof samples.\nIf\n\\(\\hat{y}_{i,j}\\)\nis the predicted value for the\n\\(j\\)\n-th label of a\ngiven sample\n\\(i\\)\n,\n\\(y_{i,j}\\)\nis the corresponding true value,\n\\(n_\\text{samples}\\)\nis the number of samples and\n\\(n_\\text{labels}\\)\nis the number of labels, then the Hamming loss\n\\(L_{Hamming}\\)\nis defined\nas:\n\\[L_{Hamming}(y, \\hat{y}) = \\frac{1}{n_\\text{samples} * n_\\text{labels}} \\sum_{i=0}^{n_\\text{samples}-1} \\sum_{j=0}^{n_\\text{labels} - 1} 1(\\hat{y}_{i,j} \\not= y_{i,j})\\]\nwhere\n\\(1(x)\\)\nis the\nindicator function\n.\nThe equation above does not hold true in the case of multiclass classification.\nPlease refer to the note below for more information.\n>>>\nfrom\nsklearn.metrics\nimport\nhamming_loss\n>>>\ny_pred\n=\n[\n1\n,\n2\n,\n3\n,\n4\n]\n>>>\ny_true\n=\n[\n2\n,\n2\n,\n3\n,\n4\n]\n>>>\nhamming_loss\n(\ny_true\n,\ny_pred\n)\n0.25\nIn the multilabel case with binary label indicators:\n>>>\nhamming_loss\n(\nnp\n.\narray\n([[\n0\n,\n1\n],\n[\n1\n,\n1\n]]),\nnp\n.\nzeros\n((\n2\n,\n2\n)))\n0.75\nNote\nIn multiclass classification, the Hamming loss corresponds to the Hamming\ndistance between\ny_true\nand\ny_pred\nwhich is similar to the\nZero one loss\nfunction.  However, while zero-one loss penalizes\nprediction sets that do not strictly match true sets, the Hamming loss\npenalizes individual labels.  Thus the Hamming loss, upper bounded by the zero-one\nloss, is always between zero and one, inclusive; and predicting a proper subset\nor superset of the true labels will give a Hamming loss between\nzero and one, exclusive.\n3.4.4.9.\nPrecision, recall and F-measures\n#\nIntuitively,\nprecision\nis the ability\nof the classifier not to label as positive a sample that is negative, and\nrecall\nis the\nability of the classifier to find all the positive samples.\nThe\nF-measure\n(\n\\(F_\\beta\\)\nand\n\\(F_1\\)\nmeasures) can be interpreted as a weighted\nharmonic mean of the precision and recall. A\n\\(F_\\beta\\)\nmeasure reaches its best value at 1 and its worst score at 0.\nWith\n\\(\\beta = 1\\)\n,\n\\(F_\\beta\\)\nand\n\\(F_1\\)\nare equivalent, and the recall and the precision are equally important.\nThe\nprecision_recall_curve\ncomputes a precision-recall curve\nfrom the ground truth label and a score given by the classifier\nby varying a decision threshold.\nThe\naverage_precision_score\nfunction computes the\naverage precision\n(AP) from prediction scores. The value is between 0 and 1 and higher is better.\nAP is defined as\n\\[\\text{AP} = \\sum_n (R_n - R_{n-1}) P_n\\]\nwhere\n\\(P_n\\)\nand\n\\(R_n\\)\nare the precision and recall at the\nnth threshold. With random predictions, the AP is the fraction of positive\nsamples.\nReferences\n[Manning2008]\nand\n[Everingham2010]\npresent alternative variants of\nAP that interpolate the precision-recall curve. Currently,\naverage_precision_score\ndoes not implement any interpolated variant.\nReferences\n[Davis2006]\nand\n[Flach2015]\ndescribe why a linear interpolation of\npoints on the precision-recall curve provides an overly-optimistic measure of\nclassifier performance. This linear interpolation is used when computing area\nunder the curve with the trapezoidal rule in\nauc\n.\nSeveral functions allow you to analyze the precision, recall and F-measures\nscore:\naverage_precision_score\n(y_true, y_score, *)\nCompute average precision (AP) from prediction scores.\nf1_score\n(y_true, y_pred, *[, labels, ...])\nCompute the F1 score, also known as balanced F-score or F-measure.\nfbeta_score\n(y_true, y_pred, *, beta[, ...])\nCompute the F-beta score.\nprecision_recall_curve\n(y_true, y_score, *[, ...])\nCompute precision-recall pairs for different probability thresholds.\nprecision_recall_fscore_support\n(y_true, ...)\nCompute precision, recall, F-measure and support for each class.\nprecision_score\n(y_true, y_pred, *[, labels, ...])\nCompute the precision.\nrecall_score\n(y_true, y_pred, *[, labels, ...])\nCompute the recall.\nNote that the\nprecision_recall_curve\nfunction is restricted to the\nbinary case. The\naverage_precision_score\nfunction supports multiclass\nand multilabel formats by computing each class score in a One-vs-the-rest (OvR)\nfashion and averaging them or not depending of its\naverage\nargument value.\nThe\nPrecisionRecallDisplay.from_estimator\nand\nPrecisionRecallDisplay.from_predictions\nfunctions will plot the\nprecision-recall curve as follows.\nExamples\nSee\nCustom refit strategy of a grid search with cross-validation\nfor an example of\nprecision_score\nand\nrecall_score\nusage\nto estimate parameters using grid search with nested cross-validation.\nSee\nPrecision-Recall\nfor an example of\nprecision_recall_curve\nusage to evaluate\nclassifier output quality.\nReferences\n[\nManning2008\n]\nC.D. Manning, P. Raghavan, H. Schütze,\nIntroduction to Information Retrieval\n,\n2008.\n[\nEveringham2010\n]\nM. Everingham, L. Van Gool, C.K.I. Williams, J. Winn, A. Zisserman,\nThe Pascal Visual Object Classes (VOC) Challenge\n,\nIJCV 2010.\n[\nDavis2006\n]\nJ. Davis, M. Goadrich,\nThe Relationship Between Precision-Recall and ROC Curves\n,\nICML 2006.\n[\nFlach2015\n]\nP.A. Flach, M. Kull,\nPrecision-Recall-Gain Curves: PR Analysis Done Right\n,\nNIPS 2015.\n3.4.4.9.1.\nBinary classification\n#\nIn a binary classification task, the terms ‘’positive’’ and ‘’negative’’ refer\nto the classifier’s prediction, and the terms ‘’true’’ and ‘’false’’ refer to\nwhether that prediction corresponds to the external judgment (sometimes known\nas the ‘’observation’’). Given these definitions, we can formulate the\nfollowing table:\nActual class (observation)\nPredicted class\n(expectation)\ntp (true positive)\nCorrect result\nfp (false positive)\nUnexpected result\nfn (false negative)\nMissing result\ntn (true negative)\nCorrect absence of result\nIn this context, we can define the notions of precision and recall:\n\\[\\text{precision} = \\frac{\\text{tp}}{\\text{tp} + \\text{fp}},\\]\n\\[\\text{recall} = \\frac{\\text{tp}}{\\text{tp} + \\text{fn}},\\]\n(Sometimes recall is also called ‘’sensitivity’’)\nF-measure is the weighted harmonic mean of precision and recall, with precision’s\ncontribution to the mean weighted by some parameter\n\\(\\beta\\)\n:\n\\[F_\\beta = (1 + \\beta^2) \\frac{\\text{precision} \\times \\text{recall}}{\\beta^2 \\text{precision} + \\text{recall}}\\]\nTo avoid division by zero when precision and recall are zero, Scikit-Learn calculates F-measure with this\notherwise-equivalent formula:\n\\[F_\\beta = \\frac{(1 + \\beta^2) \\text{tp}}{(1 + \\beta^2) \\text{tp} + \\text{fp} + \\beta^2 \\text{fn}}\\]\nNote that this formula is still undefined when there are no true positives, false\npositives, or false negatives. By default, F-1 for a set of exclusively true negatives\nis calculated as 0, however this behavior can be changed using the\nzero_division\nparameter.\nHere are some small examples in binary classification:\n>>>\nfrom\nsklearn\nimport\nmetrics\n>>>\ny_pred\n=\n[\n0\n,\n1\n,\n0\n,\n0\n]\n>>>\ny_true\n=\n[\n0\n,\n1\n,\n0\n,\n1\n]\n>>>\nmetrics\n.\nprecision_score\n(\ny_true\n,\ny_pred\n)\n1.0\n>>>\nmetrics\n.\nrecall_score\n(\ny_true\n,\ny_pred\n)\n0.5\n>>>\nmetrics\n.\nf1_score\n(\ny_true\n,\ny_pred\n)\n0.66\n>>>\nmetrics\n.\nfbeta_score\n(\ny_true\n,\ny_pred\n,\nbeta\n=\n0.5\n)\n0.83\n>>>\nmetrics\n.\nfbeta_score\n(\ny_true\n,\ny_pred\n,\nbeta\n=\n1\n)\n0.66\n>>>\nmetrics\n.\nfbeta_score\n(\ny_true\n,\ny_pred\n,\nbeta\n=\n2\n)\n0.55\n>>>\nmetrics\n.\nprecision_recall_fscore_support\n(\ny_true\n,\ny_pred\n,\nbeta\n=\n0.5\n)\n(array([0.66, 1.        ]), array([1. , 0.5]), array([0.71, 0.83]), array([2, 2]))\n>>>\nimport\nnumpy\nas\nnp\n>>>\nfrom\nsklearn.metrics\nimport\nprecision_recall_curve\n>>>\nfrom\nsklearn.metrics\nimport\naverage_precision_score\n>>>\ny_true\n=\nnp\n.\narray\n([\n0\n,\n0\n,\n1\n,\n1\n])\n>>>\ny_scores\n=\nnp\n.\narray\n([\n0.1\n,\n0.4\n,\n0.35\n,\n0.8\n])\n>>>\nprecision\n,\nrecall\n,\nthreshold\n=\nprecision_recall_curve\n(\ny_true\n,\ny_scores\n)\n>>>\nprecision\narray([0.5       , 0.66, 0.5       , 1.        , 1.        ])\n>>>\nrecall\narray([1. , 1. , 0.5, 0.5, 0. ])\n>>>\nthreshold\narray([0.1 , 0.35, 0.4 , 0.8 ])\n>>>\naverage_precision_score\n(\ny_true\n,\ny_scores\n)\n0.83\n3.4.4.9.2.\nMulticlass and multilabel classification\n#\nIn a multiclass and multilabel classification task, the notions of precision,\nrecall, and F-measures can be applied to each label independently.\nThere are a few ways to combine results across labels,\nspecified by the\naverage\nargument to the\naverage_precision_score\n,\nf1_score\n,\nfbeta_score\n,\nprecision_recall_fscore_support\n,\nprecision_score\nand\nrecall_score\nfunctions, as described\nabove\n.\nNote the following behaviors when averaging:\nIf all labels are included, “micro”-averaging in a multiclass setting will produce\nprecision, recall and\n\\(F\\)\nthat are all identical to accuracy.\n“weighted” averaging may produce a F-score that is not between precision and recall.\n“macro” averaging for F-measures is calculated as the arithmetic mean over\nper-label/class F-measures, not the harmonic mean over the arithmetic precision and\nrecall means. Both calculations can be seen in the literature but are not equivalent,\nsee\n[OB2019]\nfor details.\nTo make this more explicit, consider the following notation:\n\\(y\\)\nthe set of\ntrue\n\\((sample, label)\\)\npairs\n\\(\\hat{y}\\)\nthe set of\npredicted\n\\((sample, label)\\)\npairs\n\\(L\\)\nthe set of labels\n\\(S\\)\nthe set of samples\n\\(y_s\\)\nthe subset of\n\\(y\\)\nwith sample\n\\(s\\)\n,\ni.e.\n\\(y_s := \\left\\{(s', l) \\in y | s' = s\\right\\}\\)\n\\(y_l\\)\nthe subset of\n\\(y\\)\nwith label\n\\(l\\)\nsimilarly,\n\\(\\hat{y}_s\\)\nand\n\\(\\hat{y}_l\\)\nare subsets of\n\\(\\hat{y}\\)\n\\(P(A, B) := \\frac{\\left| A \\cap B \\right|}{\\left|B\\right|}\\)\nfor some\nsets\n\\(A\\)\nand\n\\(B\\)\n\\(R(A, B) := \\frac{\\left| A \\cap B \\right|}{\\left|A\\right|}\\)\n(Conventions vary on handling\n\\(A = \\emptyset\\)\n; this implementation uses\n\\(R(A, B):=0\\)\n, and similar for\n\\(P\\)\n.)\n\\(F_\\beta(A, B) := \\left(1 + \\beta^2\\right) \\frac{P(A, B) \\times R(A, B)}{\\beta^2 P(A, B) + R(A, B)}\\)\nThen the metrics are defined as:\naverage\nPrecision\nRecall\nF_beta\n\"micro\"\n\\(P(y, \\hat{y})\\)\n\\(R(y, \\hat{y})\\)\n\\(F_\\beta(y, \\hat{y})\\)\n\"samples\"\n\\(\\frac{1}{\\left|S\\right|} \\sum_{s \\in S} P(y_s, \\hat{y}_s)\\)\n\\(\\frac{1}{\\left|S\\right|} \\sum_{s \\in S} R(y_s, \\hat{y}_s)\\)\n\\(\\frac{1}{\\left|S\\right|} \\sum_{s \\in S} F_\\beta(y_s, \\hat{y}_s)\\)\n\"macro\"\n\\(\\frac{1}{\\left|L\\right|} \\sum_{l \\in L} P(y_l, \\hat{y}_l)\\)\n\\(\\frac{1}{\\left|L\\right|} \\sum_{l \\in L} R(y_l, \\hat{y}_l)\\)\n\\(\\frac{1}{\\left|L\\right|} \\sum_{l \\in L} F_\\beta(y_l, \\hat{y}_l)\\)\n\"weighted\"\n\\(\\frac{1}{\\sum_{l \\in L} \\left|y_l\\right|} \\sum_{l \\in L} \\left|y_l\\right| P(y_l, \\hat{y}_l)\\)\n\\(\\frac{1}{\\sum_{l \\in L} \\left|y_l\\right|} \\sum_{l \\in L} \\left|y_l\\right| R(y_l, \\hat{y}_l)\\)\n\\(\\frac{1}{\\sum_{l \\in L} \\left|y_l\\right|} \\sum_{l \\in L} \\left|y_l\\right| F_\\beta(y_l, \\hat{y}_l)\\)\nNone\n\\(\\langle P(y_l, \\hat{y}_l) | l \\in L \\rangle\\)\n\\(\\langle R(y_l, \\hat{y}_l) | l \\in L \\rangle\\)\n\\(\\langle F_\\beta(y_l, \\hat{y}_l) | l \\in L \\rangle\\)\n>>>\nfrom\nsklearn\nimport\nmetrics\n>>>\ny_true\n=\n[\n0\n,\n1\n,\n2\n,\n0\n,\n1\n,\n2\n]\n>>>\ny_pred\n=\n[\n0\n,\n2\n,\n1\n,\n0\n,\n0\n,\n1\n]\n>>>\nmetrics\n.\nprecision_score\n(\ny_true\n,\ny_pred\n,\naverage\n=\n'macro'\n)\n0.22\n>>>\nmetrics\n.\nrecall_score\n(\ny_true\n,\ny_pred\n,\naverage\n=\n'micro'\n)\n0.33\n>>>\nmetrics\n.\nf1_score\n(\ny_true\n,\ny_pred\n,\naverage\n=\n'weighted'\n)\n0.267\n>>>\nmetrics\n.\nfbeta_score\n(\ny_true\n,\ny_pred\n,\naverage\n=\n'macro'\n,\nbeta\n=\n0.5\n)\n0.238\n>>>\nmetrics\n.\nprecision_recall_fscore_support\n(\ny_true\n,\ny_pred\n,\nbeta\n=\n0.5\n,\naverage\n=\nNone\n)\n(array([0.667, 0., 0.]), array([1., 0., 0.]), array([0.714, 0., 0.]), array([2, 2, 2]))\nFor multiclass classification with a “negative class”, it is possible to exclude some labels:\n>>>\nmetrics\n.\nrecall_score\n(\ny_true\n,\ny_pred\n,\nlabels\n=\n[\n1\n,\n2\n],\naverage\n=\n'micro'\n)\n...\n# excluding 0, no labels were correctly recalled\n0.0\nSimilarly, labels not present in the data sample may be accounted for in macro-averaging.\n>>>\nmetrics\n.\nprecision_score\n(\ny_true\n,\ny_pred\n,\nlabels\n=\n[\n0\n,\n1\n,\n2\n,\n3\n],\naverage\n=\n'macro'\n)\n0.166\nReferences\n[\nOB2019\n]\nOpitz, J., & Burst, S. (2019). “Macro f1 and macro f1.”\n3.4.4.10.\nJaccard similarity coefficient score\n#\nThe\njaccard_score\nfunction computes the average of\nJaccard similarity\ncoefficients\n, also called the\nJaccard index, between pairs of label sets.\nThe Jaccard similarity coefficient with a ground truth label set\n\\(y\\)\nand\npredicted label set\n\\(\\hat{y}\\)\n, is defined as\n\\[J(y, \\hat{y}) = \\frac{|y \\cap \\hat{y}|}{|y \\cup \\hat{y}|}.\\]\nThe\njaccard_score\n(like\nprecision_recall_fscore_support\n) applies\nnatively to binary targets. By computing it set-wise it can be extended to apply\nto multilabel and multiclass through the use of\naverage\n(see\nabove\n).\nIn the binary case:\n>>>\nimport\nnumpy\nas\nnp\n>>>\nfrom\nsklearn.metrics\nimport\njaccard_score\n>>>\ny_true\n=\nnp\n.\narray\n([[\n0\n,\n1\n,\n1\n],\n...\n[\n1\n,\n1\n,\n0\n]])\n>>>\ny_pred\n=\nnp\n.\narray\n([[\n1\n,\n1\n,\n1\n],\n...\n[\n1\n,\n0\n,\n0\n]])\n>>>\njaccard_score\n(\ny_true\n[\n0\n],\ny_pred\n[\n0\n])\n0.6666\nIn the 2D comparison case (e.g. image similarity):\n>>>\njaccard_score\n(\ny_true\n,\ny_pred\n,\naverage\n=\n\"micro\"\n)\n0.6\nIn the multilabel case with binary label indicators:\n>>>\njaccard_score\n(\ny_true\n,\ny_pred\n,\naverage\n=\n'samples'\n)\n0.5833\n>>>\njaccard_score\n(\ny_true\n,\ny_pred\n,\naverage\n=\n'macro'\n)\n0.6666\n>>>\njaccard_score\n(\ny_true\n,\ny_pred\n,\naverage\n=\nNone\n)\narray([0.5, 0.5, 1. ])\nMulticlass problems are binarized and treated like the corresponding\nmultilabel problem:\n>>>\ny_pred\n=\n[\n0\n,\n2\n,\n1\n,\n2\n]\n>>>\ny_true\n=\n[\n0\n,\n1\n,\n2\n,\n2\n]\n>>>\njaccard_score\n(\ny_true\n,\ny_pred\n,\naverage\n=\nNone\n)\narray([1. , 0. , 0.33])\n>>>\njaccard_score\n(\ny_true\n,\ny_pred\n,\naverage\n=\n'macro'\n)\n0.44\n>>>\njaccard_score\n(\ny_true\n,\ny_pred\n,\naverage\n=\n'micro'\n)\n0.33\n3.4.4.11.\nHinge loss\n#\nThe\nhinge_loss\nfunction computes the average distance between\nthe model and the data using\nhinge loss\n, a one-sided metric\nthat considers only prediction errors. (Hinge\nloss is used in maximal margin classifiers such as support vector machines.)\nIf the true label\n\\(y_i\\)\nof a binary classification task is encoded as\n\\(y_i=\\left\\{-1, +1\\right\\}\\)\nfor every sample\n\\(i\\)\n; and\n\\(w_i\\)\nis the corresponding predicted decision (an array of shape (\nn_samples\n,) as\noutput by the\ndecision_function\nmethod), then the hinge loss is defined as:\n\\[L_\\text{Hinge}(y, w) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples}-1} \\max\\left\\{1 - w_i y_i, 0\\right\\}\\]\nIf there are more than two labels,\nhinge_loss\nuses a multiclass variant\ndue to Crammer & Singer.\nHere\nis\nthe paper describing it.\nIn this case the predicted decision is an array of shape (\nn_samples\n,\nn_labels\n). If\n\\(w_{i, y_i}\\)\nis the predicted decision for the true label\n\\(y_i\\)\nof the\n\\(i\\)\n-th sample; and\n\\(\\hat{w}_{i, y_i} = \\max\\left\\{w_{i, y_j}~|~y_j \\ne y_i \\right\\}\\)\nis the maximum of the\npredicted decisions for all the other labels, then the multi-class hinge loss\nis defined by:\n\\[L_\\text{Hinge}(y, w) = \\frac{1}{n_\\text{samples}}\n\\sum_{i=0}^{n_\\text{samples}-1} \\max\\left\\{1 + \\hat{w}_{i, y_i}\n- w_{i, y_i}, 0\\right\\}\\]\nHere is a small example demonstrating the use of the\nhinge_loss\nfunction\nwith a svm classifier in a binary class problem:\n>>>\nfrom\nsklearn\nimport\nsvm\n>>>\nfrom\nsklearn.metrics\nimport\nhinge_loss\n>>>\nX\n=\n[[\n0\n],\n[\n1\n]]\n>>>\ny\n=\n[\n-\n1\n,\n1\n]\n>>>\nest\n=\nsvm\n.\nLinearSVC\n(\nrandom_state\n=\n0\n)\n>>>\nest\n.\nfit\n(\nX\n,\ny\n)\nLinearSVC(random_state=0)\n>>>\npred_decision\n=\nest\n.\ndecision_function\n([[\n-\n2\n],\n[\n3\n],\n[\n0.5\n]])\n>>>\npred_decision\narray([-2.18,  2.36,  0.09])\n>>>\nhinge_loss\n([\n-\n1\n,\n1\n,\n1\n],\npred_decision\n)\n0.3\nHere is an example demonstrating the use of the\nhinge_loss\nfunction\nwith a svm classifier in a multiclass problem:\n>>>\nX\n=\nnp\n.\narray\n([[\n0\n],\n[\n1\n],\n[\n2\n],\n[\n3\n]])\n>>>\nY\n=\nnp\n.\narray\n([\n0\n,\n1\n,\n2\n,\n3\n])\n>>>\nlabels\n=\nnp\n.\narray\n([\n0\n,\n1\n,\n2\n,\n3\n])\n>>>\nest\n=\nsvm\n.\nLinearSVC\n()\n>>>\nest\n.\nfit\n(\nX\n,\nY\n)\nLinearSVC()\n>>>\npred_decision\n=\nest\n.\ndecision_function\n([[\n-\n1\n],\n[\n2\n],\n[\n3\n]])\n>>>\ny_true\n=\n[\n0\n,\n2\n,\n3\n]\n>>>\nhinge_loss\n(\ny_true\n,\npred_decision\n,\nlabels\n=\nlabels\n)\n0.56\n3.4.4.12.\nLog loss\n#\nLog loss, also called logistic regression loss or\ncross-entropy loss, is defined on probability estimates.  It is\ncommonly used in (multinomial) logistic regression and neural networks, as well\nas in some variants of expectation-maximization, and can be used to evaluate the\nprobability outputs (\npredict_proba\n) of a classifier instead of its\ndiscrete predictions.\nFor binary classification with a true label\n\\(y \\in \\{0,1\\}\\)\nand a probability estimate\n\\(\\hat{p} \\approx \\operatorname{Pr}(y = 1)\\)\n,\nthe log loss per sample is the negative log-likelihood\nof the classifier given the true label:\n\\[L_{\\log}(y, \\hat{p}) = -\\log \\operatorname{Pr}(y|\\hat{p}) = -(y \\log (\\hat{p}) + (1 - y) \\log (1 - \\hat{p}))\\]\nThis extends to the multiclass case as follows.\nLet the true labels for a set of samples\nbe encoded as a 1-of-K binary indicator matrix\n\\(Y\\)\n,\ni.e.,\n\\(y_{i,k} = 1\\)\nif sample\n\\(i\\)\nhas label\n\\(k\\)\ntaken from a set of\n\\(K\\)\nlabels.\nLet\n\\(\\hat{P}\\)\nbe a matrix of probability estimates,\nwith elements\n\\(\\hat{p}_{i,k} \\approx \\operatorname{Pr}(y_{i,k} = 1)\\)\n.\nThen the log loss of the whole set is\n\\[L_{\\log}(Y, \\hat{P}) = -\\log \\operatorname{Pr}(Y|\\hat{P}) = - \\frac{1}{N} \\sum_{i=0}^{N-1} \\sum_{k=0}^{K-1} y_{i,k} \\log \\hat{p}_{i,k}\\]\nTo see how this generalizes the binary log loss given above,\nnote that in the binary case,\n\\(\\hat{p}_{i,0} = 1 - \\hat{p}_{i,1}\\)\nand\n\\(y_{i,0} = 1 - y_{i,1}\\)\n,\nso expanding the inner sum over\n\\(y_{i,k} \\in \\{0,1\\}\\)\ngives the binary log loss.\nThe\nlog_loss\nfunction computes log loss given a list of ground-truth\nlabels and a probability matrix, as returned by an estimator’s\npredict_proba\nmethod.\n>>>\nfrom\nsklearn.metrics\nimport\nlog_loss\n>>>\ny_true\n=\n[\n0\n,\n0\n,\n1\n,\n1\n]\n>>>\ny_pred\n=\n[[\n.9\n,\n.1\n],\n[\n.8\n,\n.2\n],\n[\n.3\n,\n.7\n],\n[\n.01\n,\n.99\n]]\n>>>\nlog_loss\n(\ny_true\n,\ny_pred\n)\n0.1738\nThe first\n[.9,\n.1]\nin\ny_pred\ndenotes 90% probability that the first\nsample has label 0.  The log loss is non-negative.\n3.4.4.13.\nMatthews correlation coefficient\n#\nThe\nmatthews_corrcoef\nfunction computes the\nMatthew’s correlation coefficient (MCC)\nfor binary classes.  Quoting Wikipedia:\n“The Matthews correlation coefficient is used in machine learning as a\nmeasure of the quality of binary (two-class) classifications. It takes\ninto account true and false positives and negatives and is generally\nregarded as a balanced measure which can be used even if the classes are\nof very different sizes. The MCC is in essence a correlation coefficient\nvalue between -1 and +1. A coefficient of +1 represents a perfect\nprediction, 0 an average random prediction and -1 an inverse prediction.\nThe statistic is also known as the phi coefficient.”\nIn the binary (two-class) case,\n\\(tp\\)\n,\n\\(tn\\)\n,\n\\(fp\\)\nand\n\\(fn\\)\nare respectively the number of true positives, true negatives, false\npositives and false negatives, the MCC is defined as\n\\[MCC = \\frac{tp \\times tn - fp \\times fn}{\\sqrt{(tp + fp)(tp + fn)(tn + fp)(tn + fn)}}.\\]\nIn the multiclass case, the Matthews correlation coefficient can be\ndefined\nin terms of a\nconfusion_matrix\n\\(C\\)\nfor\n\\(K\\)\nclasses.  To simplify the\ndefinition consider the following intermediate variables:\n\\(t_k=\\sum_{i}^{K} C_{ik}\\)\nthe number of times class\n\\(k\\)\ntruly occurred,\n\\(p_k=\\sum_{i}^{K} C_{ki}\\)\nthe number of times class\n\\(k\\)\nwas predicted,\n\\(c=\\sum_{k}^{K} C_{kk}\\)\nthe total number of samples correctly predicted,\n\\(s=\\sum_{i}^{K} \\sum_{j}^{K} C_{ij}\\)\nthe total number of samples.\nThen the multiclass MCC is defined as:\n\\[MCC = \\frac{\n    c \\times s - \\sum_{k}^{K} p_k \\times t_k\n}{\\sqrt{\n    (s^2 - \\sum_{k}^{K} p_k^2) \\times\n    (s^2 - \\sum_{k}^{K} t_k^2)\n}}\\]\nWhen there are more than two labels, the value of the MCC will no longer range\nbetween -1 and +1. Instead the minimum value will be somewhere between -1 and 0\ndepending on the number and distribution of ground truth labels. The maximum\nvalue is always +1.\nFor additional information, see\n[WikipediaMCC2021]\n.\nHere is a small example illustrating the usage of the\nmatthews_corrcoef\nfunction:\n>>>\nfrom\nsklearn.metrics\nimport\nmatthews_corrcoef\n>>>\ny_true\n=\n[\n+\n1\n,\n+\n1\n,\n+\n1\n,\n-\n1\n]\n>>>\ny_pred\n=\n[\n+\n1\n,\n-\n1\n,\n+\n1\n,\n+\n1\n]\n>>>\nmatthews_corrcoef\n(\ny_true\n,\ny_pred\n)\n-0.33\nReferences\n[\nWikipediaMCC2021\n]\nWikipedia contributors. Phi coefficient.\nWikipedia, The Free Encyclopedia. April 21, 2021, 12:21 CEST.\nAvailable at:\nhttps://en.wikipedia.org/wiki/Phi_coefficient\nAccessed April 21, 2021.\n3.4.4.14.\nMulti-label confusion matrix\n#\nThe\nmultilabel_confusion_matrix\nfunction computes class-wise (default)\nor sample-wise (samplewise=True) multilabel confusion matrix to evaluate\nthe accuracy of a classification. multilabel_confusion_matrix also treats\nmulticlass data as if it were multilabel, as this is a transformation commonly\napplied to evaluate multiclass problems with binary classification metrics\n(such as precision, recall, etc.).\nWhen calculating class-wise multilabel confusion matrix\n\\(C\\)\n, the\ncount of true negatives for class\n\\(i\\)\nis\n\\(C_{i,0,0}\\)\n, false\nnegatives is\n\\(C_{i,1,0}\\)\n, true positives is\n\\(C_{i,1,1}\\)\nand false positives is\n\\(C_{i,0,1}\\)\n.\nHere is an example demonstrating the use of the\nmultilabel_confusion_matrix\nfunction with\nmultilabel indicator matrix\ninput:\n>>>\nimport\nnumpy\nas\nnp\n>>>\nfrom\nsklearn.metrics\nimport\nmultilabel_confusion_matrix\n>>>\ny_true\n=\nnp\n.\narray\n([[\n1\n,\n0\n,\n1\n],\n...\n[\n0\n,\n1\n,\n0\n]])\n>>>\ny_pred\n=\nnp\n.\narray\n([[\n1\n,\n0\n,\n0\n],\n...\n[\n0\n,\n1\n,\n1\n]])\n>>>\nmultilabel_confusion_matrix\n(\ny_true\n,\ny_pred\n)\narray([[[1, 0],\n[0, 1]],\n[[1, 0],\n[0, 1]],\n[[0, 1],\n[1, 0]]])\nOr a confusion matrix can be constructed for each sample’s labels:\n>>>\nmultilabel_confusion_matrix\n(\ny_true\n,\ny_pred\n,\nsamplewise\n=\nTrue\n)\narray([[[1, 0],\n[1, 1]],\n[[1, 1],\n[0, 1]]])\nHere is an example demonstrating the use of the\nmultilabel_confusion_matrix\nfunction with\nmulticlass\ninput:\n>>>\ny_true\n=\n[\n\"cat\"\n,\n\"ant\"\n,\n\"cat\"\n,\n\"cat\"\n,\n\"ant\"\n,\n\"bird\"\n]\n>>>\ny_pred\n=\n[\n\"ant\"\n,\n\"ant\"\n,\n\"cat\"\n,\n\"cat\"\n,\n\"ant\"\n,\n\"cat\"\n]\n>>>\nmultilabel_confusion_matrix\n(\ny_true\n,\ny_pred\n,\n...\nlabels\n=\n[\n\"ant\"\n,\n\"bird\"\n,\n\"cat\"\n])\narray([[[3, 1],\n[0, 2]],\n[[5, 0],\n[1, 0]],\n[[2, 1],\n[1, 2]]])\nHere are some examples demonstrating the use of the\nmultilabel_confusion_matrix\nfunction to calculate recall\n(or sensitivity), specificity, fall out and miss rate for each class in a\nproblem with multilabel indicator matrix input.\nCalculating\nrecall\n(also called the true positive rate or the sensitivity) for each class:\n>>>\ny_true\n=\nnp\n.\narray\n([[\n0\n,\n0\n,\n1\n],\n...\n[\n0\n,\n1\n,\n0\n],\n...\n[\n1\n,\n1\n,\n0\n]])\n>>>\ny_pred\n=\nnp\n.\narray\n([[\n0\n,\n1\n,\n0\n],\n...\n[\n0\n,\n0\n,\n1\n],\n...\n[\n1\n,\n1\n,\n0\n]])\n>>>\nmcm\n=\nmultilabel_confusion_matrix\n(\ny_true\n,\ny_pred\n)\n>>>\ntn\n=\nmcm\n[:,\n0\n,\n0\n]\n>>>\ntp\n=\nmcm\n[:,\n1\n,\n1\n]\n>>>\nfn\n=\nmcm\n[:,\n1\n,\n0\n]\n>>>\nfp\n=\nmcm\n[:,\n0\n,\n1\n]\n>>>\ntp\n/\n(\ntp\n+\nfn\n)\narray([1. , 0.5, 0. ])\nCalculating\nspecificity\n(also called the true negative rate) for each class:\n>>>\ntn\n/\n(\ntn\n+\nfp\n)\narray([1. , 0. , 0.5])\nCalculating\nfall out\n(also called the false positive rate) for each class:\n>>>\nfp\n/\n(\nfp\n+\ntn\n)\narray([0. , 1. , 0.5])\nCalculating\nmiss rate\n(also called the false negative rate) for each class:\n>>>\nfn\n/\n(\nfn\n+\ntp\n)\narray([0. , 0.5, 1. ])\n3.4.4.15.\nReceiver operating characteristic (ROC)\n#\nThe function\nroc_curve\ncomputes the\nreceiver operating characteristic curve, or ROC curve\n.\nQuoting Wikipedia :\n“A receiver operating characteristic (ROC), or simply ROC curve, is a\ngraphical plot which illustrates the performance of a binary classifier\nsystem as its discrimination threshold is varied. It is created by plotting\nthe fraction of true positives out of the positives (TPR = true positive\nrate) vs. the fraction of false positives out of the negatives (FPR = false\npositive rate), at various threshold settings. TPR is also known as\nsensitivity, and FPR is one minus the specificity or true negative rate.”\nThis function requires the true binary value and the target scores, which can\neither be probability estimates of the positive class, confidence values, or\nbinary decisions. Here is a small example of how to use the\nroc_curve\nfunction:\n>>>\nimport\nnumpy\nas\nnp\n>>>\nfrom\nsklearn.metrics\nimport\nroc_curve\n>>>\ny\n=\nnp\n.\narray\n([\n1\n,\n1\n,\n2\n,\n2\n])\n>>>\nscores\n=\nnp\n.\narray\n([\n0.1\n,\n0.4\n,\n0.35\n,\n0.8\n])\n>>>\nfpr\n,\ntpr\n,\nthresholds\n=\nroc_curve\n(\ny\n,\nscores\n,\npos_label\n=\n2\n)\n>>>\nfpr\narray([0. , 0. , 0.5, 0.5, 1. ])\n>>>\ntpr\narray([0. , 0.5, 0.5, 1. , 1. ])\n>>>\nthresholds\narray([ inf, 0.8 , 0.4 , 0.35, 0.1 ])\nCompared to metrics such as the subset accuracy, the Hamming loss, or the\nF1 score, ROC doesn’t require optimizing a threshold for each label.\nThe\nroc_auc_score\nfunction, denoted by ROC-AUC or AUROC, computes the\narea under the ROC curve. By doing so, the curve information is summarized in\none number.\nThe following figure shows the ROC curve and ROC-AUC score for a classifier\naimed to distinguish the virginica flower from the rest of the species in the\nIris plants dataset\n:\nFor more information see the\nWikipedia article on AUC\n.\n3.4.4.15.1.\nBinary case\n#\nIn the\nbinary case\n, you can either provide the probability estimates, using\nthe\nclassifier.predict_proba()\nmethod, or the non-thresholded decision values\ngiven by the\nclassifier.decision_function()\nmethod. In the case of providing\nthe probability estimates, the probability of the class with the\n“greater label” should be provided. The “greater label” corresponds to\nclassifier.classes_[1]\nand thus\nclassifier.predict_proba(X)[:,\n1]\n.\nTherefore, the\ny_score\nparameter is of size (n_samples,).\n>>>\nfrom\nsklearn.datasets\nimport\nload_breast_cancer\n>>>\nfrom\nsklearn.linear_model\nimport\nLogisticRegression\n>>>\nfrom\nsklearn.metrics\nimport\nroc_auc_score\n>>>\nX\n,\ny\n=\nload_breast_cancer\n(\nreturn_X_y\n=\nTrue\n)\n>>>\nclf\n=\nLogisticRegression\n()\n.\nfit\n(\nX\n,\ny\n)\n>>>\nclf\n.\nclasses_\narray([0, 1])\nWe can use the probability estimates corresponding to\nclf.classes_[1]\n.\n>>>\ny_score\n=\nclf\n.\npredict_proba\n(\nX\n)[:,\n1\n]\n>>>\nroc_auc_score\n(\ny\n,\ny_score\n)\n0.99\nOtherwise, we can use the non-thresholded decision values\n>>>\nroc_auc_score\n(\ny\n,\nclf\n.\ndecision_function\n(\nX\n))\n0.99\n3.4.4.15.2.\nMulti-class case\n#\nThe\nroc_auc_score\nfunction can also be used in\nmulti-class\nclassification\n. Two averaging strategies are currently supported: the\none-vs-one algorithm computes the average of the pairwise ROC AUC scores, and\nthe one-vs-rest algorithm computes the average of the ROC AUC scores for each\nclass against all other classes. In both cases, the predicted labels are\nprovided in an array with values from 0 to\nn_classes\n, and the scores\ncorrespond to the probability estimates that a sample belongs to a particular\nclass. The OvO and OvR algorithms support weighting uniformly\n(\naverage='macro'\n) and by prevalence (\naverage='weighted'\n).\nOne-vs-one Algorithm\n#\nComputes the average AUC of all possible pairwise\ncombinations of classes.\n[HT2001]\ndefines a multiclass AUC metric weighted\nuniformly:\n\\[\\frac{1}{c(c-1)}\\sum_{j=1}^{c}\\sum_{k > j}^c (\\text{AUC}(j | k) +\n\\text{AUC}(k | j))\\]\nwhere\n\\(c\\)\nis the number of classes and\n\\(\\text{AUC}(j | k)\\)\nis the\nAUC with class\n\\(j\\)\nas the positive class and class\n\\(k\\)\nas the\nnegative class. In general,\n\\(\\text{AUC}(j | k) \\neq \\text{AUC}(k | j))\\)\nin the multiclass\ncase. This algorithm is used by setting the keyword argument\nmulticlass\nto\n'ovo'\nand\naverage\nto\n'macro'\n.\nThe\n[HT2001]\nmulticlass AUC metric can be extended to be weighted by the\nprevalence:\n\\[\\frac{1}{c(c-1)}\\sum_{j=1}^{c}\\sum_{k > j}^c p(j \\cup k)(\n\\text{AUC}(j | k) + \\text{AUC}(k | j))\\]\nwhere\n\\(c\\)\nis the number of classes. This algorithm is used by setting\nthe keyword argument\nmulticlass\nto\n'ovo'\nand\naverage\nto\n'weighted'\n. The\n'weighted'\noption returns a prevalence-weighted average\nas described in\n[FC2009]\n.\nOne-vs-rest Algorithm\n#\nComputes the AUC of each class against the rest\n[PD2000]\n. The algorithm is functionally the same as the multilabel case. To\nenable this algorithm set the keyword argument\nmulticlass\nto\n'ovr'\n.\nAdditionally to\n'macro'\n[F2006]\nand\n'weighted'\n[F2001]\naveraging, OvR\nsupports\n'micro'\naveraging.\nIn applications where a high false positive rate is not tolerable the parameter\nmax_fpr\nof\nroc_auc_score\ncan be used to summarize the ROC curve up\nto the given limit.\nThe following figure shows the micro-averaged ROC curve and its corresponding\nROC-AUC score for a classifier aimed to distinguish the different species in\nthe\nIris plants dataset\n:\n3.4.4.15.3.\nMulti-label case\n#\nIn\nmulti-label classification\n, the\nroc_auc_score\nfunction is\nextended by averaging over the labels as\nabove\n. In this case,\nyou should provide a\ny_score\nof shape\n(n_samples,\nn_classes)\n. Thus, when\nusing the probability estimates, one needs to select the probability of the\nclass with the greater label for each output.\n>>>\nfrom\nsklearn.datasets\nimport\nmake_multilabel_classification\n>>>\nfrom\nsklearn.multioutput\nimport\nMultiOutputClassifier\n>>>\nX\n,\ny\n=\nmake_multilabel_classification\n(\nrandom_state\n=\n0\n)\n>>>\ninner_clf\n=\nLogisticRegression\n(\nrandom_state\n=\n0\n)\n>>>\nclf\n=\nMultiOutputClassifier\n(\ninner_clf\n)\n.\nfit\n(\nX\n,\ny\n)\n>>>\ny_score\n=\nnp\n.\ntranspose\n([\ny_pred\n[:,\n1\n]\nfor\ny_pred\nin\nclf\n.\npredict_proba\n(\nX\n)])\n>>>\nroc_auc_score\n(\ny\n,\ny_score\n,\naverage\n=\nNone\n)\narray([0.828, 0.851, 0.94, 0.87, 0.95])\nAnd the decision values do not require such processing.\n>>>\nfrom\nsklearn.linear_model\nimport\nRidgeClassifierCV\n>>>\nclf\n=\nRidgeClassifierCV\n()\n.\nfit\n(\nX\n,\ny\n)\n>>>\ny_score\n=\nclf\n.\ndecision_function\n(\nX\n)\n>>>\nroc_auc_score\n(\ny\n,\ny_score\n,\naverage\n=\nNone\n)\narray([0.82, 0.85, 0.93, 0.87, 0.94])\nExamples\nSee\nMulticlass Receiver Operating Characteristic (ROC)\nfor an example of\nusing ROC to evaluate the quality of the output of a classifier.\nSee\nReceiver Operating Characteristic (ROC) with cross validation\nfor an\nexample of using ROC to evaluate classifier output quality, using cross-validation.\nSee\nSpecies distribution modeling\nfor an example of using ROC to model species distribution.\nReferences\n[\nHT2001\n]\n(\n1\n,\n2\n)\nHand, D.J. and Till, R.J., (2001).\nA simple generalisation\nof the area under the ROC curve for multiple class classification problems.\nMachine learning, 45(2), pp. 171-186.\n[\nFC2009\n]\nFerri, Cèsar & Hernandez-Orallo, Jose & Modroiu, R. (2009).\nAn Experimental Comparison of Performance Measures for Classification.\nPattern Recognition Letters. 30. 27-38.\n[\nPD2000\n]\nProvost, F., Domingos, P. (2000).\nWell-trained PETs: Improving\nprobability estimation trees\n(Section 6.2), CeDER Working Paper #IS-00-04, Stern School of Business,\nNew York University.\n[\nF2006\n]\nFawcett, T., 2006.\nAn introduction to ROC analysis.\nPattern Recognition Letters, 27(8), pp. 861-874.\n[\nF2001\n]\nFawcett, T., 2001.\nUsing rule sets to maximize\nROC performance\nIn Data Mining, 2001.\nProceedings IEEE International Conference, pp. 131-138.\n3.4.4.16.\nDetection error tradeoff (DET)\n#\nThe function\ndet_curve\ncomputes the\ndetection error tradeoff curve (DET) curve\n[WikipediaDET2017]\n.\nQuoting Wikipedia:\n“A detection error tradeoff (DET) graph is a graphical plot of error rates\nfor binary classification systems, plotting false reject rate vs. false\naccept rate. The x- and y-axes are scaled non-linearly by their standard\nnormal deviates (or just by logarithmic transformation), yielding tradeoff\ncurves that are more linear than ROC curves, and use most of the image area\nto highlight the differences of importance in the critical operating region.”\nDET curves are a variation of receiver operating characteristic (ROC) curves\nwhere False Negative Rate is plotted on the y-axis instead of True Positive\nRate.\nDET curves are commonly plotted in normal deviate scale by transformation with\n\\(\\phi^{-1}\\)\n(with\n\\(\\phi\\)\nbeing the cumulative distribution\nfunction).\nThe resulting performance curves explicitly visualize the tradeoff of error\ntypes for given classification algorithms.\nSee\n[Martin1997]\nfor examples and further motivation.\nThis figure compares the ROC and DET curves of two example classifiers on the\nsame classification task:\nProperties\n#\nDET curves form a linear curve in normal deviate scale if the detection\nscores are normally (or close-to normally) distributed.\nIt was shown by\n[Navratil2007]\nthat the reverse is not necessarily true and\neven more general distributions are able to produce linear DET curves.\nThe normal deviate scale transformation spreads out the points such that a\ncomparatively larger space of plot is occupied.\nTherefore curves with similar classification performance might be easier to\ndistinguish on a DET plot.\nWith False Negative Rate being “inverse” to True Positive Rate the point\nof perfection for DET curves is the origin (in contrast to the top left\ncorner for ROC curves).\nApplications and limitations\n#\nDET curves are intuitive to read and hence allow quick visual assessment of a\nclassifier’s performance.\nAdditionally DET curves can be consulted for threshold analysis and operating\npoint selection.\nThis is particularly helpful if a comparison of error types is required.\nOn the other hand DET curves do not provide their metric as a single number.\nTherefore for either automated evaluation or comparison to other\nclassification tasks metrics like the derived area under ROC curve might be\nbetter suited.\nExamples\nSee\nDetection error tradeoff (DET) curve\nfor an example comparison between receiver operating characteristic (ROC)\ncurves and Detection error tradeoff (DET) curves.\nReferences\n[\nWikipediaDET2017\n]\nWikipedia contributors. Detection error tradeoff.\nWikipedia, The Free Encyclopedia. September 4, 2017, 23:33 UTC.\nAvailable at:\nhttps://en.wikipedia.org/w/index.php?title=Detection_error_tradeoff&oldid=798982054\n.\nAccessed February 19, 2018.\n[\nMartin1997\n]\nA. Martin, G. Doddington, T. Kamm, M. Ordowski, and M. Przybocki,\nThe DET Curve in Assessment of Detection Task Performance\n, NIST 1997.\n[\nNavratil2007\n]\nJ. Navratil and D. Klusacek,\n“On Linear DETs”\n,\n2007 IEEE International Conference on Acoustics,\nSpeech and Signal Processing - ICASSP ‘07, Honolulu,\nHI, 2007, pp. IV-229-IV-232.\n3.4.4.17.\nZero one loss\n#\nThe\nzero_one_loss\nfunction computes the sum or the average of the 0-1\nclassification loss (\n\\(L_{0-1}\\)\n) over\n\\(n_{\\text{samples}}\\)\n. By\ndefault, the function normalizes over the sample. To get the sum of the\n\\(L_{0-1}\\)\n, set\nnormalize\nto\nFalse\n.\nIn multilabel classification, the\nzero_one_loss\nscores a subset as\none if its labels strictly match the predictions, and as a zero if there\nare any errors.  By default, the function returns the percentage of imperfectly\npredicted subsets.  To get the count of such subsets instead, set\nnormalize\nto\nFalse\n.\nIf\n\\(\\hat{y}_i\\)\nis the predicted value of\nthe\n\\(i\\)\n-th sample and\n\\(y_i\\)\nis the corresponding true value,\nthen the 0-1 loss\n\\(L_{0-1}\\)\nis defined as:\n\\[L_{0-1}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples}-1} 1(\\hat{y}_i \\not= y_i)\\]\nwhere\n\\(1(x)\\)\nis the\nindicator function\n. The zero-one\nloss can also be computed as\n\\(\\text{zero-one loss} = 1 - \\text{accuracy}\\)\n.\n>>>\nfrom\nsklearn.metrics\nimport\nzero_one_loss\n>>>\ny_pred\n=\n[\n1\n,\n2\n,\n3\n,\n4\n]\n>>>\ny_true\n=\n[\n2\n,\n2\n,\n3\n,\n4\n]\n>>>\nzero_one_loss\n(\ny_true\n,\ny_pred\n)\n0.25\n>>>\nzero_one_loss\n(\ny_true\n,\ny_pred\n,\nnormalize\n=\nFalse\n)\n1.0\nIn the multilabel case with binary label indicators, where the first label\nset [0,1] has an error:\n>>>\nzero_one_loss\n(\nnp\n.\narray\n([[\n0\n,\n1\n],\n[\n1\n,\n1\n]]),\nnp\n.\nones\n((\n2\n,\n2\n)))\n0.5\n>>>\nzero_one_loss\n(\nnp\n.\narray\n([[\n0\n,\n1\n],\n[\n1\n,\n1\n]]),\nnp\n.\nones\n((\n2\n,\n2\n)),\nnormalize\n=\nFalse\n)\n1.0\nExamples\nSee\nRecursive feature elimination with cross-validation\nfor an example of zero one loss usage to perform recursive feature\nelimination with cross-validation.\n3.4.4.18.\nBrier score loss\n#\nThe\nbrier_score_loss\nfunction computes the\nBrier score\nfor binary and multiclass\nprobabilistic predictions and is equivalent to the mean squared error.\nQuoting Wikipedia:\n“The Brier score is a strictly proper scoring rule that measures the accuracy of\nprobabilistic predictions. […] [It] is applicable to tasks in which predictions\nmust assign probabilities to a set of mutually exclusive discrete outcomes or\nclasses.”\nLet the true labels for a set of\n\\(N\\)\ndata points be encoded as a 1-of-K binary\nindicator matrix\n\\(Y\\)\n, i.e.,\n\\(y_{i,k} = 1\\)\nif sample\n\\(i\\)\nhas\nlabel\n\\(k\\)\ntaken from a set of\n\\(K\\)\nlabels. Let\n\\(\\hat{P}\\)\nbe a matrix\nof probability estimates with elements\n\\(\\hat{p}_{i,k} \\approx \\operatorname{Pr}(y_{i,k} = 1)\\)\n.\nFollowing the original definition by\n[Brier1950]\n, the Brier score is given by:\n\\[BS(Y, \\hat{P}) = \\frac{1}{N}\\sum_{i=0}^{N-1}\\sum_{k=0}^{K-1}(y_{i,k} - \\hat{p}_{i,k})^{2}\\]\nThe Brier score lies in the interval\n\\([0, 2]\\)\nand the lower the value the\nbetter the probability estimates are (the mean squared difference is smaller).\nActually, the Brier score is a strictly proper scoring rule, meaning that it\nachieves the best score only when the estimated probabilities equal the\ntrue ones.\nNote that in the binary case, the Brier score is usually divided by two and\nranges between\n\\([0,1]\\)\n. For binary targets\n\\(y_i \\in {0, 1}\\)\nand\nprobability estimates\n\\(\\hat{p}_i  \\approx \\operatorname{Pr}(y_i = 1)\\)\nfor the positive class, the Brier score is then equal to:\n\\[BS(y, \\hat{p}) = \\frac{1}{N} \\sum_{i=0}^{N - 1}(y_i - \\hat{p}_i)^2\\]\nThe\nbrier_score_loss\nfunction computes the Brier score given the\nground-truth labels and predicted probabilities, as returned by an estimator’s\npredict_proba\nmethod. The\nscale_by_half\nparameter controls which of the\ntwo above definitions to follow.\n>>>\nimport\nnumpy\nas\nnp\n>>>\nfrom\nsklearn.metrics\nimport\nbrier_score_loss\n>>>\ny_true\n=\nnp\n.\narray\n([\n0\n,\n1\n,\n1\n,\n0\n])\n>>>\ny_true_categorical\n=\nnp\n.\narray\n([\n\"spam\"\n,\n\"ham\"\n,\n\"ham\"\n,\n\"spam\"\n])\n>>>\ny_prob\n=\nnp\n.\narray\n([\n0.1\n,\n0.9\n,\n0.8\n,\n0.4\n])\n>>>\nbrier_score_loss\n(\ny_true\n,\ny_prob\n)\n0.055\n>>>\nbrier_score_loss\n(\ny_true\n,\n1\n-\ny_prob\n,\npos_label\n=\n0\n)\n0.055\n>>>\nbrier_score_loss\n(\ny_true_categorical\n,\ny_prob\n,\npos_label\n=\n\"ham\"\n)\n0.055\n>>>\nbrier_score_loss\n(\n...\n[\n\"eggs\"\n,\n\"ham\"\n,\n\"spam\"\n],\n...\n[[\n0.8\n,\n0.1\n,\n0.1\n],\n[\n0.2\n,\n0.7\n,\n0.1\n],\n[\n0.2\n,\n0.2\n,\n0.6\n]],\n...\nlabels\n=\n[\n\"eggs\"\n,\n\"ham\"\n,\n\"spam\"\n],\n...\n)\n0.146\nThe Brier score can be used to assess how well a classifier is calibrated.\nHowever, a lower Brier score loss does not always mean a better calibration.\nThis is because, by analogy with the bias-variance decomposition of the mean\nsquared error, the Brier score loss can be decomposed as the sum of calibration\nloss and refinement loss\n[Bella2012]\n. Calibration loss is defined as the mean\nsquared deviation from empirical probabilities derived from the slope of ROC\nsegments. Refinement loss can be defined as the expected optimal loss as\nmeasured by the area under the optimal cost curve. Refinement loss can change\nindependently from calibration loss, thus a lower Brier score loss does not\nnecessarily mean a better calibrated model. “Only when refinement loss remains\nthe same does a lower Brier score loss always mean better calibration”\n[Bella2012]\n,\n[Flach2008]\n.\nExamples\nSee\nProbability calibration of classifiers\nfor an example of Brier score loss usage to perform probability\ncalibration of classifiers.\nReferences\n[\nBrier1950\n]\nG. Brier,\nVerification of forecasts expressed in terms of probability\n,\nMonthly weather review 78.1 (1950)\n[\nBella2012\n]\n(\n1\n,\n2\n)\nBella, Ferri, Hernández-Orallo, and Ramírez-Quintana\n“Calibration of Machine Learning Models”\nin Khosrow-Pour, M. “Machine learning: concepts, methodologies, tools\nand applications.” Hershey, PA: Information Science Reference (2012).\n[\nFlach2008\n]\nFlach, Peter, and Edson Matsubara.\n“On classification, ranking,\nand probability estimation.”\nDagstuhl Seminar Proceedings. Schloss Dagstuhl-Leibniz-Zentrum für Informatik (2008).\n3.4.4.19.\nClass likelihood ratios\n#\nThe\nclass_likelihood_ratios\nfunction computes the\npositive and negative\nlikelihood ratios\n\\(LR_\\pm\\)\nfor binary classes, which can be interpreted as the ratio of\npost-test to pre-test odds as explained below. As a consequence, this metric is\ninvariant w.r.t. the class prevalence (the number of samples in the positive\nclass divided by the total number of samples) and\ncan be extrapolated between\npopulations regardless of any possible class imbalance.\nThe\n\\(LR_\\pm\\)\nmetrics are therefore very useful in settings where the data\navailable to learn and evaluate a classifier is a study population with nearly\nbalanced classes, such as a case-control study, while the target application,\ni.e. the general population, has very low prevalence.\nThe positive likelihood ratio\n\\(LR_+\\)\nis the probability of a classifier to\ncorrectly predict that a sample belongs to the positive class divided by the\nprobability of predicting the positive class for a sample belonging to the\nnegative class:\n\\[LR_+ = \\frac{\\text{PR}(P+|T+)}{\\text{PR}(P+|T-)}.\\]\nThe notation here refers to predicted (\n\\(P\\)\n) or true (\n\\(T\\)\n) label and\nthe sign\n\\(+\\)\nand\n\\(-\\)\nrefer to the positive and negative class,\nrespectively, e.g.\n\\(P+\\)\nstands for “predicted positive”.\nAnalogously, the negative likelihood ratio\n\\(LR_-\\)\nis the probability of a\nsample of the positive class being classified as belonging to the negative class\ndivided by the probability of a sample of the negative class being correctly\nclassified:\n\\[LR_- = \\frac{\\text{PR}(P-|T+)}{\\text{PR}(P-|T-)}.\\]\nFor classifiers above chance\n\\(LR_+\\)\nabove 1\nhigher is better\n, while\n\\(LR_-\\)\nranges from 0 to 1 and\nlower is better\n.\nValues of\n\\(LR_\\pm\\approx 1\\)\ncorrespond to chance level.\nNotice that probabilities differ from counts, for instance\n\\(\\operatorname{PR}(P+|T+)\\)\nis not equal to the number of true positive\ncounts\ntp\n(see\nthe wikipedia page\nfor\nthe actual formulas).\nExamples\nClass Likelihood Ratios to measure classification performance\nInterpretation across varying prevalence\n#\nBoth class likelihood ratios are interpretable in terms of an odds ratio\n(pre-test and post-tests):\n\\[\\text{post-test odds} = \\text{Likelihood ratio} \\times \\text{pre-test odds}.\\]\nOdds are in general related to probabilities via\n\\[\\text{odds} = \\frac{\\text{probability}}{1 - \\text{probability}},\\]\nor equivalently\n\\[\\text{probability} = \\frac{\\text{odds}}{1 + \\text{odds}}.\\]\nOn a given population, the pre-test probability is given by the prevalence. By\nconverting odds to probabilities, the likelihood ratios can be translated into a\nprobability of truly belonging to either class before and after a classifier\nprediction:\n\\[\\text{post-test odds} = \\text{Likelihood ratio} \\times\n\\frac{\\text{pre-test probability}}{1 - \\text{pre-test probability}},\\]\n\\[\\text{post-test probability} = \\frac{\\text{post-test odds}}{1 + \\text{post-test odds}}.\\]\nMathematical divergences\n#\nThe positive likelihood ratio (\nLR+\n) is undefined when\n\\(fp=0\\)\n, meaning the\nclassifier does not misclassify any negative labels as positives. This condition can\neither indicate a perfect identification of all the negative cases or, if there are\nalso no true positive predictions (\n\\(tp=0\\)\n), that the classifier does not predict\nthe positive class at all. In the first case,\nLR+\ncan be interpreted as\nnp.inf\n, in\nthe second case (for instance, with highly imbalanced data) it can be interpreted as\nnp.nan\n.\nThe negative likelihood ratio (\nLR-\n) is undefined when\n\\(tn=0\\)\n. Such\ndivergence is invalid, as\n\\(LR_- > 1.0\\)\nwould indicate an increase in the odds of\na sample belonging to the positive class after being classified as negative, as if the\nact of classifying caused the positive condition. This includes the case of a\nDummyClassifier\nthat always predicts the positive class\n(i.e. when\n\\(tn=fn=0\\)\n).\nBoth class likelihood ratios (\nLR+\nand\nLR-\n) are undefined when\n\\(tp=fn=0\\)\n, which\nmeans that no samples of the positive class were present in the test set. This can\nhappen when cross-validating on highly imbalanced data and also leads to a division by\nzero.\nIf a division by zero occurs and\nraise_warning\nis set to\nTrue\n(default),\nclass_likelihood_ratios\nraises an\nUndefinedMetricWarning\nand returns\nnp.nan\nby default to avoid pollution when averaging over cross-validation folds.\nUsers can set return values in case of a division by zero with the\nreplace_undefined_by\nparam.\nFor a worked-out demonstration of the\nclass_likelihood_ratios\nfunction,\nsee the example below.\nReferences\n#\nWikipedia entry for Likelihood ratios in diagnostic testing\nBrenner, H., & Gefeller, O. (1997).\nVariation of sensitivity, specificity, likelihood ratios and predictive\nvalues with disease prevalence. Statistics in medicine, 16(9), 981-991.\n3.4.4.20.\nD² score for classification\n#\nThe D² score computes the fraction of deviance explained.\nIt is a generalization of R², where the squared error is generalized and replaced\nby a classification deviance of choice\n\\(\\text{dev}(y, \\hat{y})\\)\n(e.g., Log loss). D² is a form of a\nskill score\n.\nIt is calculated as\n\\[D^2(y, \\hat{y}) = 1 - \\frac{\\text{dev}(y, \\hat{y})}{\\text{dev}(y, y_{\\text{null}})} \\,.\\]\nWhere\n\\(y_{\\text{null}}\\)\nis the optimal prediction of an intercept-only model\n(e.g., the per-class proportion of\ny_true\nin the case of the Log loss).\nLike R², the best possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always predicts\n\\(y_{\\text{null}}\\)\n, disregarding the input features, would get a D² score\nof 0.0.\nD2 log loss score\n#\nThe\nd2_log_loss_score\nfunction implements the special case\nof D² with the log loss, see\nLog loss\n, i.e.:\n\\[\\text{dev}(y, \\hat{y}) = \\text{log_loss}(y, \\hat{y}).\\]\nHere are some usage examples of the\nd2_log_loss_score\nfunction:\n>>>\nfrom\nsklearn.metrics\nimport\nd2_log_loss_score\n>>>\ny_true\n=\n[\n1\n,\n1\n,\n2\n,\n3\n]\n>>>\ny_pred\n=\n[\n...\n[\n0.5\n,\n0.25\n,\n0.25\n],\n...\n[\n0.5\n,\n0.25\n,\n0.25\n],\n...\n[\n0.5\n,\n0.25\n,\n0.25\n],\n...\n[\n0.5\n,\n0.25\n,\n0.25\n],\n...\n]\n>>>\nd2_log_loss_score\n(\ny_true\n,\ny_pred\n)\n0.0\n>>>\ny_true\n=\n[\n1\n,\n2\n,\n3\n]\n>>>\ny_pred\n=\n[\n...\n[\n0.98\n,\n0.01\n,\n0.01\n],\n...\n[\n0.01\n,\n0.98\n,\n0.01\n],\n...\n[\n0.01\n,\n0.01\n,\n0.98\n],\n...\n]\n>>>\nd2_log_loss_score\n(\ny_true\n,\ny_pred\n)\n0.981\n>>>\ny_true\n=\n[\n1\n,\n2\n,\n3\n]\n>>>\ny_pred\n=\n[\n...\n[\n0.1\n,\n0.6\n,\n0.3\n],\n...\n[\n0.1\n,\n0.6\n,\n0.3\n],\n...\n[\n0.4\n,\n0.5\n,\n0.1\n],\n...\n]\n>>>\nd2_log_loss_score\n(\ny_true\n,\ny_pred\n)\n-0.552\n3.4.5.\nMultilabel ranking metrics\n#\nIn multilabel learning, each sample can have any number of ground truth labels\nassociated with it. The goal is to give high scores and better rank to\nthe ground truth labels.\n3.4.5.1.\nCoverage error\n#\nThe\ncoverage_error\nfunction computes the average number of labels that\nhave to be included in the final prediction such that all true labels\nare predicted. This is useful if you want to know how many top-scored-labels\nyou have to predict in average without missing any true one. The best value\nof this metric is thus the average number of true labels.\nNote\nOur implementation’s score is 1 greater than the one given in Tsoumakas\net al., 2010. This extends it to handle the degenerate case in which an\ninstance has 0 true labels.\nFormally, given a binary indicator matrix of the ground truth labels\n\\(y \\in \\left\\{0, 1\\right\\}^{n_\\text{samples} \\times n_\\text{labels}}\\)\nand the\nscore associated with each label\n\\(\\hat{f} \\in \\mathbb{R}^{n_\\text{samples} \\times n_\\text{labels}}\\)\n,\nthe coverage is defined as\n\\[coverage(y, \\hat{f}) = \\frac{1}{n_{\\text{samples}}}\n  \\sum_{i=0}^{n_{\\text{samples}} - 1} \\max_{j:y_{ij} = 1} \\text{rank}_{ij}\\]\nwith\n\\(\\text{rank}_{ij} = \\left|\\left\\{k: \\hat{f}_{ik} \\geq \\hat{f}_{ij} \\right\\}\\right|\\)\n.\nGiven the rank definition, ties in\ny_scores\nare broken by giving the\nmaximal rank that would have been assigned to all tied values.\nHere is a small example of usage of this function:\n>>>\nimport\nnumpy\nas\nnp\n>>>\nfrom\nsklearn.metrics\nimport\ncoverage_error\n>>>\ny_true\n=\nnp\n.\narray\n([[\n1\n,\n0\n,\n0\n],\n[\n0\n,\n0\n,\n1\n]])\n>>>\ny_score\n=\nnp\n.\narray\n([[\n0.75\n,\n0.5\n,\n1\n],\n[\n1\n,\n0.2\n,\n0.1\n]])\n>>>\ncoverage_error\n(\ny_true\n,\ny_score\n)\n2.5\n3.4.5.2.\nLabel ranking average precision\n#\nThe\nlabel_ranking_average_precision_score\nfunction\nimplements label ranking average precision (LRAP). This metric is linked to\nthe\naverage_precision_score\nfunction, but is based on the notion of\nlabel ranking instead of precision and recall.\nLabel ranking average precision (LRAP) averages over the samples the answer to\nthe following question: for each ground truth label, what fraction of\nhigher-ranked labels were true labels? This performance measure will be higher\nif you are able to give better rank to the labels associated with each sample.\nThe obtained score is always strictly greater than 0, and the best value is 1.\nIf there is exactly one relevant label per sample, label ranking average\nprecision is equivalent to the\nmean\nreciprocal rank\n.\nFormally, given a binary indicator matrix of the ground truth labels\n\\(y \\in \\left\\{0, 1\\right\\}^{n_\\text{samples} \\times n_\\text{labels}}\\)\nand the score associated with each label\n\\(\\hat{f} \\in \\mathbb{R}^{n_\\text{samples} \\times n_\\text{labels}}\\)\n,\nthe average precision is defined as\n\\[LRAP(y, \\hat{f}) = \\frac{1}{n_{\\text{samples}}}\n  \\sum_{i=0}^{n_{\\text{samples}} - 1} \\frac{1}{||y_i||_0}\n  \\sum_{j:y_{ij} = 1} \\frac{|\\mathcal{L}_{ij}|}{\\text{rank}_{ij}}\\]\nwhere\n\\(\\mathcal{L}_{ij} = \\left\\{k: y_{ik} = 1, \\hat{f}_{ik} \\geq \\hat{f}_{ij} \\right\\}\\)\n,\n\\(\\text{rank}_{ij} = \\left|\\left\\{k: \\hat{f}_{ik} \\geq \\hat{f}_{ij} \\right\\}\\right|\\)\n,\n\\(|\\cdot|\\)\ncomputes the cardinality of the set (i.e., the number of\nelements in the set), and\n\\(||\\cdot||_0\\)\nis the\n\\(\\ell_0\\)\n“norm”\n(which computes the number of nonzero elements in a vector).\nHere is a small example of usage of this function:\n>>>\nimport\nnumpy\nas\nnp\n>>>\nfrom\nsklearn.metrics\nimport\nlabel_ranking_average_precision_score\n>>>\ny_true\n=\nnp\n.\narray\n([[\n1\n,\n0\n,\n0\n],\n[\n0\n,\n0\n,\n1\n]])\n>>>\ny_score\n=\nnp\n.\narray\n([[\n0.75\n,\n0.5\n,\n1\n],\n[\n1\n,\n0.2\n,\n0.1\n]])\n>>>\nlabel_ranking_average_precision_score\n(\ny_true\n,\ny_score\n)\n0.416\n3.4.5.3.\nRanking loss\n#\nThe\nlabel_ranking_loss\nfunction computes the ranking loss which\naverages over the samples the number of label pairs that are incorrectly\nordered, i.e. true labels have a lower score than false labels, weighted by\nthe inverse of the number of ordered pairs of false and true labels.\nThe lowest achievable ranking loss is zero.\nFormally, given a binary indicator matrix of the ground truth labels\n\\(y \\in \\left\\{0, 1\\right\\}^{n_\\text{samples} \\times n_\\text{labels}}\\)\nand the\nscore associated with each label\n\\(\\hat{f} \\in \\mathbb{R}^{n_\\text{samples} \\times n_\\text{labels}}\\)\n,\nthe ranking loss is defined as\n\\[ranking\\_loss(y, \\hat{f}) =  \\frac{1}{n_{\\text{samples}}}\n  \\sum_{i=0}^{n_{\\text{samples}} - 1} \\frac{1}{||y_i||_0(n_\\text{labels} - ||y_i||_0)}\n  \\left|\\left\\{(k, l): \\hat{f}_{ik} \\leq \\hat{f}_{il}, y_{ik} = 1, y_{il} = 0 \\right\\}\\right|\\]\nwhere\n\\(|\\cdot|\\)\ncomputes the cardinality of the set (i.e., the number of\nelements in the set) and\n\\(||\\cdot||_0\\)\nis the\n\\(\\ell_0\\)\n“norm”\n(which computes the number of nonzero elements in a vector).\nHere is a small example of usage of this function:\n>>>\nimport\nnumpy\nas\nnp\n>>>\nfrom\nsklearn.metrics\nimport\nlabel_ranking_loss\n>>>\ny_true\n=\nnp\n.\narray\n([[\n1\n,\n0\n,\n0\n],\n[\n0\n,\n0\n,\n1\n]])\n>>>\ny_score\n=\nnp\n.\narray\n([[\n0.75\n,\n0.5\n,\n1\n],\n[\n1\n,\n0.2\n,\n0.1\n]])\n>>>\nlabel_ranking_loss\n(\ny_true\n,\ny_score\n)\n0.75\n>>>\n# With the following prediction, we have perfect and minimal loss\n>>>\ny_score\n=\nnp\n.\narray\n([[\n1.0\n,\n0.1\n,\n0.2\n],\n[\n0.1\n,\n0.2\n,\n0.9\n]])\n>>>\nlabel_ranking_loss\n(\ny_true\n,\ny_score\n)\n0.0\nReferences\n#\nTsoumakas, G., Katakis, I., & Vlahavas, I. (2010). Mining multi-label data. In\nData mining and knowledge discovery handbook (pp. 667-685). Springer US.\n3.4.5.4.\nNormalized Discounted Cumulative Gain\n#\nDiscounted Cumulative Gain (DCG) and Normalized Discounted Cumulative Gain\n(NDCG) are ranking metrics implemented in\ndcg_score\nand\nndcg_score\n; they compare a predicted order to\nground-truth scores, such as the relevance of answers to a query.\nFrom the Wikipedia page for Discounted Cumulative Gain:\n“Discounted cumulative gain (DCG) is a measure of ranking quality. In\ninformation retrieval, it is often used to measure effectiveness of web search\nengine algorithms or related applications. Using a graded relevance scale of\ndocuments in a search-engine result set, DCG measures the usefulness, or gain,\nof a document based on its position in the result list. The gain is accumulated\nfrom the top of the result list to the bottom, with the gain of each result\ndiscounted at lower ranks.”\nDCG orders the true targets (e.g. relevance of query answers) in the predicted\norder, then multiplies them by a logarithmic decay and sums the result. The sum\ncan be truncated after the first\n\\(K\\)\nresults, in which case we call it\nDCG@K.\nNDCG, or NDCG@K is DCG divided by the DCG obtained by a perfect prediction, so\nthat it is always between 0 and 1. Usually, NDCG is preferred to DCG.\nCompared with the ranking loss, NDCG can take into account relevance scores,\nrather than a ground-truth ranking. So if the ground-truth consists only of an\nordering, the ranking loss should be preferred; if the ground-truth consists of\nactual usefulness scores (e.g. 0 for irrelevant, 1 for relevant, 2 for very\nrelevant), NDCG can be used.\nFor one sample, given the vector of continuous ground-truth values for each\ntarget\n\\(y \\in \\mathbb{R}^{M}\\)\n, where\n\\(M\\)\nis the number of outputs, and\nthe prediction\n\\(\\hat{y}\\)\n, which induces the ranking function\n\\(f\\)\n, the\nDCG score is\n\\[\\sum_{r=1}^{\\min(K, M)}\\frac{y_{f(r)}}{\\log(1 + r)}\\]\nand the NDCG score is the DCG score divided by the DCG score obtained for\n\\(y\\)\n.\nReferences\n#\nWikipedia entry for Discounted Cumulative Gain\nJarvelin, K., & Kekalainen, J. (2002).\nCumulated gain-based evaluation of IR techniques. ACM Transactions on\nInformation Systems (TOIS), 20(4), 422-446.\nWang, Y., Wang, L., Li, Y., He, D., Chen, W., & Liu, T. Y. (2013, May).\nA theoretical analysis of NDCG ranking measures. In Proceedings of the 26th\nAnnual Conference on Learning Theory (COLT 2013)\nMcSherry, F., & Najork, M. (2008, March). Computing information retrieval\nperformance measures efficiently in the presence of tied scores. In\nEuropean conference on information retrieval (pp. 414-421). Springer,\nBerlin, Heidelberg.\n3.4.6.\nRegression metrics\n#\nThe\nsklearn.metrics\nmodule implements several loss, score, and utility\nfunctions to measure regression performance. Some of those have been enhanced\nto handle the multioutput case:\nmean_squared_error\n,\nmean_absolute_error\n,\nr2_score\n,\nexplained_variance_score\n,\nmean_pinball_loss\n,\nd2_pinball_score\nand\nd2_absolute_error_score\n.\nThese functions have a\nmultioutput\nkeyword argument which specifies the\nway the scores or losses for each individual target should be averaged. The\ndefault is\n'uniform_average'\n, which specifies a uniformly weighted mean\nover outputs. If an\nndarray\nof shape\n(n_outputs,)\nis passed, then its\nentries are interpreted as weights and an according weighted average is\nreturned. If\nmultioutput\nis\n'raw_values'\n, then all unaltered\nindividual scores or losses will be returned in an array of shape\n(n_outputs,)\n.\nThe\nr2_score\nand\nexplained_variance_score\naccept an additional\nvalue\n'variance_weighted'\nfor the\nmultioutput\nparameter. This option\nleads to a weighting of each individual score by the variance of the\ncorresponding target variable. This setting quantifies the globally captured\nunscaled variance. If the target variables are of different scale, then this\nscore puts more importance on explaining the higher variance variables.\n3.4.6.1.\nR² score, the coefficient of determination\n#\nThe\nr2_score\nfunction computes the\ncoefficient of\ndetermination\n,\nusually denoted as\n\\(R^2\\)\n.\nIt represents the proportion of variance (of y) that has been explained by the\nindependent variables in the model. It provides an indication of goodness of\nfit and therefore a measure of how well unseen samples are likely to be\npredicted by the model, through the proportion of explained variance.\nAs such variance is dataset dependent,\n\\(R^2\\)\nmay not be meaningfully comparable\nacross different datasets. Best possible score is 1.0 and it can be negative\n(because the model can be arbitrarily worse). A constant model that always\npredicts the expected (average) value of y, disregarding the input features,\nwould get an\n\\(R^2\\)\nscore of 0.0.\nNote: when the prediction residuals have zero mean, the\n\\(R^2\\)\nscore and\nthe\nExplained variance score\nare identical.\nIf\n\\(\\hat{y}_i\\)\nis the predicted value of the\n\\(i\\)\n-th sample\nand\n\\(y_i\\)\nis the corresponding true value for total\n\\(n\\)\nsamples,\nthe estimated\n\\(R^2\\)\nis defined as:\n\\[R^2(y, \\hat{y}) = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\\]\nwhere\n\\(\\bar{y} = \\frac{1}{n} \\sum_{i=1}^{n} y_i\\)\nand\n\\(\\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} \\epsilon_i^2\\)\n.\nNote that\nr2_score\ncalculates unadjusted\n\\(R^2\\)\nwithout correcting for\nbias in sample variance of y.\nIn the particular case where the true target is constant, the\n\\(R^2\\)\nscore is\nnot finite: it is either\nNaN\n(perfect predictions) or\n-Inf\n(imperfect\npredictions). Such non-finite scores may prevent correct model optimization\nsuch as grid-search cross-validation to be performed correctly. For this reason\nthe default behaviour of\nr2_score\nis to replace them with 1.0 (perfect\npredictions) or 0.0 (imperfect predictions). If\nforce_finite\nis set to\nFalse\n, this score falls back on the original\n\\(R^2\\)\ndefinition.\nHere is a small example of usage of the\nr2_score\nfunction:\n>>>\nfrom\nsklearn.metrics\nimport\nr2_score\n>>>\ny_true\n=\n[\n3\n,\n-\n0.5\n,\n2\n,\n7\n]\n>>>\ny_pred\n=\n[\n2.5\n,\n0.0\n,\n2\n,\n8\n]\n>>>\nr2_score\n(\ny_true\n,\ny_pred\n)\n0.948\n>>>\ny_true\n=\n[[\n0.5\n,\n1\n],\n[\n-\n1\n,\n1\n],\n[\n7\n,\n-\n6\n]]\n>>>\ny_pred\n=\n[[\n0\n,\n2\n],\n[\n-\n1\n,\n2\n],\n[\n8\n,\n-\n5\n]]\n>>>\nr2_score\n(\ny_true\n,\ny_pred\n,\nmultioutput\n=\n'variance_weighted'\n)\n0.938\n>>>\ny_true\n=\n[[\n0.5\n,\n1\n],\n[\n-\n1\n,\n1\n],\n[\n7\n,\n-\n6\n]]\n>>>\ny_pred\n=\n[[\n0\n,\n2\n],\n[\n-\n1\n,\n2\n],\n[\n8\n,\n-\n5\n]]\n>>>\nr2_score\n(\ny_true\n,\ny_pred\n,\nmultioutput\n=\n'uniform_average'\n)\n0.936\n>>>\nr2_score\n(\ny_true\n,\ny_pred\n,\nmultioutput\n=\n'raw_values'\n)\narray([0.965, 0.908])\n>>>\nr2_score\n(\ny_true\n,\ny_pred\n,\nmultioutput\n=\n[\n0.3\n,\n0.7\n])\n0.925\n>>>\ny_true\n=\n[\n-\n2\n,\n-\n2\n,\n-\n2\n]\n>>>\ny_pred\n=\n[\n-\n2\n,\n-\n2\n,\n-\n2\n]\n>>>\nr2_score\n(\ny_true\n,\ny_pred\n)\n1.0\n>>>\nr2_score\n(\ny_true\n,\ny_pred\n,\nforce_finite\n=\nFalse\n)\nnan\n>>>\ny_true\n=\n[\n-\n2\n,\n-\n2\n,\n-\n2\n]\n>>>\ny_pred\n=\n[\n-\n2\n,\n-\n2\n,\n-\n2\n+\n1e-8\n]\n>>>\nr2_score\n(\ny_true\n,\ny_pred\n)\n0.0\n>>>\nr2_score\n(\ny_true\n,\ny_pred\n,\nforce_finite\n=\nFalse\n)\n-inf\nExamples\nSee\nL1-based models for Sparse Signals\nfor an example of R² score usage to\nevaluate Lasso and Elastic Net on sparse signals.\n3.4.6.2.\nMean absolute error\n#\nThe\nmean_absolute_error\nfunction computes\nmean absolute\nerror\n, a risk\nmetric corresponding to the expected value of the absolute error loss or\n\\(l1\\)\n-norm loss.\nIf\n\\(\\hat{y}_i\\)\nis the predicted value of the\n\\(i\\)\n-th sample,\nand\n\\(y_i\\)\nis the corresponding true value, then the mean absolute error\n(MAE) estimated over\n\\(n_{\\text{samples}}\\)\nis defined as\n\\[\\text{MAE}(y, \\hat{y}) = \\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}}-1} \\left| y_i - \\hat{y}_i \\right|.\\]\nHere is a small example of usage of the\nmean_absolute_error\nfunction:\n>>>\nfrom\nsklearn.metrics\nimport\nmean_absolute_error\n>>>\ny_true\n=\n[\n3\n,\n-\n0.5\n,\n2\n,\n7\n]\n>>>\ny_pred\n=\n[\n2.5\n,\n0.0\n,\n2\n,\n8\n]\n>>>\nmean_absolute_error\n(\ny_true\n,\ny_pred\n)\n0.5\n>>>\ny_true\n=\n[[\n0.5\n,\n1\n],\n[\n-\n1\n,\n1\n],\n[\n7\n,\n-\n6\n]]\n>>>\ny_pred\n=\n[[\n0\n,\n2\n],\n[\n-\n1\n,\n2\n],\n[\n8\n,\n-\n5\n]]\n>>>\nmean_absolute_error\n(\ny_true\n,\ny_pred\n)\n0.75\n>>>\nmean_absolute_error\n(\ny_true\n,\ny_pred\n,\nmultioutput\n=\n'raw_values'\n)\narray([0.5, 1. ])\n>>>\nmean_absolute_error\n(\ny_true\n,\ny_pred\n,\nmultioutput\n=\n[\n0.3\n,\n0.7\n])\n0.85\n3.4.6.3.\nMean squared error\n#\nThe\nmean_squared_error\nfunction computes\nmean squared\nerror\n, a risk\nmetric corresponding to the expected value of the squared (quadratic) error or\nloss.\nIf\n\\(\\hat{y}_i\\)\nis the predicted value of the\n\\(i\\)\n-th sample,\nand\n\\(y_i\\)\nis the corresponding true value, then the mean squared error\n(MSE) estimated over\n\\(n_{\\text{samples}}\\)\nis defined as\n\\[\\text{MSE}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples} - 1} (y_i - \\hat{y}_i)^2.\\]\nHere is a small example of usage of the\nmean_squared_error\nfunction:\n>>>\nfrom\nsklearn.metrics\nimport\nmean_squared_error\n>>>\ny_true\n=\n[\n3\n,\n-\n0.5\n,\n2\n,\n7\n]\n>>>\ny_pred\n=\n[\n2.5\n,\n0.0\n,\n2\n,\n8\n]\n>>>\nmean_squared_error\n(\ny_true\n,\ny_pred\n)\n0.375\n>>>\ny_true\n=\n[[\n0.5\n,\n1\n],\n[\n-\n1\n,\n1\n],\n[\n7\n,\n-\n6\n]]\n>>>\ny_pred\n=\n[[\n0\n,\n2\n],\n[\n-\n1\n,\n2\n],\n[\n8\n,\n-\n5\n]]\n>>>\nmean_squared_error\n(\ny_true\n,\ny_pred\n)\n0.7083\nExamples\nSee\nGradient Boosting regression\nfor an example of mean squared error usage to evaluate gradient boosting regression.\nTaking the square root of the MSE, called the root mean squared error (RMSE), is another\ncommon metric that provides a measure in the same units as the target variable. RMSE is\navailable through the\nroot_mean_squared_error\nfunction.\n3.4.6.4.\nMean squared logarithmic error\n#\nThe\nmean_squared_log_error\nfunction computes a risk metric\ncorresponding to the expected value of the squared logarithmic (quadratic)\nerror or loss.\nIf\n\\(\\hat{y}_i\\)\nis the predicted value of the\n\\(i\\)\n-th sample,\nand\n\\(y_i\\)\nis the corresponding true value, then the mean squared\nlogarithmic error (MSLE) estimated over\n\\(n_{\\text{samples}}\\)\nis\ndefined as\n\\[\\text{MSLE}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}} \\sum_{i=0}^{n_\\text{samples} - 1} (\\log_e (1 + y_i) - \\log_e (1 + \\hat{y}_i) )^2.\\]\nWhere\n\\(\\log_e (x)\\)\nmeans the natural logarithm of\n\\(x\\)\n. This metric\nis best to use when targets having exponential growth, such as population\ncounts, average sales of a commodity over a span of years etc. Note that this\nmetric penalizes an under-predicted estimate greater than an over-predicted\nestimate.\nHere is a small example of usage of the\nmean_squared_log_error\nfunction:\n>>>\nfrom\nsklearn.metrics\nimport\nmean_squared_log_error\n>>>\ny_true\n=\n[\n3\n,\n5\n,\n2.5\n,\n7\n]\n>>>\ny_pred\n=\n[\n2.5\n,\n5\n,\n4\n,\n8\n]\n>>>\nmean_squared_log_error\n(\ny_true\n,\ny_pred\n)\n0.0397\n>>>\ny_true\n=\n[[\n0.5\n,\n1\n],\n[\n1\n,\n2\n],\n[\n7\n,\n6\n]]\n>>>\ny_pred\n=\n[[\n0.5\n,\n2\n],\n[\n1\n,\n2.5\n],\n[\n8\n,\n8\n]]\n>>>\nmean_squared_log_error\n(\ny_true\n,\ny_pred\n)\n0.044\nThe root mean squared logarithmic error (RMSLE) is available through the\nroot_mean_squared_log_error\nfunction.\n3.4.6.5.\nMean absolute percentage error\n#\nThe\nmean_absolute_percentage_error\n(MAPE), also known as mean absolute\npercentage deviation (MAPD), is an evaluation metric for regression problems.\nThe idea of this metric is to be sensitive to relative errors. It is for example\nnot changed by a global scaling of the target variable.\nIf\n\\(\\hat{y}_i\\)\nis the predicted value of the\n\\(i\\)\n-th sample\nand\n\\(y_i\\)\nis the corresponding true value, then the mean absolute percentage\nerror (MAPE) estimated over\n\\(n_{\\text{samples}}\\)\nis defined as\n\\[\\text{MAPE}(y, \\hat{y}) = \\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}}-1} \\frac{{}\\left| y_i - \\hat{y}_i \\right|}{\\max(\\epsilon, \\left| y_i \\right|)}\\]\nwhere\n\\(\\epsilon\\)\nis an arbitrary small yet strictly positive number to\navoid undefined results when y is zero.\nThe\nmean_absolute_percentage_error\nfunction supports multioutput.\nHere is a small example of usage of the\nmean_absolute_percentage_error\nfunction:\n>>>\nfrom\nsklearn.metrics\nimport\nmean_absolute_percentage_error\n>>>\ny_true\n=\n[\n1\n,\n10\n,\n1e6\n]\n>>>\ny_pred\n=\n[\n0.9\n,\n15\n,\n1.2e6\n]\n>>>\nmean_absolute_percentage_error\n(\ny_true\n,\ny_pred\n)\n0.2666\nIn above example, if we had used\nmean_absolute_error\n, it would have ignored\nthe small magnitude values and only reflected the error in prediction of highest\nmagnitude value. But that problem is resolved in case of MAPE because it calculates\nrelative percentage error with respect to actual output.\nNote\nThe MAPE formula here does not represent the common “percentage” definition: the\npercentage in the range [0, 100] is converted to a relative value in the range [0,\n1] by dividing by 100. Thus, an error of 200% corresponds to a relative error of 2.\nThe motivation here is to have a range of values that is more consistent with other\nerror metrics in scikit-learn, such as\naccuracy_score\n.\nTo obtain the mean absolute percentage error as per the Wikipedia formula,\nmultiply the\nmean_absolute_percentage_error\ncomputed here by 100.\nReferences\n#\nWikipedia entry for Mean Absolute Percentage Error\n3.4.6.6.\nMedian absolute error\n#\nThe\nmedian_absolute_error\nis particularly interesting because it is\nrobust to outliers. The loss is calculated by taking the median of all absolute\ndifferences between the target and the prediction.\nIf\n\\(\\hat{y}_i\\)\nis the predicted value of the\n\\(i\\)\n-th sample\nand\n\\(y_i\\)\nis the corresponding true value, then the median absolute error\n(MedAE) estimated over\n\\(n_{\\text{samples}}\\)\nis defined as\n\\[\\text{MedAE}(y, \\hat{y}) = \\text{median}(\\mid y_1 - \\hat{y}_1 \\mid, \\ldots, \\mid y_n - \\hat{y}_n \\mid).\\]\nThe\nmedian_absolute_error\ndoes not support multioutput.\nHere is a small example of usage of the\nmedian_absolute_error\nfunction:\n>>>\nfrom\nsklearn.metrics\nimport\nmedian_absolute_error\n>>>\ny_true\n=\n[\n3\n,\n-\n0.5\n,\n2\n,\n7\n]\n>>>\ny_pred\n=\n[\n2.5\n,\n0.0\n,\n2\n,\n8\n]\n>>>\nmedian_absolute_error\n(\ny_true\n,\ny_pred\n)\n0.5\n3.4.6.7.\nMax error\n#\nThe\nmax_error\nfunction computes the maximum\nresidual error\n, a metric\nthat captures the worst case error between the predicted value and\nthe true value. In a perfectly fitted single output regression\nmodel,\nmax_error\nwould be\n0\non the training set and though this\nwould be highly unlikely in the real world, this metric shows the\nextent of error that the model had when it was fitted.\nIf\n\\(\\hat{y}_i\\)\nis the predicted value of the\n\\(i\\)\n-th sample,\nand\n\\(y_i\\)\nis the corresponding true value, then the max error is\ndefined as\n\\[\\text{Max Error}(y, \\hat{y}) = \\max(| y_i - \\hat{y}_i |)\\]\nHere is a small example of usage of the\nmax_error\nfunction:\n>>>\nfrom\nsklearn.metrics\nimport\nmax_error\n>>>\ny_true\n=\n[\n3\n,\n2\n,\n7\n,\n1\n]\n>>>\ny_pred\n=\n[\n9\n,\n2\n,\n7\n,\n1\n]\n>>>\nmax_error\n(\ny_true\n,\ny_pred\n)\n6.0\nThe\nmax_error\ndoes not support multioutput.\n3.4.6.8.\nExplained variance score\n#\nThe\nexplained_variance_score\ncomputes the\nexplained variance\nregression score\n.\nIf\n\\(\\hat{y}\\)\nis the estimated target output,\n\\(y\\)\nthe corresponding\n(correct) target output, and\n\\(Var\\)\nis\nVariance\n, the square of the standard deviation,\nthen the explained variance is estimated as follow:\n\\[explained\\_{}variance(y, \\hat{y}) = 1 - \\frac{Var\\{ y - \\hat{y}\\}}{Var\\{y\\}}\\]\nThe best possible score is 1.0, lower values are worse.\nLink to\nR² score, the coefficient of determination\nThe difference between the explained variance score and the\nR² score, the coefficient of determination\nis that the explained variance score does not account for\nsystematic offset in the prediction. For this reason, the\nR² score, the coefficient of determination\nshould be preferred in general.\nIn the particular case where the true target is constant, the Explained\nVariance score is not finite: it is either\nNaN\n(perfect predictions) or\n-Inf\n(imperfect predictions). Such non-finite scores may prevent correct\nmodel optimization such as grid-search cross-validation to be performed\ncorrectly. For this reason the default behaviour of\nexplained_variance_score\nis to replace them with 1.0 (perfect\npredictions) or 0.0 (imperfect predictions). You can set the\nforce_finite\nparameter to\nFalse\nto prevent this fix from happening and fallback on the\noriginal Explained Variance score.\nHere is a small example of usage of the\nexplained_variance_score\nfunction:\n>>>\nfrom\nsklearn.metrics\nimport\nexplained_variance_score\n>>>\ny_true\n=\n[\n3\n,\n-\n0.5\n,\n2\n,\n7\n]\n>>>\ny_pred\n=\n[\n2.5\n,\n0.0\n,\n2\n,\n8\n]\n>>>\nexplained_variance_score\n(\ny_true\n,\ny_pred\n)\n0.957\n>>>\ny_true\n=\n[[\n0.5\n,\n1\n],\n[\n-\n1\n,\n1\n],\n[\n7\n,\n-\n6\n]]\n>>>\ny_pred\n=\n[[\n0\n,\n2\n],\n[\n-\n1\n,\n2\n],\n[\n8\n,\n-\n5\n]]\n>>>\nexplained_variance_score\n(\ny_true\n,\ny_pred\n,\nmultioutput\n=\n'raw_values'\n)\narray([0.967, 1.        ])\n>>>\nexplained_variance_score\n(\ny_true\n,\ny_pred\n,\nmultioutput\n=\n[\n0.3\n,\n0.7\n])\n0.990\n>>>\ny_true\n=\n[\n-\n2\n,\n-\n2\n,\n-\n2\n]\n>>>\ny_pred\n=\n[\n-\n2\n,\n-\n2\n,\n-\n2\n]\n>>>\nexplained_variance_score\n(\ny_true\n,\ny_pred\n)\n1.0\n>>>\nexplained_variance_score\n(\ny_true\n,\ny_pred\n,\nforce_finite\n=\nFalse\n)\nnan\n>>>\ny_true\n=\n[\n-\n2\n,\n-\n2\n,\n-\n2\n]\n>>>\ny_pred\n=\n[\n-\n2\n,\n-\n2\n,\n-\n2\n+\n1e-8\n]\n>>>\nexplained_variance_score\n(\ny_true\n,\ny_pred\n)\n0.0\n>>>\nexplained_variance_score\n(\ny_true\n,\ny_pred\n,\nforce_finite\n=\nFalse\n)\n-inf\n3.4.6.9.\nMean Poisson, Gamma, and Tweedie deviances\n#\nThe\nmean_tweedie_deviance\nfunction computes the\nmean Tweedie\ndeviance error\nwith a\npower\nparameter (\n\\(p\\)\n). This is a metric that elicits\npredicted expectation values of regression targets.\nFollowing special cases exist,\nwhen\npower=0\nit is equivalent to\nmean_squared_error\n.\nwhen\npower=1\nit is equivalent to\nmean_poisson_deviance\n.\nwhen\npower=2\nit is equivalent to\nmean_gamma_deviance\n.\nIf\n\\(\\hat{y}_i\\)\nis the predicted value of the\n\\(i\\)\n-th sample,\nand\n\\(y_i\\)\nis the corresponding true value, then the mean Tweedie\ndeviance error (D) for power\n\\(p\\)\n, estimated over\n\\(n_{\\text{samples}}\\)\nis defined as\n\\[\\begin{split}\\text{D}(y, \\hat{y}) = \\frac{1}{n_\\text{samples}}\n\\sum_{i=0}^{n_\\text{samples} - 1}\n\\begin{cases}\n(y_i-\\hat{y}_i)^2, & \\text{for }p=0\\text{ (Normal)}\\\\\n2(y_i \\log(y_i/\\hat{y}_i) + \\hat{y}_i - y_i),  & \\text{for }p=1\\text{ (Poisson)}\\\\\n2(\\log(\\hat{y}_i/y_i) + y_i/\\hat{y}_i - 1),  & \\text{for }p=2\\text{ (Gamma)}\\\\\n2\\left(\\frac{\\max(y_i,0)^{2-p}}{(1-p)(2-p)}-\n\\frac{y_i\\,\\hat{y}_i^{1-p}}{1-p}+\\frac{\\hat{y}_i^{2-p}}{2-p}\\right),\n& \\text{otherwise}\n\\end{cases}\\end{split}\\]\nTweedie deviance is a homogeneous function of degree\n2-power\n.\nThus, Gamma distribution with\npower=2\nmeans that simultaneously scaling\ny_true\nand\ny_pred\nhas no effect on the deviance. For Poisson\ndistribution\npower=1\nthe deviance scales linearly, and for Normal\ndistribution (\npower=0\n), quadratically.  In general, the higher\npower\nthe less weight is given to extreme deviations between true\nand predicted targets.\nFor instance, let’s compare the two predictions 1.5 and 150 that are both\n50% larger than their corresponding true value.\nThe mean squared error (\npower=0\n) is very sensitive to the\nprediction difference of the second point,:\n>>>\nfrom\nsklearn.metrics\nimport\nmean_tweedie_deviance\n>>>\nmean_tweedie_deviance\n([\n1.0\n],\n[\n1.5\n],\npower\n=\n0\n)\n0.25\n>>>\nmean_tweedie_deviance\n([\n100.\n],\n[\n150.\n],\npower\n=\n0\n)\n2500.0\nIf we increase\npower\nto 1,:\n>>>\nmean_tweedie_deviance\n([\n1.0\n],\n[\n1.5\n],\npower\n=\n1\n)\n0.189\n>>>\nmean_tweedie_deviance\n([\n100.\n],\n[\n150.\n],\npower\n=\n1\n)\n18.9\nthe difference in errors decreases. Finally, by setting,\npower=2\n:\n>>>\nmean_tweedie_deviance\n([\n1.0\n],\n[\n1.5\n],\npower\n=\n2\n)\n0.144\n>>>\nmean_tweedie_deviance\n([\n100.\n],\n[\n150.\n],\npower\n=\n2\n)\n0.144\nwe would get identical errors. The deviance when\npower=2\nis thus only\nsensitive to relative errors.\n3.4.6.10.\nPinball loss\n#\nThe\nmean_pinball_loss\nfunction is used to evaluate the predictive\nperformance of\nquantile regression\nmodels.\n\\[\\text{pinball}(y, \\hat{y}) = \\frac{1}{n_{\\text{samples}}} \\sum_{i=0}^{n_{\\text{samples}}-1}  \\alpha \\max(y_i - \\hat{y}_i, 0) + (1 - \\alpha) \\max(\\hat{y}_i - y_i, 0)\\]\nThe value of pinball loss is equivalent to half of\nmean_absolute_error\nwhen the quantile\nparameter\nalpha\nis set to 0.5.\nHere is a small example of usage of the\nmean_pinball_loss\nfunction:\n>>>\nfrom\nsklearn.metrics\nimport\nmean_pinball_loss\n>>>\ny_true\n=\n[\n1\n,\n2\n,\n3\n]\n>>>\nmean_pinball_loss\n(\ny_true\n,\n[\n0\n,\n2\n,\n3\n],\nalpha\n=\n0.1\n)\n0.033\n>>>\nmean_pinball_loss\n(\ny_true\n,\n[\n1\n,\n2\n,\n4\n],\nalpha\n=\n0.1\n)\n0.3\n>>>\nmean_pinball_loss\n(\ny_true\n,\n[\n0\n,\n2\n,\n3\n],\nalpha\n=\n0.9\n)\n0.3\n>>>\nmean_pinball_loss\n(\ny_true\n,\n[\n1\n,\n2\n,\n4\n],\nalpha\n=\n0.9\n)\n0.033\n>>>\nmean_pinball_loss\n(\ny_true\n,\ny_true\n,\nalpha\n=\n0.1\n)\n0.0\n>>>\nmean_pinball_loss\n(\ny_true\n,\ny_true\n,\nalpha\n=\n0.9\n)\n0.0\nIt is possible to build a scorer object with a specific choice of\nalpha\n:\n>>>\nfrom\nsklearn.metrics\nimport\nmake_scorer\n>>>\nmean_pinball_loss_95p\n=\nmake_scorer\n(\nmean_pinball_loss\n,\nalpha\n=\n0.95\n)\nSuch a scorer can be used to evaluate the generalization performance of a\nquantile regressor via cross-validation:\n>>>\nfrom\nsklearn.datasets\nimport\nmake_regression\n>>>\nfrom\nsklearn.model_selection\nimport\ncross_val_score\n>>>\nfrom\nsklearn.ensemble\nimport\nGradientBoostingRegressor\n>>>\n>>>\nX\n,\ny\n=\nmake_regression\n(\nn_samples\n=\n100\n,\nrandom_state\n=\n0\n)\n>>>\nestimator\n=\nGradientBoostingRegressor\n(\n...\nloss\n=\n\"quantile\"\n,\n...\nalpha\n=\n0.95\n,\n...\nrandom_state\n=\n0\n,\n...\n)\n>>>\ncross_val_score\n(\nestimator\n,\nX\n,\ny\n,\ncv\n=\n5\n,\nscoring\n=\nmean_pinball_loss_95p\n)\narray([13.6, 9.7, 23.3, 9.5, 10.4])\nIt is also possible to build scorer objects for hyper-parameter tuning. The\nsign of the loss must be switched to ensure that greater means better as\nexplained in the example linked below.\nExamples\nSee\nPrediction Intervals for Gradient Boosting Regression\nfor an example of using the pinball loss to evaluate and tune the\nhyper-parameters of quantile regression models on data with non-symmetric\nnoise and outliers.\n3.4.6.11.\nD² score\n#\nThe D² score computes the fraction of deviance explained.\nIt is a generalization of R², where the squared error is generalized and replaced\nby a deviance of choice\n\\(\\text{dev}(y, \\hat{y})\\)\n(e.g., Tweedie, pinball or mean absolute error). D² is a form of a\nskill score\n.\nIt is calculated as\n\\[D^2(y, \\hat{y}) = 1 - \\frac{\\text{dev}(y, \\hat{y})}{\\text{dev}(y, y_{\\text{null}})} \\,.\\]\nWhere\n\\(y_{\\text{null}}\\)\nis the optimal prediction of an intercept-only model\n(e.g., the mean of\ny_true\nfor the Tweedie case, the median for absolute\nerror and the alpha-quantile for pinball loss).\nLike R², the best possible score is 1.0 and it can be negative (because the\nmodel can be arbitrarily worse). A constant model that always predicts\n\\(y_{\\text{null}}\\)\n, disregarding the input features, would get a D² score\nof 0.0.\nD² Tweedie score\n#\nThe\nd2_tweedie_score\nfunction implements the special case of D²\nwhere\n\\(\\text{dev}(y, \\hat{y})\\)\nis the Tweedie deviance, see\nMean Poisson, Gamma, and Tweedie deviances\n.\nIt is also known as D² Tweedie and is related to McFadden’s likelihood ratio index.\nThe argument\npower\ndefines the Tweedie power as for\nmean_tweedie_deviance\n. Note that for\npower=0\n,\nd2_tweedie_score\nequals\nr2_score\n(for single targets).\nA scorer object with a specific choice of\npower\ncan be built by:\n>>>\nfrom\nsklearn.metrics\nimport\nd2_tweedie_score\n,\nmake_scorer\n>>>\nd2_tweedie_score_15\n=\nmake_scorer\n(\nd2_tweedie_score\n,\npower\n=\n1.5\n)\nD² pinball score\n#\nThe\nd2_pinball_score\nfunction implements the special case\nof D² with the pinball loss, see\nPinball loss\n, i.e.:\n\\[\\text{dev}(y, \\hat{y}) = \\text{pinball}(y, \\hat{y}).\\]\nThe argument\nalpha\ndefines the slope of the pinball loss as for\nmean_pinball_loss\n(\nPinball loss\n). It determines the\nquantile level\nalpha\nfor which the pinball loss and also D²\nare optimal. Note that for\nalpha=0.5\n(the default)\nd2_pinball_score\nequals\nd2_absolute_error_score\n.\nA scorer object with a specific choice of\nalpha\ncan be built by:\n>>>\nfrom\nsklearn.metrics\nimport\nd2_pinball_score\n,\nmake_scorer\n>>>\nd2_pinball_score_08\n=\nmake_scorer\n(\nd2_pinball_score\n,\nalpha\n=\n0.8\n)\nD² absolute error score\n#\nThe\nd2_absolute_error_score\nfunction implements the special case of\nthe\nMean absolute error\n:\n\\[\\text{dev}(y, \\hat{y}) = \\text{MAE}(y, \\hat{y}).\\]\nHere are some usage examples of the\nd2_absolute_error_score\nfunction:\n>>>\nfrom\nsklearn.metrics\nimport\nd2_absolute_error_score\n>>>\ny_true\n=\n[\n3\n,\n-\n0.5\n,\n2\n,\n7\n]\n>>>\ny_pred\n=\n[\n2.5\n,\n0.0\n,\n2\n,\n8\n]\n>>>\nd2_absolute_error_score\n(\ny_true\n,\ny_pred\n)\n0.764\n>>>\ny_true\n=\n[\n1\n,\n2\n,\n3\n]\n>>>\ny_pred\n=\n[\n1\n,\n2\n,\n3\n]\n>>>\nd2_absolute_error_score\n(\ny_true\n,\ny_pred\n)\n1.0\n>>>\ny_true\n=\n[\n1\n,\n2\n,\n3\n]\n>>>\ny_pred\n=\n[\n2\n,\n2\n,\n2\n]\n>>>\nd2_absolute_error_score\n(\ny_true\n,\ny_pred\n)\n0.0\n3.4.6.12.\nVisual evaluation of regression models\n#\nAmong methods to assess the quality of regression models, scikit-learn provides\nthe\nPredictionErrorDisplay\nclass. It allows to\nvisually inspect the prediction errors of a model in two different manners.\nThe plot on the left shows the actual values vs predicted values. For a\nnoise-free regression task aiming to predict the (conditional) expectation of\ny\n, a perfect regression model would display data points on the diagonal\ndefined by predicted equal to actual values. The further away from this optimal\nline, the larger the error of the model. In a more realistic setting with\nirreducible noise, that is, when not all the variations of\ny\ncan be explained\nby features in\nX\n, then the best model would lead to a cloud of points densely\narranged around the diagonal.\nNote that the above only holds when the predicted values is the expected value\nof\ny\ngiven\nX\n. This is typically the case for regression models that\nminimize the mean squared error objective function or more generally the\nmean Tweedie deviance\nfor any value of its\n“power” parameter.\nWhen plotting the predictions of an estimator that predicts a quantile\nof\ny\ngiven\nX\n, e.g.\nQuantileRegressor\nor any other model minimizing the\npinball loss\n, a\nfraction of the points are either expected to lie above or below the diagonal\ndepending on the estimated quantile level.\nAll in all, while intuitive to read, this plot does not really inform us on\nwhat to do to obtain a better model.\nThe right-hand side plot shows the residuals (i.e. the difference between the\nactual and the predicted values) vs. the predicted values.\nThis plot makes it easier to visualize if the residuals follow and\nhomoscedastic or heteroschedastic\ndistribution.\nIn particular, if the true distribution of\ny|X\nis Poisson or Gamma\ndistributed, it is expected that the variance of the residuals of the optimal\nmodel would grow with the predicted value of\nE[y|X]\n(either linearly for\nPoisson or quadratically for Gamma).\nWhen fitting a linear least squares regression model (see\nLinearRegression\nand\nRidge\n), we can use this plot to check\nif some of the\nmodel assumptions\nare met, in particular that the residuals should be uncorrelated, their\nexpected value should be null and that their variance should be constant\n(homoschedasticity).\nIf this is not the case, and in particular if the residuals plot show some\nbanana-shaped structure, this is a hint that the model is likely mis-specified\nand that non-linear feature engineering or switching to a non-linear regression\nmodel might be useful.\nRefer to the example below to see a model evaluation that makes use of this\ndisplay.\nExamples\nSee\nEffect of transforming the targets in regression model\nfor\nan example on how to use\nPredictionErrorDisplay\nto visualize the prediction quality improvement of a regression model\nobtained by transforming the target before learning.\n3.4.7.\nClustering metrics\n#\nThe\nsklearn.metrics\nmodule implements several loss, score, and utility\nfunctions to measure clustering performance. For more information see the\nClustering performance evaluation\nsection for instance clustering, and\nBiclustering evaluation\nfor biclustering.\n3.4.8.\nDummy estimators\n#\nWhen doing supervised learning, a simple sanity check consists of comparing\none’s estimator against simple rules of thumb.\nDummyClassifier\nimplements several such simple strategies for classification:\nstratified\ngenerates random predictions by respecting the training\nset class distribution.\nmost_frequent\nalways predicts the most frequent label in the training set.\nprior\nalways predicts the class that maximizes the class prior\n(like\nmost_frequent\n) and\npredict_proba\nreturns the class prior.\nuniform\ngenerates predictions uniformly at random.\nconstant\nalways predicts a constant label that is provided by the user.\nA major motivation of this method is F1-scoring, when the positive class\nis in the minority.\nNote that with all these strategies, the\npredict\nmethod completely ignores\nthe input data!\nTo illustrate\nDummyClassifier\n, first let’s create an imbalanced\ndataset:\n>>>\nfrom\nsklearn.datasets\nimport\nload_iris\n>>>\nfrom\nsklearn.model_selection\nimport\ntrain_test_split\n>>>\nX\n,\ny\n=\nload_iris\n(\nreturn_X_y\n=\nTrue\n)\n>>>\ny\n[\ny\n!=\n1\n]\n=\n-\n1\n>>>\nX_train\n,\nX_test\n,\ny_train\n,\ny_test\n=\ntrain_test_split\n(\nX\n,\ny\n,\nrandom_state\n=\n0\n)\nNext, let’s compare the accuracy of\nSVC\nand\nmost_frequent\n:\n>>>\nfrom\nsklearn.dummy\nimport\nDummyClassifier\n>>>\nfrom\nsklearn.svm\nimport\nSVC\n>>>\nclf\n=\nSVC\n(\nkernel\n=\n'linear'\n,\nC\n=\n1\n)\n.\nfit\n(\nX_train\n,\ny_train\n)\n>>>\nclf\n.\nscore\n(\nX_test\n,\ny_test\n)\n0.63\n>>>\nclf\n=\nDummyClassifier\n(\nstrategy\n=\n'most_frequent'\n,\nrandom_state\n=\n0\n)\n>>>\nclf\n.\nfit\n(\nX_train\n,\ny_train\n)\nDummyClassifier(random_state=0, strategy='most_frequent')\n>>>\nclf\n.\nscore\n(\nX_test\n,\ny_test\n)\n0.579\nWe see that\nSVC\ndoesn’t do much better than a dummy classifier. Now, let’s\nchange the kernel:\n>>>\nclf\n=\nSVC\n(\nkernel\n=\n'rbf'\n,\nC\n=\n1\n)\n.\nfit\n(\nX_train\n,\ny_train\n)\n>>>\nclf\n.\nscore\n(\nX_test\n,\ny_test\n)\n0.94\nWe see that the accuracy was boosted to almost 100%.  A cross validation\nstrategy is recommended for a better estimate of the accuracy, if it\nis not too CPU costly. For more information see the\nCross-validation: evaluating estimator performance\nsection. Moreover if you want to optimize over the parameter space, it is highly\nrecommended to use an appropriate methodology; see the\nTuning the hyper-parameters of an estimator\nsection for details.\nMore generally, when the accuracy of a classifier is too close to random, it\nprobably means that something went wrong: features are not helpful, a\nhyperparameter is not correctly tuned, the classifier is suffering from class\nimbalance, etc…\nDummyRegressor\nalso implements four simple rules of thumb for regression:\nmean\nalways predicts the mean of the training targets.\nmedian\nalways predicts the median of the training targets.\nquantile\nalways predicts a user provided quantile of the training targets.\nconstant\nalways predicts a constant value that is provided by the user.\nIn all these strategies, the\npredict\nmethod completely ignores\nthe input data.",
    "url": "https://scikit-learn.org/stable/modules/model_evaluation.html"
  },
  {
    "title": "3.2. Tuning the hyper-parameters of an estimator — scikit-learn 1.7.0 documentation",
    "content": "3.2.\nTuning the hyper-parameters of an estimator\n#\nHyper-parameters are parameters that are not directly learnt within estimators.\nIn scikit-learn they are passed as arguments to the constructor of the\nestimator classes. Typical examples include\nC\n,\nkernel\nand\ngamma\nfor Support Vector Classifier,\nalpha\nfor Lasso, etc.\nIt is possible and recommended to search the hyper-parameter space for the\nbest\ncross validation\nscore.\nAny parameter provided when constructing an estimator may be optimized in this\nmanner. Specifically, to find the names and current values for all parameters\nfor a given estimator, use:\nestimator\n.\nget_params\n()\nA search consists of:\nan estimator (regressor or classifier such as\nsklearn.svm.SVC()\n);\na parameter space;\na method for searching or sampling candidates;\na cross-validation scheme; and\na\nscore function\n.\nTwo generic approaches to parameter search are provided in\nscikit-learn: for given values,\nGridSearchCV\nexhaustively considers\nall parameter combinations, while\nRandomizedSearchCV\ncan sample a\ngiven number of candidates from a parameter space with a specified\ndistribution. Both these tools have successive halving counterparts\nHalvingGridSearchCV\nand\nHalvingRandomSearchCV\n, which can be\nmuch faster at finding a good parameter combination.\nAfter describing these tools we detail\nbest practices\napplicable to these approaches. Some models allow for\nspecialized, efficient parameter search strategies, outlined in\nAlternatives to brute force parameter search\n.\nNote that it is common that a small subset of those parameters can have a large\nimpact on the predictive or computation performance of the model while others\ncan be left to their default values. It is recommended to read the docstring of\nthe estimator class to get a finer understanding of their expected behavior,\npossibly by reading the enclosed reference to the literature.\n3.2.1.\nExhaustive Grid Search\n#\nThe grid search provided by\nGridSearchCV\nexhaustively generates\ncandidates from a grid of parameter values specified with the\nparam_grid\nparameter. For instance, the following\nparam_grid\n:\nparam_grid\n=\n[\n{\n'C'\n:\n[\n1\n,\n10\n,\n100\n,\n1000\n],\n'kernel'\n:\n[\n'linear'\n]},\n{\n'C'\n:\n[\n1\n,\n10\n,\n100\n,\n1000\n],\n'gamma'\n:\n[\n0.001\n,\n0.0001\n],\n'kernel'\n:\n[\n'rbf'\n]},\n]\nspecifies that two grids should be explored: one with a linear kernel and\nC values in [1, 10, 100, 1000], and the second one with an RBF kernel,\nand the cross-product of C values ranging in [1, 10, 100, 1000] and gamma\nvalues in [0.001, 0.0001].\nThe\nGridSearchCV\ninstance implements the usual estimator API: when\n“fitting” it on a dataset all the possible combinations of parameter values are\nevaluated and the best combination is retained.\nExamples\nSee\nNested versus non-nested cross-validation\nfor an example of Grid Search within a cross validation loop on the iris\ndataset. This is the best practice for evaluating the performance of a\nmodel with grid search.\nSee\nSample pipeline for text feature extraction and evaluation\nfor an example\nof Grid Search coupling parameters from a text documents feature\nextractor (n-gram count vectorizer and TF-IDF transformer) with a\nclassifier (here a linear SVM trained with SGD with either elastic\nnet or L2 penalty) using a\nPipeline\ninstance.\nAdvanced examples\n#\nSee\nNested versus non-nested cross-validation\nfor an example of Grid Search within a cross validation loop on the iris\ndataset. This is the best practice for evaluating the performance of a\nmodel with grid search.\nSee\nDemonstration of multi-metric evaluation on cross_val_score and GridSearchCV\nfor an example of\nGridSearchCV\nbeing used to evaluate multiple\nmetrics simultaneously.\nSee\nBalance model complexity and cross-validated score\nfor an example of using\nrefit=callable\ninterface in\nGridSearchCV\n. The example shows how this interface adds a certain\namount of flexibility in identifying the “best” estimator. This interface\ncan also be used in multiple metrics evaluation.\nSee\nStatistical comparison of models using grid search\nfor an example of how to do a statistical comparison on the outputs of\nGridSearchCV\n.\n3.2.2.\nRandomized Parameter Optimization\n#\nWhile using a grid of parameter settings is currently the most widely used\nmethod for parameter optimization, other search methods have more\nfavorable properties.\nRandomizedSearchCV\nimplements a randomized search over parameters,\nwhere each setting is sampled from a distribution over possible parameter values.\nThis has two main benefits over an exhaustive search:\nA budget can be chosen independent of the number of parameters and possible values.\nAdding parameters that do not influence the performance does not decrease efficiency.\nSpecifying how parameters should be sampled is done using a dictionary, very\nsimilar to specifying parameters for\nGridSearchCV\n. Additionally,\na computation budget, being the number of sampled candidates or sampling\niterations, is specified using the\nn_iter\nparameter.\nFor each parameter, either a distribution over possible values or a list of\ndiscrete choices (which will be sampled uniformly) can be specified:\n{\n'C'\n:\nscipy\n.\nstats\n.\nexpon\n(\nscale\n=\n100\n),\n'gamma'\n:\nscipy\n.\nstats\n.\nexpon\n(\nscale\n=\n.1\n),\n'kernel'\n:\n[\n'rbf'\n],\n'class_weight'\n:[\n'balanced'\n,\nNone\n]}\nThis example uses the\nscipy.stats\nmodule, which contains many useful\ndistributions for sampling parameters, such as\nexpon\n,\ngamma\n,\nuniform\n,\nloguniform\nor\nrandint\n.\nIn principle, any function can be passed that provides a\nrvs\n(random\nvariate sample) method to sample a value. A call to the\nrvs\nfunction should\nprovide independent random samples from possible parameter values on\nconsecutive calls.\nWarning\nThe distributions in\nscipy.stats\nprior to version scipy 0.16\ndo not allow specifying a random state. Instead, they use the global\nnumpy random state, that can be seeded via\nnp.random.seed\nor set\nusing\nnp.random.set_state\n. However, beginning scikit-learn 0.18,\nthe\nsklearn.model_selection\nmodule sets the random state provided\nby the user if scipy >= 0.16 is also available.\nFor continuous parameters, such as\nC\nabove, it is important to specify\na continuous distribution to take full advantage of the randomization. This way,\nincreasing\nn_iter\nwill always lead to a finer search.\nA continuous log-uniform random variable is the continuous version of\na log-spaced parameter. For example to specify the equivalent of\nC\nfrom above,\nloguniform(1,\n100)\ncan be used instead of\n[1,\n10,\n100]\n.\nMirroring the example above in grid search, we can specify a continuous random\nvariable that is log-uniformly distributed between\n1e0\nand\n1e3\n:\nfrom\nsklearn.utils.fixes\nimport\nloguniform\n{\n'C'\n:\nloguniform\n(\n1e0\n,\n1e3\n),\n'gamma'\n:\nloguniform\n(\n1e-4\n,\n1e-3\n),\n'kernel'\n:\n[\n'rbf'\n],\n'class_weight'\n:[\n'balanced'\n,\nNone\n]}\nExamples\nComparing randomized search and grid search for hyperparameter estimation\ncompares the usage and efficiency\nof randomized search and grid search.\nReferences\nBergstra, J. and Bengio, Y.,\nRandom search for hyper-parameter optimization,\nThe Journal of Machine Learning Research (2012)\n3.2.3.\nSearching for optimal parameters with successive halving\n#\nScikit-learn also provides the\nHalvingGridSearchCV\nand\nHalvingRandomSearchCV\nestimators that can be used to\nsearch a parameter space using successive halving\n[\n1\n]\n[\n2\n]\n. Successive\nhalving (SH) is like a tournament among candidate parameter combinations.\nSH is an iterative selection process where all candidates (the\nparameter combinations) are evaluated with a small amount of resources at\nthe first iteration. Only some of these candidates are selected for the next\niteration, which will be allocated more resources. For parameter tuning, the\nresource is typically the number of training samples, but it can also be an\narbitrary numeric parameter such as\nn_estimators\nin a random forest.\nNote\nThe resource increase chosen should be large enough so that a large improvement\nin scores is obtained when taking into account statistical significance.\nAs illustrated in the figure below, only a subset of candidates\n‘survive’ until the last iteration. These are the candidates that have\nconsistently ranked among the top-scoring candidates across all iterations.\nEach iteration is allocated an increasing amount of resources per candidate,\nhere the number of samples.\nWe here briefly describe the main parameters, but each parameter and their\ninteractions are described more in detail in the dropdown section below. The\nfactor\n(> 1) parameter controls the rate at which the resources grow, and\nthe rate at which the number of candidates decreases. In each iteration, the\nnumber of resources per candidate is multiplied by\nfactor\nand the number\nof candidates is divided by the same factor. Along with\nresource\nand\nmin_resources\n,\nfactor\nis the most important parameter to control the\nsearch in our implementation, though a value of 3 usually works well.\nfactor\neffectively controls the number of iterations in\nHalvingGridSearchCV\nand the number of candidates (by default) and\niterations in\nHalvingRandomSearchCV\n.\naggressive_elimination=True\ncan also be used if the number of available resources is small. More control\nis available through tuning the\nmin_resources\nparameter.\nThese estimators are still\nexperimental\n: their predictions\nand their API might change without any deprecation cycle. To use them, you\nneed to explicitly import\nenable_halving_search_cv\n:\n>>>\nfrom\nsklearn.experimental\nimport\nenable_halving_search_cv\n# noqa\n>>>\nfrom\nsklearn.model_selection\nimport\nHalvingGridSearchCV\n>>>\nfrom\nsklearn.model_selection\nimport\nHalvingRandomSearchCV\nExamples\nComparison between grid search and successive halving\nSuccessive Halving Iterations\nThe sections below dive into technical aspects of successive halving.\nChoosing\nmin_resources\nand the number of candidates\n#\nBeside\nfactor\n, the two main parameters that influence the behaviour of a\nsuccessive halving search are the\nmin_resources\nparameter, and the\nnumber of candidates (or parameter combinations) that are evaluated.\nmin_resources\nis the amount of resources allocated at the first\niteration for each candidate. The number of candidates is specified directly\nin\nHalvingRandomSearchCV\n, and is determined from the\nparam_grid\nparameter of\nHalvingGridSearchCV\n.\nConsider a case where the resource is the number of samples, and where we\nhave 1000 samples. In theory, with\nmin_resources=10\nand\nfactor=2\n, we\nare able to run\nat most\n7 iterations with the following number of\nsamples:\n[10,\n20,\n40,\n80,\n160,\n320,\n640]\n.\nBut depending on the number of candidates, we might run less than 7\niterations: if we start with a\nsmall\nnumber of candidates, the last\niteration might use less than 640 samples, which means not using all the\navailable resources (samples). For example if we start with 5 candidates, we\nonly need 2 iterations: 5 candidates for the first iteration, then\n5\n//\n2\n=\n2\ncandidates at the second iteration, after which we know which\ncandidate performs the best (so we don’t need a third one). We would only be\nusing at most 20 samples which is a waste since we have 1000 samples at our\ndisposal. On the other hand, if we start with a\nhigh\nnumber of\ncandidates, we might end up with a lot of candidates at the last iteration,\nwhich may not always be ideal: it means that many candidates will run with\nthe full resources, basically reducing the procedure to standard search.\nIn the case of\nHalvingRandomSearchCV\n, the number of candidates is set\nby default such that the last iteration uses as much of the available\nresources as possible. For\nHalvingGridSearchCV\n, the number of\ncandidates is determined by the\nparam_grid\nparameter. Changing the value of\nmin_resources\nwill impact the number of possible iterations, and as a\nresult will also have an effect on the ideal number of candidates.\nAnother consideration when choosing\nmin_resources\nis whether or not it\nis easy to discriminate between good and bad candidates with a small amount\nof resources. For example, if you need a lot of samples to distinguish\nbetween good and bad parameters, a high\nmin_resources\nis recommended. On\nthe other hand if the distinction is clear even with a small amount of\nsamples, then a small\nmin_resources\nmay be preferable since it would\nspeed up the computation.\nNotice in the example above that the last iteration does not use the maximum\namount of resources available: 1000 samples are available, yet only 640 are\nused, at most. By default, both\nHalvingRandomSearchCV\nand\nHalvingGridSearchCV\ntry to use as many resources as possible in the\nlast iteration, with the constraint that this amount of resources must be a\nmultiple of both\nmin_resources\nand\nfactor\n(this constraint will be clear\nin the next section).\nHalvingRandomSearchCV\nachieves this by\nsampling the right amount of candidates, while\nHalvingGridSearchCV\nachieves this by properly setting\nmin_resources\n.\nAmount of resource and number of candidates at each iteration\n#\nAt any iteration\ni\n, each candidate is allocated a given amount of resources\nwhich we denote\nn_resources_i\n. This quantity is controlled by the\nparameters\nfactor\nand\nmin_resources\nas follows (\nfactor\nis strictly\ngreater than 1):\nn_resources_i\n=\nfactor\n**\ni\n*\nmin_resources\n,\nor equivalently:\nn_resources_\n{\ni\n+\n1\n}\n=\nn_resources_i\n*\nfactor\nwhere\nmin_resources\n==\nn_resources_0\nis the amount of resources used at\nthe first iteration.\nfactor\nalso defines the proportions of candidates\nthat will be selected for the next iteration:\nn_candidates_i\n=\nn_candidates\n//\n(\nfactor\n**\ni\n)\nor equivalently:\nn_candidates_0\n=\nn_candidates\nn_candidates_\n{\ni\n+\n1\n}\n=\nn_candidates_i\n//\nfactor\nSo in the first iteration, we use\nmin_resources\nresources\nn_candidates\ntimes. In the second iteration, we use\nmin_resources\n*\nfactor\nresources\nn_candidates\n//\nfactor\ntimes. The third again\nmultiplies the resources per candidate and divides the number of candidates.\nThis process stops when the maximum amount of resource per candidate is\nreached, or when we have identified the best candidate. The best candidate\nis identified at the iteration that is evaluating\nfactor\nor less candidates\n(see just below for an explanation).\nHere is an example with\nmin_resources=3\nand\nfactor=2\n, starting with\n70 candidates:\nn_resources_i\nn_candidates_i\n3 (=min_resources)\n70 (=n_candidates)\n3 * 2 = 6\n70 // 2 = 35\n6 * 2 = 12\n35 // 2 = 17\n12 * 2 = 24\n17 // 2 = 8\n24 * 2 = 48\n8 // 2 = 4\n48 * 2 = 96\n4 // 2 = 2\nWe can note that:\nthe process stops at the first iteration which evaluates\nfactor=2\ncandidates: the best candidate is the best out of these 2 candidates. It\nis not necessary to run an additional iteration, since it would only\nevaluate one candidate (namely the best one, which we have already\nidentified). For this reason, in general, we want the last iteration to\nrun at most\nfactor\ncandidates. If the last iteration evaluates more\nthan\nfactor\ncandidates, then this last iteration reduces to a regular\nsearch (as in\nRandomizedSearchCV\nor\nGridSearchCV\n).\neach\nn_resources_i\nis a multiple of both\nfactor\nand\nmin_resources\n(which is confirmed by its definition above).\nThe amount of resources that is used at each iteration can be found in the\nn_resources_\nattribute.\nChoosing a resource\n#\nBy default, the resource is defined in terms of number of samples. That is,\neach iteration will use an increasing amount of samples to train on. You can\nhowever manually specify a parameter to use as the resource with the\nresource\nparameter. Here is an example where the resource is defined in\nterms of the number of estimators of a random forest:\n>>>\nfrom\nsklearn.datasets\nimport\nmake_classification\n>>>\nfrom\nsklearn.ensemble\nimport\nRandomForestClassifier\n>>>\nfrom\nsklearn.experimental\nimport\nenable_halving_search_cv\n# noqa\n>>>\nfrom\nsklearn.model_selection\nimport\nHalvingGridSearchCV\n>>>\nimport\npandas\nas\npd\n>>>\nparam_grid\n=\n{\n'max_depth'\n:\n[\n3\n,\n5\n,\n10\n],\n...\n'min_samples_split'\n:\n[\n2\n,\n5\n,\n10\n]}\n>>>\nbase_estimator\n=\nRandomForestClassifier\n(\nrandom_state\n=\n0\n)\n>>>\nX\n,\ny\n=\nmake_classification\n(\nn_samples\n=\n1000\n,\nrandom_state\n=\n0\n)\n>>>\nsh\n=\nHalvingGridSearchCV\n(\nbase_estimator\n,\nparam_grid\n,\ncv\n=\n5\n,\n...\nfactor\n=\n2\n,\nresource\n=\n'n_estimators'\n,\n...\nmax_resources\n=\n30\n)\n.\nfit\n(\nX\n,\ny\n)\n>>>\nsh\n.\nbest_estimator_\nRandomForestClassifier(max_depth=5, n_estimators=24, random_state=0)\nNote that it is not possible to budget on a parameter that is part of the\nparameter grid.\nExhausting the available resources\n#\nAs mentioned above, the number of resources that is used at each iteration\ndepends on the\nmin_resources\nparameter.\nIf you have a lot of resources available but start with a low number of\nresources, some of them might be wasted (i.e. not used):\n>>>\nfrom\nsklearn.datasets\nimport\nmake_classification\n>>>\nfrom\nsklearn.svm\nimport\nSVC\n>>>\nfrom\nsklearn.experimental\nimport\nenable_halving_search_cv\n# noqa\n>>>\nfrom\nsklearn.model_selection\nimport\nHalvingGridSearchCV\n>>>\nimport\npandas\nas\npd\n>>>\nparam_grid\n=\n{\n'kernel'\n:\n(\n'linear'\n,\n'rbf'\n),\n...\n'C'\n:\n[\n1\n,\n10\n,\n100\n]}\n>>>\nbase_estimator\n=\nSVC\n(\ngamma\n=\n'scale'\n)\n>>>\nX\n,\ny\n=\nmake_classification\n(\nn_samples\n=\n1000\n)\n>>>\nsh\n=\nHalvingGridSearchCV\n(\nbase_estimator\n,\nparam_grid\n,\ncv\n=\n5\n,\n...\nfactor\n=\n2\n,\nmin_resources\n=\n20\n)\n.\nfit\n(\nX\n,\ny\n)\n>>>\nsh\n.\nn_resources_\n[20, 40, 80]\nThe search process will only use 80 resources at most, while our maximum\namount of available resources is\nn_samples=1000\n. Here, we have\nmin_resources\n=\nr_0\n=\n20\n.\nFor\nHalvingGridSearchCV\n, by default, the\nmin_resources\nparameter\nis set to ‘exhaust’. This means that\nmin_resources\nis automatically set\nsuch that the last iteration can use as many resources as possible, within\nthe\nmax_resources\nlimit:\n>>>\nsh\n=\nHalvingGridSearchCV\n(\nbase_estimator\n,\nparam_grid\n,\ncv\n=\n5\n,\n...\nfactor\n=\n2\n,\nmin_resources\n=\n'exhaust'\n)\n.\nfit\n(\nX\n,\ny\n)\n>>>\nsh\n.\nn_resources_\n[250, 500, 1000]\nmin_resources\nwas here automatically set to 250, which results in the last\niteration using all the resources. The exact value that is used depends on\nthe number of candidate parameters, on\nmax_resources\nand on\nfactor\n.\nFor\nHalvingRandomSearchCV\n, exhausting the resources can be done in 2\nways:\nby setting\nmin_resources='exhaust'\n, just like for\nHalvingGridSearchCV\n;\nby setting\nn_candidates='exhaust'\n.\nBoth options are mutually exclusive: using\nmin_resources='exhaust'\nrequires\nknowing the number of candidates, and symmetrically\nn_candidates='exhaust'\nrequires knowing\nmin_resources\n.\nIn general, exhausting the total number of resources leads to a better final\ncandidate parameter, and is slightly more time-intensive.\n3.2.3.1.\nAggressive elimination of candidates\n#\nUsing the\naggressive_elimination\nparameter, you can force the search\nprocess to end up with less than\nfactor\ncandidates at the last\niteration.\nCode example of aggressive elimination\n#\nIdeally, we want the last iteration to evaluate\nfactor\ncandidates. We\nthen just have to pick the best one. When the number of available resources is\nsmall with respect to the number of candidates, the last iteration may have to\nevaluate more than\nfactor\ncandidates:\n>>>\nfrom\nsklearn.datasets\nimport\nmake_classification\n>>>\nfrom\nsklearn.svm\nimport\nSVC\n>>>\nfrom\nsklearn.experimental\nimport\nenable_halving_search_cv\n# noqa\n>>>\nfrom\nsklearn.model_selection\nimport\nHalvingGridSearchCV\n>>>\nimport\npandas\nas\npd\n>>>\nparam_grid\n=\n{\n'kernel'\n:\n(\n'linear'\n,\n'rbf'\n),\n...\n'C'\n:\n[\n1\n,\n10\n,\n100\n]}\n>>>\nbase_estimator\n=\nSVC\n(\ngamma\n=\n'scale'\n)\n>>>\nX\n,\ny\n=\nmake_classification\n(\nn_samples\n=\n1000\n)\n>>>\nsh\n=\nHalvingGridSearchCV\n(\nbase_estimator\n,\nparam_grid\n,\ncv\n=\n5\n,\n...\nfactor\n=\n2\n,\nmax_resources\n=\n40\n,\n...\naggressive_elimination\n=\nFalse\n)\n.\nfit\n(\nX\n,\ny\n)\n>>>\nsh\n.\nn_resources_\n[20, 40]\n>>>\nsh\n.\nn_candidates_\n[6, 3]\nSince we cannot use more than\nmax_resources=40\nresources, the process\nhas to stop at the second iteration which evaluates more than\nfactor=2\ncandidates.\nWhen using\naggressive_elimination\n, the process will eliminate as many\ncandidates as necessary using\nmin_resources\nresources:\n>>>\nsh\n=\nHalvingGridSearchCV\n(\nbase_estimator\n,\nparam_grid\n,\ncv\n=\n5\n,\n...\nfactor\n=\n2\n,\n...\nmax_resources\n=\n40\n,\n...\naggressive_elimination\n=\nTrue\n,\n...\n)\n.\nfit\n(\nX\n,\ny\n)\n>>>\nsh\n.\nn_resources_\n[20, 20, 40]\n>>>\nsh\n.\nn_candidates_\n[6, 3, 2]\nNotice that we end with 2 candidates at the last iteration since we have\neliminated enough candidates during the first iterations, using\nn_resources\n=\nmin_resources\n=\n20\n.\n3.2.3.2.\nAnalyzing results with the\ncv_results_\nattribute\n#\nThe\ncv_results_\nattribute contains useful information for analyzing the\nresults of a search. It can be converted to a pandas dataframe with\ndf\n=\npd.DataFrame(est.cv_results_)\n. The\ncv_results_\nattribute of\nHalvingGridSearchCV\nand\nHalvingRandomSearchCV\nis similar\nto that of\nGridSearchCV\nand\nRandomizedSearchCV\n, with\nadditional information related to the successive halving process.\nExample of a (truncated) output dataframe:\n#\niter\nn_resources\nmean_test_score\nparams\n0\n0\n125\n0.983667\n{‘criterion’: ‘log_loss’, ‘max_depth’: None, ‘max_features’: 9, ‘min_samples_split’: 5}\n1\n0\n125\n0.983667\n{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 8, ‘min_samples_split’: 7}\n2\n0\n125\n0.983667\n{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 10, ‘min_samples_split’: 10}\n3\n0\n125\n0.983667\n{‘criterion’: ‘log_loss’, ‘max_depth’: None, ‘max_features’: 6, ‘min_samples_split’: 6}\n…\n…\n…\n…\n…\n15\n2\n500\n0.951958\n{‘criterion’: ‘log_loss’, ‘max_depth’: None, ‘max_features’: 9, ‘min_samples_split’: 10}\n16\n2\n500\n0.947958\n{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 10, ‘min_samples_split’: 10}\n17\n2\n500\n0.951958\n{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 10, ‘min_samples_split’: 4}\n18\n3\n1000\n0.961009\n{‘criterion’: ‘log_loss’, ‘max_depth’: None, ‘max_features’: 9, ‘min_samples_split’: 10}\n19\n3\n1000\n0.955989\n{‘criterion’: ‘gini’, ‘max_depth’: None, ‘max_features’: 10, ‘min_samples_split’: 4}\nEach row corresponds to a given parameter combination (a candidate) and a given\niteration. The iteration is given by the\niter\ncolumn. The\nn_resources\ncolumn tells you how many resources were used.\nIn the example above, the best parameter combination is\n{'criterion':\n'log_loss',\n'max_depth':\nNone,\n'max_features':\n9,\n'min_samples_split':\n10}\nsince it has reached the last iteration (3) with the highest score:\n0.96.\nReferences\n[\n1\n]\nK. Jamieson, A. Talwalkar,\nNon-stochastic Best Arm Identification and Hyperparameter\nOptimization\n, in\nproc. of Machine Learning Research, 2016.\n[\n2\n]\nL. Li, K. Jamieson, G. DeSalvo, A. Rostamizadeh, A. Talwalkar,\nHyperband: A Novel Bandit-Based Approach to Hyperparameter Optimization\n, in Machine Learning Research 18, 2018.\n3.2.4.\nTips for parameter search\n#\n3.2.4.1.\nSpecifying an objective metric\n#\nBy default, parameter search uses the\nscore\nfunction of the estimator to\nevaluate a parameter setting. These are the\nsklearn.metrics.accuracy_score\nfor classification and\nsklearn.metrics.r2_score\nfor regression.  For some applications, other\nscoring functions are better suited (for example in unbalanced classification,\nthe accuracy score is often uninformative), see\nWhich scoring function should I use?\nfor some guidance. An alternative scoring function can be specified via the\nscoring\nparameter of most parameter search tools, see\nThe scoring parameter: defining model evaluation rules\nfor more details.\n3.2.4.2.\nSpecifying multiple metrics for evaluation\n#\nGridSearchCV\nand\nRandomizedSearchCV\nallow specifying\nmultiple metrics for the\nscoring\nparameter.\nMultimetric scoring can either be specified as a list of strings of predefined\nscores names or a dict mapping the scorer name to the scorer function and/or\nthe predefined scorer name(s). See\nUsing multiple metric evaluation\nfor more details.\nWhen specifying multiple metrics, the\nrefit\nparameter must be set to the\nmetric (string) for which the\nbest_params_\nwill be found and used to build\nthe\nbest_estimator_\non the whole dataset. If the search should not be\nrefit, set\nrefit=False\n. Leaving refit to the default value\nNone\nwill\nresult in an error when using multiple metrics.\nSee\nDemonstration of multi-metric evaluation on cross_val_score and GridSearchCV\nfor an example usage.\nHalvingRandomSearchCV\nand\nHalvingGridSearchCV\ndo not support\nmultimetric scoring.\n3.2.4.3.\nComposite estimators and parameter spaces\n#\nGridSearchCV\nand\nRandomizedSearchCV\nallow searching over\nparameters of composite or nested estimators such as\nPipeline\n,\nColumnTransformer\n,\nVotingClassifier\nor\nCalibratedClassifierCV\nusing a dedicated\n<estimator>__<parameter>\nsyntax:\n>>>\nfrom\nsklearn.model_selection\nimport\nGridSearchCV\n>>>\nfrom\nsklearn.calibration\nimport\nCalibratedClassifierCV\n>>>\nfrom\nsklearn.ensemble\nimport\nRandomForestClassifier\n>>>\nfrom\nsklearn.datasets\nimport\nmake_moons\n>>>\nX\n,\ny\n=\nmake_moons\n()\n>>>\ncalibrated_forest\n=\nCalibratedClassifierCV\n(\n...\nestimator\n=\nRandomForestClassifier\n(\nn_estimators\n=\n10\n))\n>>>\nparam_grid\n=\n{\n...\n'estimator__max_depth'\n:\n[\n2\n,\n4\n,\n6\n,\n8\n]}\n>>>\nsearch\n=\nGridSearchCV\n(\ncalibrated_forest\n,\nparam_grid\n,\ncv\n=\n5\n)\n>>>\nsearch\n.\nfit\n(\nX\n,\ny\n)\nGridSearchCV(cv=5,\nestimator=CalibratedClassifierCV(estimator=RandomForestClassifier(n_estimators=10)),\nparam_grid={'estimator__max_depth': [2, 4, 6, 8]})\nHere,\n<estimator>\nis the parameter name of the nested estimator,\nin this case\nestimator\n.\nIf the meta-estimator is constructed as a collection of estimators as in\npipeline.Pipeline\n, then\n<estimator>\nrefers to the name of the estimator,\nsee\nAccess to nested parameters\n. In practice, there can be several\nlevels of nesting:\n>>>\nfrom\nsklearn.pipeline\nimport\nPipeline\n>>>\nfrom\nsklearn.feature_selection\nimport\nSelectKBest\n>>>\npipe\n=\nPipeline\n([\n...\n(\n'select'\n,\nSelectKBest\n()),\n...\n(\n'model'\n,\ncalibrated_forest\n)])\n>>>\nparam_grid\n=\n{\n...\n'select__k'\n:\n[\n1\n,\n2\n],\n...\n'model__estimator__max_depth'\n:\n[\n2\n,\n4\n,\n6\n,\n8\n]}\n>>>\nsearch\n=\nGridSearchCV\n(\npipe\n,\nparam_grid\n,\ncv\n=\n5\n)\n.\nfit\n(\nX\n,\ny\n)\nPlease refer to\nPipeline: chaining estimators\nfor performing parameter searches over\npipelines.\n3.2.4.4.\nModel selection: development and evaluation\n#\nModel selection by evaluating various parameter settings can be seen as a way\nto use the labeled data to “train” the parameters of the grid.\nWhen evaluating the resulting model it is important to do it on\nheld-out samples that were not seen during the grid search process:\nit is recommended to split the data into a\ndevelopment set\n(to\nbe fed to the\nGridSearchCV\ninstance) and an\nevaluation set\nto compute performance metrics.\nThis can be done by using the\ntrain_test_split\nutility function.\n3.2.4.5.\nParallelism\n#\nThe parameter search tools evaluate each parameter combination on each data\nfold independently. Computations can be run in parallel by using the keyword\nn_jobs=-1\n. See function signature for more details, and also the Glossary\nentry for\nn_jobs\n.\n3.2.4.6.\nRobustness to failure\n#\nSome parameter settings may result in a failure to\nfit\none or more folds of\nthe data. By default, the score for those settings will be\nnp.nan\n. This can\nbe controlled by setting\nerror_score=\"raise\"\nto raise an exception if one fit\nfails, or for example\nerror_score=0\nto set another value for the score of\nfailing parameter combinations.\n3.2.5.\nAlternatives to brute force parameter search\n#\n3.2.5.1.\nModel specific cross-validation\n#\nSome models can fit data for a range of values of some parameter almost\nas efficiently as fitting the estimator for a single value of the\nparameter. This feature can be leveraged to perform a more efficient\ncross-validation used for model selection of this parameter.\nThe most common parameter amenable to this strategy is the parameter\nencoding the strength of the regularizer. In this case we say that we\ncompute the\nregularization path\nof the estimator.\nHere is the list of such models:\nlinear_model.ElasticNetCV\n(*[, l1_ratio, ...])\nElastic Net model with iterative fitting along a regularization path.\nlinear_model.LarsCV\n(*[, fit_intercept, ...])\nCross-validated Least Angle Regression model.\nlinear_model.LassoCV\n(*[, eps, n_alphas, ...])\nLasso linear model with iterative fitting along a regularization path.\nlinear_model.LassoLarsCV\n(*[, fit_intercept, ...])\nCross-validated Lasso, using the LARS algorithm.\nlinear_model.LogisticRegressionCV\n(*[, Cs, ...])\nLogistic Regression CV (aka logit, MaxEnt) classifier.\nlinear_model.MultiTaskElasticNetCV\n(*[, ...])\nMulti-task L1/L2 ElasticNet with built-in cross-validation.\nlinear_model.MultiTaskLassoCV\n(*[, eps, ...])\nMulti-task Lasso model trained with L1/L2 mixed-norm as regularizer.\nlinear_model.OrthogonalMatchingPursuitCV\n(*)\nCross-validated Orthogonal Matching Pursuit model (OMP).\nlinear_model.RidgeCV\n([alphas, ...])\nRidge regression with built-in cross-validation.\nlinear_model.RidgeClassifierCV\n([alphas, ...])\nRidge classifier with built-in cross-validation.\n3.2.5.2.\nInformation Criterion\n#\nSome models can offer an information-theoretic closed-form formula of the\noptimal estimate of the regularization parameter by computing a single\nregularization path (instead of several when using cross-validation).\nHere is the list of models benefiting from the Akaike Information\nCriterion (AIC) or the Bayesian Information Criterion (BIC) for automated\nmodel selection:\nlinear_model.LassoLarsIC\n([criterion, ...])\nLasso model fit with Lars using BIC or AIC for model selection.\n3.2.5.3.\nOut of Bag Estimates\n#\nWhen using ensemble methods based upon bagging, i.e. generating new\ntraining sets using sampling with replacement, part of the training set\nremains unused.  For each classifier in the ensemble, a different part\nof the training set is left out.\nThis left out portion can be used to estimate the generalization error\nwithout having to rely on a separate validation set.  This estimate\ncomes “for free” as no additional data is needed and can be used for\nmodel selection.\nThis is currently implemented in the following classes:\nensemble.RandomForestClassifier\n([...])\nA random forest classifier.\nensemble.RandomForestRegressor\n([...])\nA random forest regressor.\nensemble.ExtraTreesClassifier\n([...])\nAn extra-trees classifier.\nensemble.ExtraTreesRegressor\n([n_estimators, ...])\nAn extra-trees regressor.\nensemble.GradientBoostingClassifier\n(*[, ...])\nGradient Boosting for classification.\nensemble.GradientBoostingRegressor\n(*[, ...])\nGradient Boosting for regression.",
    "url": "https://scikit-learn.org/stable/modules/grid_search.html"
  },
  {
    "title": "3.1. Cross-validation: evaluating estimator performance — scikit-learn 1.7.0 documentation",
    "content": "3.1.\nCross-validation: evaluating estimator performance\n#\nLearning the parameters of a prediction function and testing it on the\nsame data is a methodological mistake: a model that would just repeat\nthe labels of the samples that it has just seen would have a perfect\nscore but would fail to predict anything useful on yet-unseen data.\nThis situation is called\noverfitting\n.\nTo avoid it, it is common practice when performing\na (supervised) machine learning experiment\nto hold out part of the available data as a\ntest set\nX_test,\ny_test\n.\nNote that the word “experiment” is not intended\nto denote academic use only,\nbecause even in commercial settings\nmachine learning usually starts out experimentally.\nHere is a flowchart of typical cross validation workflow in model training.\nThe best parameters can be determined by\ngrid search\ntechniques.\nIn scikit-learn a random split into training and test sets\ncan be quickly computed with the\ntrain_test_split\nhelper function.\nLet’s load the iris data set to fit a linear support vector machine on it:\n>>>\nimport\nnumpy\nas\nnp\n>>>\nfrom\nsklearn.model_selection\nimport\ntrain_test_split\n>>>\nfrom\nsklearn\nimport\ndatasets\n>>>\nfrom\nsklearn\nimport\nsvm\n>>>\nX\n,\ny\n=\ndatasets\n.\nload_iris\n(\nreturn_X_y\n=\nTrue\n)\n>>>\nX\n.\nshape\n,\ny\n.\nshape\n((150, 4), (150,))\nWe can now quickly sample a training set while holding out 40% of the\ndata for testing (evaluating) our classifier:\n>>>\nX_train\n,\nX_test\n,\ny_train\n,\ny_test\n=\ntrain_test_split\n(\n...\nX\n,\ny\n,\ntest_size\n=\n0.4\n,\nrandom_state\n=\n0\n)\n>>>\nX_train\n.\nshape\n,\ny_train\n.\nshape\n((90, 4), (90,))\n>>>\nX_test\n.\nshape\n,\ny_test\n.\nshape\n((60, 4), (60,))\n>>>\nclf\n=\nsvm\n.\nSVC\n(\nkernel\n=\n'linear'\n,\nC\n=\n1\n)\n.\nfit\n(\nX_train\n,\ny_train\n)\n>>>\nclf\n.\nscore\n(\nX_test\n,\ny_test\n)\n0.96\nWhen evaluating different settings (“hyperparameters”) for estimators,\nsuch as the\nC\nsetting that must be manually set for an SVM,\nthere is still a risk of overfitting\non the test set\nbecause the parameters can be tweaked until the estimator performs optimally.\nThis way, knowledge about the test set can “leak” into the model\nand evaluation metrics no longer report on generalization performance.\nTo solve this problem, yet another part of the dataset can be held out\nas a so-called “validation set”: training proceeds on the training set,\nafter which evaluation is done on the validation set,\nand when the experiment seems to be successful,\nfinal evaluation can be done on the test set.\nHowever, by partitioning the available data into three sets,\nwe drastically reduce the number of samples\nwhich can be used for learning the model,\nand the results can depend on a particular random choice for the pair of\n(train, validation) sets.\nA solution to this problem is a procedure called\ncross-validation\n(CV for short).\nA test set should still be held out for final evaluation,\nbut the validation set is no longer needed when doing CV.\nIn the basic approach, called\nk\n-fold CV,\nthe training set is split into\nk\nsmaller sets\n(other approaches are described below,\nbut generally follow the same principles).\nThe following procedure is followed for each of the\nk\n“folds”:\nA model is trained using\n\\(k-1\\)\nof the folds as training data;\nthe resulting model is validated on the remaining part of the data\n(i.e., it is used as a test set to compute a performance measure\nsuch as accuracy).\nThe performance measure reported by\nk\n-fold cross-validation\nis then the average of the values computed in the loop.\nThis approach can be computationally expensive,\nbut does not waste too much data\n(as is the case when fixing an arbitrary validation set),\nwhich is a major advantage in problems such as inverse inference\nwhere the number of samples is very small.\n3.1.1.\nComputing cross-validated metrics\n#\nThe simplest way to use cross-validation is to call the\ncross_val_score\nhelper function on the estimator and the dataset.\nThe following example demonstrates how to estimate the accuracy of a linear\nkernel support vector machine on the iris dataset by splitting the data, fitting\na model and computing the score 5 consecutive times (with different splits each\ntime):\n>>>\nfrom\nsklearn.model_selection\nimport\ncross_val_score\n>>>\nclf\n=\nsvm\n.\nSVC\n(\nkernel\n=\n'linear'\n,\nC\n=\n1\n,\nrandom_state\n=\n42\n)\n>>>\nscores\n=\ncross_val_score\n(\nclf\n,\nX\n,\ny\n,\ncv\n=\n5\n)\n>>>\nscores\narray([0.96, 1. , 0.96, 0.96, 1. ])\nThe mean score and the standard deviation are hence given by:\n>>>\nprint\n(\n\"\n%0.2f\naccuracy with a standard deviation of\n%0.2f\n\"\n%\n(\nscores\n.\nmean\n(),\nscores\n.\nstd\n()))\n0.98 accuracy with a standard deviation of 0.02\nBy default, the score computed at each CV iteration is the\nscore\nmethod of the estimator. It is possible to change this by using the\nscoring parameter:\n>>>\nfrom\nsklearn\nimport\nmetrics\n>>>\nscores\n=\ncross_val_score\n(\n...\nclf\n,\nX\n,\ny\n,\ncv\n=\n5\n,\nscoring\n=\n'f1_macro'\n)\n>>>\nscores\narray([0.96, 1., 0.96, 0.96, 1.])\nSee\nThe scoring parameter: defining model evaluation rules\nfor details.\nIn the case of the Iris dataset, the samples are balanced across target\nclasses hence the accuracy and the F1-score are almost equal.\nWhen the\ncv\nargument is an integer,\ncross_val_score\nuses the\nKFold\nor\nStratifiedKFold\nstrategies by default, the latter\nbeing used if the estimator derives from\nClassifierMixin\n.\nIt is also possible to use other cross validation strategies by passing a cross\nvalidation iterator instead, for instance:\n>>>\nfrom\nsklearn.model_selection\nimport\nShuffleSplit\n>>>\nn_samples\n=\nX\n.\nshape\n[\n0\n]\n>>>\ncv\n=\nShuffleSplit\n(\nn_splits\n=\n5\n,\ntest_size\n=\n0.3\n,\nrandom_state\n=\n0\n)\n>>>\ncross_val_score\n(\nclf\n,\nX\n,\ny\n,\ncv\n=\ncv\n)\narray([0.977, 0.977, 1., 0.955, 1.])\nAnother option is to use an iterable yielding (train, test) splits as arrays of\nindices, for example:\n>>>\ndef\ncustom_cv_2folds\n(\nX\n):\n...\nn\n=\nX\n.\nshape\n[\n0\n]\n...\ni\n=\n1\n...\nwhile\ni\n<=\n2\n:\n...\nidx\n=\nnp\n.\narange\n(\nn\n*\n(\ni\n-\n1\n)\n/\n2\n,\nn\n*\ni\n/\n2\n,\ndtype\n=\nint\n)\n...\nyield\nidx\n,\nidx\n...\ni\n+=\n1\n...\n>>>\ncustom_cv\n=\ncustom_cv_2folds\n(\nX\n)\n>>>\ncross_val_score\n(\nclf\n,\nX\n,\ny\n,\ncv\n=\ncustom_cv\n)\narray([1.        , 0.973])\nData transformation with held-out data\n#\nJust as it is important to test a predictor on data held-out from\ntraining, preprocessing (such as standardization, feature selection, etc.)\nand similar\ndata transformations\nsimilarly should\nbe learnt from a training set and applied to held-out data for prediction:\n>>>\nfrom\nsklearn\nimport\npreprocessing\n>>>\nX_train\n,\nX_test\n,\ny_train\n,\ny_test\n=\ntrain_test_split\n(\n...\nX\n,\ny\n,\ntest_size\n=\n0.4\n,\nrandom_state\n=\n0\n)\n>>>\nscaler\n=\npreprocessing\n.\nStandardScaler\n()\n.\nfit\n(\nX_train\n)\n>>>\nX_train_transformed\n=\nscaler\n.\ntransform\n(\nX_train\n)\n>>>\nclf\n=\nsvm\n.\nSVC\n(\nC\n=\n1\n)\n.\nfit\n(\nX_train_transformed\n,\ny_train\n)\n>>>\nX_test_transformed\n=\nscaler\n.\ntransform\n(\nX_test\n)\n>>>\nclf\n.\nscore\n(\nX_test_transformed\n,\ny_test\n)\n0.9333\nA\nPipeline\nmakes it easier to compose\nestimators, providing this behavior under cross-validation:\n>>>\nfrom\nsklearn.pipeline\nimport\nmake_pipeline\n>>>\nclf\n=\nmake_pipeline\n(\npreprocessing\n.\nStandardScaler\n(),\nsvm\n.\nSVC\n(\nC\n=\n1\n))\n>>>\ncross_val_score\n(\nclf\n,\nX\n,\ny\n,\ncv\n=\ncv\n)\narray([0.977, 0.933, 0.955, 0.933, 0.977])\nSee\nPipelines and composite estimators\n.\n3.1.1.1.\nThe cross_validate function and multiple metric evaluation\n#\nThe\ncross_validate\nfunction differs from\ncross_val_score\nin\ntwo ways:\nIt allows specifying multiple metrics for evaluation.\nIt returns a dict containing fit-times, score-times\n(and optionally training scores, fitted estimators, train-test split indices)\nin addition to the test score.\nFor single metric evaluation, where the scoring parameter is a string,\ncallable or None, the keys will be -\n['test_score',\n'fit_time',\n'score_time']\nAnd for multiple metric evaluation, the return value is a dict with the\nfollowing keys -\n['test_<scorer1_name>',\n'test_<scorer2_name>',\n'test_<scorer...>',\n'fit_time',\n'score_time']\nreturn_train_score\nis set to\nFalse\nby default to save computation time.\nTo evaluate the scores on the training set as well you need to set it to\nTrue\n. You may also retain the estimator fitted on each training set by\nsetting\nreturn_estimator=True\n. Similarly, you may set\nreturn_indices=True\nto retain the training and testing indices used to split\nthe dataset into train and test sets for each cv split.\nThe multiple metrics can be specified either as a list, tuple or set of\npredefined scorer names:\n>>>\nfrom\nsklearn.model_selection\nimport\ncross_validate\n>>>\nfrom\nsklearn.metrics\nimport\nrecall_score\n>>>\nscoring\n=\n[\n'precision_macro'\n,\n'recall_macro'\n]\n>>>\nclf\n=\nsvm\n.\nSVC\n(\nkernel\n=\n'linear'\n,\nC\n=\n1\n,\nrandom_state\n=\n0\n)\n>>>\nscores\n=\ncross_validate\n(\nclf\n,\nX\n,\ny\n,\nscoring\n=\nscoring\n)\n>>>\nsorted\n(\nscores\n.\nkeys\n())\n['fit_time', 'score_time', 'test_precision_macro', 'test_recall_macro']\n>>>\nscores\n[\n'test_recall_macro'\n]\narray([0.96, 1., 0.96, 0.96, 1.])\nOr as a dict mapping scorer name to a predefined or custom scoring function:\n>>>\nfrom\nsklearn.metrics\nimport\nmake_scorer\n>>>\nscoring\n=\n{\n'prec_macro'\n:\n'precision_macro'\n,\n...\n'rec_macro'\n:\nmake_scorer\n(\nrecall_score\n,\naverage\n=\n'macro'\n)}\n>>>\nscores\n=\ncross_validate\n(\nclf\n,\nX\n,\ny\n,\nscoring\n=\nscoring\n,\n...\ncv\n=\n5\n,\nreturn_train_score\n=\nTrue\n)\n>>>\nsorted\n(\nscores\n.\nkeys\n())\n['fit_time', 'score_time', 'test_prec_macro', 'test_rec_macro',\n'train_prec_macro', 'train_rec_macro']\n>>>\nscores\n[\n'train_rec_macro'\n]\narray([0.97, 0.97, 0.99, 0.98, 0.98])\nHere is an example of\ncross_validate\nusing a single metric:\n>>>\nscores\n=\ncross_validate\n(\nclf\n,\nX\n,\ny\n,\n...\nscoring\n=\n'precision_macro'\n,\ncv\n=\n5\n,\n...\nreturn_estimator\n=\nTrue\n)\n>>>\nsorted\n(\nscores\n.\nkeys\n())\n['estimator', 'fit_time', 'score_time', 'test_score']\n3.1.1.2.\nObtaining predictions by cross-validation\n#\nThe function\ncross_val_predict\nhas a similar interface to\ncross_val_score\n, but returns, for each element in the input, the\nprediction that was obtained for that element when it was in the test set. Only\ncross-validation strategies that assign all elements to a test set exactly once\ncan be used (otherwise, an exception is raised).\nWarning\nNote on inappropriate usage of cross_val_predict\nThe result of\ncross_val_predict\nmay be different from those\nobtained using\ncross_val_score\nas the elements are grouped in\ndifferent ways. The function\ncross_val_score\ntakes an average\nover cross-validation folds, whereas\ncross_val_predict\nsimply\nreturns the labels (or probabilities) from several distinct models\nundistinguished. Thus,\ncross_val_predict\nis not an appropriate\nmeasure of generalization error.\nThe function\ncross_val_predict\nis appropriate for:\nVisualization of predictions obtained from different models.\nModel blending: When predictions of one supervised estimator are used to\ntrain another estimator in ensemble methods.\nThe available cross validation iterators are introduced in the following\nsection.\nExamples\nReceiver Operating Characteristic (ROC) with cross validation\n,\nRecursive feature elimination with cross-validation\n,\nCustom refit strategy of a grid search with cross-validation\n,\nSample pipeline for text feature extraction and evaluation\n,\nPlotting Cross-Validated Predictions\n,\nNested versus non-nested cross-validation\n.\n3.1.2.\nCross validation iterators\n#\nThe following sections list utilities to generate indices\nthat can be used to generate dataset splits according to different cross\nvalidation strategies.\n3.1.2.1.\nCross-validation iterators for i.i.d. data\n#\nAssuming that some data is Independent and Identically Distributed (i.i.d.) is\nmaking the assumption that all samples stem from the same generative process\nand that the generative process is assumed to have no memory of past generated\nsamples.\nThe following cross-validators can be used in such cases.\nNote\nWhile i.i.d. data is a common assumption in machine learning theory, it rarely\nholds in practice. If one knows that the samples have been generated using a\ntime-dependent process, it is safer to\nuse a\ntime-series aware cross-validation scheme\n.\nSimilarly, if we know that the generative process has a group structure\n(samples collected from different subjects, experiments, measurement\ndevices), it is safer to use\ngroup-wise cross-validation\n.\n3.1.2.1.1.\nK-fold\n#\nKFold\ndivides all the samples in\n\\(k\\)\ngroups of samples,\ncalled folds (if\n\\(k = n\\)\n, this is equivalent to the\nLeave One\nOut\nstrategy), of equal sizes (if possible). The prediction function is\nlearned using\n\\(k - 1\\)\nfolds, and the fold left out is used for test.\nExample of 2-fold cross-validation on a dataset with 4 samples:\n>>>\nimport\nnumpy\nas\nnp\n>>>\nfrom\nsklearn.model_selection\nimport\nKFold\n>>>\nX\n=\n[\n\"a\"\n,\n\"b\"\n,\n\"c\"\n,\n\"d\"\n]\n>>>\nkf\n=\nKFold\n(\nn_splits\n=\n2\n)\n>>>\nfor\ntrain\n,\ntest\nin\nkf\n.\nsplit\n(\nX\n):\n...\nprint\n(\n\"\n%s\n%s\n\"\n%\n(\ntrain\n,\ntest\n))\n[2 3] [0 1]\n[0 1] [2 3]\nHere is a visualization of the cross-validation behavior. Note that\nKFold\nis not affected by classes or groups.\nEach fold is constituted by two arrays: the first one is related to the\ntraining set\n, and the second one to the\ntest set\n.\nThus, one can create the training/test sets using numpy indexing:\n>>>\nX\n=\nnp\n.\narray\n([[\n0.\n,\n0.\n],\n[\n1.\n,\n1.\n],\n[\n-\n1.\n,\n-\n1.\n],\n[\n2.\n,\n2.\n]])\n>>>\ny\n=\nnp\n.\narray\n([\n0\n,\n1\n,\n0\n,\n1\n])\n>>>\nX_train\n,\nX_test\n,\ny_train\n,\ny_test\n=\nX\n[\ntrain\n],\nX\n[\ntest\n],\ny\n[\ntrain\n],\ny\n[\ntest\n]\n3.1.2.1.2.\nRepeated K-Fold\n#\nRepeatedKFold\nrepeats\nKFold\n\\(n\\)\ntimes, producing different splits in\neach repetition.\nExample of 2-fold K-Fold repeated 2 times:\n>>>\nimport\nnumpy\nas\nnp\n>>>\nfrom\nsklearn.model_selection\nimport\nRepeatedKFold\n>>>\nX\n=\nnp\n.\narray\n([[\n1\n,\n2\n],\n[\n3\n,\n4\n],\n[\n1\n,\n2\n],\n[\n3\n,\n4\n]])\n>>>\nrandom_state\n=\n12883823\n>>>\nrkf\n=\nRepeatedKFold\n(\nn_splits\n=\n2\n,\nn_repeats\n=\n2\n,\nrandom_state\n=\nrandom_state\n)\n>>>\nfor\ntrain\n,\ntest\nin\nrkf\n.\nsplit\n(\nX\n):\n...\nprint\n(\n\"\n%s\n%s\n\"\n%\n(\ntrain\n,\ntest\n))\n...\n[2 3] [0 1]\n[0 1] [2 3]\n[0 2] [1 3]\n[1 3] [0 2]\nSimilarly,\nRepeatedStratifiedKFold\nrepeats\nStratifiedKFold\n\\(n\\)\ntimes\nwith different randomization in each repetition.\n3.1.2.1.3.\nLeave One Out (LOO)\n#\nLeaveOneOut\n(or LOO) is a simple cross-validation. Each learning\nset is created by taking all the samples except one, the test set being\nthe sample left out. Thus, for\n\\(n\\)\nsamples, we have\n\\(n\\)\ndifferent\ntraining sets and\n\\(n\\)\ndifferent test sets. This cross-validation\nprocedure does not waste much data as only one sample is removed from the\ntraining set:\n>>>\nfrom\nsklearn.model_selection\nimport\nLeaveOneOut\n>>>\nX\n=\n[\n1\n,\n2\n,\n3\n,\n4\n]\n>>>\nloo\n=\nLeaveOneOut\n()\n>>>\nfor\ntrain\n,\ntest\nin\nloo\n.\nsplit\n(\nX\n):\n...\nprint\n(\n\"\n%s\n%s\n\"\n%\n(\ntrain\n,\ntest\n))\n[1 2 3] [0]\n[0 2 3] [1]\n[0 1 3] [2]\n[0 1 2] [3]\nPotential users of LOO for model selection should weigh a few known caveats.\nWhen compared with\n\\(k\\)\n-fold cross validation, one builds\n\\(n\\)\nmodels\nfrom\n\\(n\\)\nsamples instead of\n\\(k\\)\nmodels, where\n\\(n > k\\)\n.\nMoreover, each is trained on\n\\(n - 1\\)\nsamples rather than\n\\((k-1) n / k\\)\n. In both ways, assuming\n\\(k\\)\nis not too large\nand\n\\(k < n\\)\n, LOO is more computationally expensive than\n\\(k\\)\n-fold\ncross validation.\nIn terms of accuracy, LOO often results in high variance as an estimator for the\ntest error. Intuitively, since\n\\(n - 1\\)\nof\nthe\n\\(n\\)\nsamples are used to build each model, models constructed from\nfolds are virtually identical to each other and to the model built from the\nentire training set.\nHowever, if the learning curve is steep for the training size in question,\nthen 5 or 10-fold cross validation can overestimate the generalization error.\nAs a general rule, most authors and empirical evidence suggest that 5 or 10-fold\ncross validation should be preferred to LOO.\nReferences\n#\nhttp://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-12.html\n;\nT. Hastie, R. Tibshirani, J. Friedman,\nThe Elements of Statistical Learning\n, Springer 2009\nL. Breiman, P. Spector\nSubmodel selection and evaluation in regression: The X-random case\n, International Statistical Review 1992;\nR. Kohavi,\nA Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection\n, Intl. Jnt. Conf. AI\nR. Bharat Rao, G. Fung, R. Rosales,\nOn the Dangers of Cross-Validation. An Experimental Evaluation\n, SIAM 2008;\nG. James, D. Witten, T. Hastie, R. Tibshirani,\nAn Introduction to\nStatistical Learning\n, Springer 2013.\n3.1.2.1.4.\nLeave P Out (LPO)\n#\nLeavePOut\nis very similar to\nLeaveOneOut\nas it creates all\nthe possible training/test sets by removing\n\\(p\\)\nsamples from the complete\nset. For\n\\(n\\)\nsamples, this produces\n\\({n \\choose p}\\)\ntrain-test\npairs. Unlike\nLeaveOneOut\nand\nKFold\n, the test sets will\noverlap for\n\\(p > 1\\)\n.\nExample of Leave-2-Out on a dataset with 4 samples:\n>>>\nfrom\nsklearn.model_selection\nimport\nLeavePOut\n>>>\nX\n=\nnp\n.\nones\n(\n4\n)\n>>>\nlpo\n=\nLeavePOut\n(\np\n=\n2\n)\n>>>\nfor\ntrain\n,\ntest\nin\nlpo\n.\nsplit\n(\nX\n):\n...\nprint\n(\n\"\n%s\n%s\n\"\n%\n(\ntrain\n,\ntest\n))\n[2 3] [0 1]\n[1 3] [0 2]\n[1 2] [0 3]\n[0 3] [1 2]\n[0 2] [1 3]\n[0 1] [2 3]\n3.1.2.1.5.\nRandom permutations cross-validation a.k.a. Shuffle & Split\n#\nThe\nShuffleSplit\niterator will generate a user defined number of\nindependent train / test dataset splits. Samples are first shuffled and\nthen split into a pair of train and test sets.\nIt is possible to control the randomness for reproducibility of the\nresults by explicitly seeding the\nrandom_state\npseudo random number\ngenerator.\nHere is a usage example:\n>>>\nfrom\nsklearn.model_selection\nimport\nShuffleSplit\n>>>\nX\n=\nnp\n.\narange\n(\n10\n)\n>>>\nss\n=\nShuffleSplit\n(\nn_splits\n=\n5\n,\ntest_size\n=\n0.25\n,\nrandom_state\n=\n0\n)\n>>>\nfor\ntrain_index\n,\ntest_index\nin\nss\n.\nsplit\n(\nX\n):\n...\nprint\n(\n\"\n%s\n%s\n\"\n%\n(\ntrain_index\n,\ntest_index\n))\n[9 1 6 7 3 0 5] [2 8 4]\n[2 9 8 0 6 7 4] [3 5 1]\n[4 5 1 0 6 9 7] [2 3 8]\n[2 7 5 8 0 3 4] [6 1 9]\n[4 1 0 6 8 9 3] [5 2 7]\nHere is a visualization of the cross-validation behavior. Note that\nShuffleSplit\nis not affected by classes or groups.\nShuffleSplit\nis thus a good alternative to\nKFold\ncross\nvalidation that allows a finer control on the number of iterations and\nthe proportion of samples on each side of the train / test split.\n3.1.2.2.\nCross-validation iterators with stratification based on class labels\n#\nSome classification tasks can naturally exhibit rare classes: for instance,\nthere could be orders of magnitude more negative observations than positive\nobservations (e.g. medical screening, fraud detection, etc). As a result,\ncross-validation splitting can generate train or validation folds without any\noccurrence of a particular class. This typically leads to undefined\nclassification metrics (e.g. ROC AUC), exceptions raised when attempting to\ncall\nfit\nor missing columns in the output of the\npredict_proba\nor\ndecision_function\nmethods of multiclass classifiers trained on different\nfolds.\nTo mitigate such problems, splitters such as\nStratifiedKFold\nand\nStratifiedShuffleSplit\nimplement stratified sampling to ensure that\nrelative class frequencies are approximately preserved in each fold.\nNote\nStratified sampling was introduced in scikit-learn to workaround the\naforementioned engineering problems rather than solve a statistical one.\nStratification makes cross-validation folds more homogeneous, and as a result\nhides some of the variability inherent to fitting models with a limited\nnumber of observations.\nAs a result, stratification can artificially shrink the spread of the metric\nmeasured across cross-validation iterations: the inter-fold variability does\nno longer reflect the uncertainty in the performance of classifiers in the\npresence of rare classes.\n3.1.2.2.1.\nStratified K-fold\n#\nStratifiedKFold\nis a variation of\nK-fold\nwhich returns\nstratified\nfolds: each set contains approximately the same percentage of samples of each\ntarget class as the complete set.\nHere is an example of stratified 3-fold cross-validation on a dataset with 50 samples from\ntwo unbalanced classes.  We show the number of samples in each class and compare with\nKFold\n.\n>>>\nfrom\nsklearn.model_selection\nimport\nStratifiedKFold\n,\nKFold\n>>>\nimport\nnumpy\nas\nnp\n>>>\nX\n,\ny\n=\nnp\n.\nones\n((\n50\n,\n1\n)),\nnp\n.\nhstack\n(([\n0\n]\n*\n45\n,\n[\n1\n]\n*\n5\n))\n>>>\nskf\n=\nStratifiedKFold\n(\nn_splits\n=\n3\n)\n>>>\nfor\ntrain\n,\ntest\nin\nskf\n.\nsplit\n(\nX\n,\ny\n):\n...\nprint\n(\n'train -\n{}\n|   test -\n{}\n'\n.\nformat\n(\n...\nnp\n.\nbincount\n(\ny\n[\ntrain\n]),\nnp\n.\nbincount\n(\ny\n[\ntest\n])))\ntrain -  [30  3]   |   test -  [15  2]\ntrain -  [30  3]   |   test -  [15  2]\ntrain -  [30  4]   |   test -  [15  1]\n>>>\nkf\n=\nKFold\n(\nn_splits\n=\n3\n)\n>>>\nfor\ntrain\n,\ntest\nin\nkf\n.\nsplit\n(\nX\n,\ny\n):\n...\nprint\n(\n'train -\n{}\n|   test -\n{}\n'\n.\nformat\n(\n...\nnp\n.\nbincount\n(\ny\n[\ntrain\n]),\nnp\n.\nbincount\n(\ny\n[\ntest\n])))\ntrain -  [28  5]   |   test -  [17]\ntrain -  [28  5]   |   test -  [17]\ntrain -  [34]   |   test -  [11  5]\nWe can see that\nStratifiedKFold\npreserves the class ratios\n(approximately 1 / 10) in both train and test datasets.\nHere is a visualization of the cross-validation behavior.\nRepeatedStratifiedKFold\ncan be used to repeat Stratified K-Fold n times\nwith different randomization in each repetition.\n3.1.2.2.2.\nStratified Shuffle Split\n#\nStratifiedShuffleSplit\nis a variation of\nShuffleSplit\n, which returns\nstratified splits,\ni.e.\nwhich creates splits by preserving the same\npercentage for each target class as in the complete set.\nHere is a visualization of the cross-validation behavior.\n3.1.2.3.\nPredefined fold-splits / Validation-sets\n#\nFor some datasets, a pre-defined split of the data into training- and\nvalidation fold or into several cross-validation folds already\nexists. Using\nPredefinedSplit\nit is possible to use these folds\ne.g. when searching for hyperparameters.\nFor example, when using a validation set, set the\ntest_fold\nto 0 for all\nsamples that are part of the validation set, and to -1 for all other samples.\n3.1.2.4.\nCross-validation iterators for grouped data\n#\nThe i.i.d. assumption is broken if the underlying generative process yields\ngroups of dependent samples.\nSuch a grouping of data is domain specific. An example would be when there is\nmedical data collected from multiple patients, with multiple samples taken from\neach patient. And such data is likely to be dependent on the individual group.\nIn our example, the patient id for each sample will be its group identifier.\nIn this case we would like to know if a model trained on a particular set of\ngroups generalizes well to the unseen groups. To measure this, we need to\nensure that all the samples in the validation fold come from groups that are\nnot represented at all in the paired training fold.\nThe following cross-validation splitters can be used to do that.\nThe grouping identifier for the samples is specified via the\ngroups\nparameter.\n3.1.2.4.1.\nGroup K-fold\n#\nGroupKFold\nis a variation of K-fold which ensures that the same group is\nnot represented in both testing and training sets. For example if the data is\nobtained from different subjects with several samples per-subject and if the\nmodel is flexible enough to learn from highly person specific features it\ncould fail to generalize to new subjects.\nGroupKFold\nmakes it possible\nto detect this kind of overfitting situations.\nImagine you have three subjects, each with an associated number from 1 to 3:\n>>>\nfrom\nsklearn.model_selection\nimport\nGroupKFold\n>>>\nX\n=\n[\n0.1\n,\n0.2\n,\n2.2\n,\n2.4\n,\n2.3\n,\n4.55\n,\n5.8\n,\n8.8\n,\n9\n,\n10\n]\n>>>\ny\n=\n[\n\"a\"\n,\n\"b\"\n,\n\"b\"\n,\n\"b\"\n,\n\"c\"\n,\n\"c\"\n,\n\"c\"\n,\n\"d\"\n,\n\"d\"\n,\n\"d\"\n]\n>>>\ngroups\n=\n[\n1\n,\n1\n,\n1\n,\n2\n,\n2\n,\n2\n,\n3\n,\n3\n,\n3\n,\n3\n]\n>>>\ngkf\n=\nGroupKFold\n(\nn_splits\n=\n3\n)\n>>>\nfor\ntrain\n,\ntest\nin\ngkf\n.\nsplit\n(\nX\n,\ny\n,\ngroups\n=\ngroups\n):\n...\nprint\n(\n\"\n%s\n%s\n\"\n%\n(\ntrain\n,\ntest\n))\n[0 1 2 3 4 5] [6 7 8 9]\n[0 1 2 6 7 8 9] [3 4 5]\n[3 4 5 6 7 8 9] [0 1 2]\nEach subject is in a different testing fold, and the same subject is never in\nboth testing and training. Notice that the folds do not have exactly the same\nsize due to the imbalance in the data. If class proportions must be balanced\nacross folds,\nStratifiedGroupKFold\nis a better option.\nHere is a visualization of the cross-validation behavior.\nSimilar to\nKFold\n, the test sets from\nGroupKFold\nwill form a\ncomplete partition of all the data.\nWhile\nGroupKFold\nattempts to place the same number of samples in each\nfold when\nshuffle=False\n, when\nshuffle=True\nit attempts to place an equal\nnumber of distinct groups in each fold (but does not account for group sizes).\n3.1.2.4.2.\nStratifiedGroupKFold\n#\nStratifiedGroupKFold\nis a cross-validation scheme that combines both\nStratifiedKFold\nand\nGroupKFold\n. The idea is to try to\npreserve the distribution of classes in each split while keeping each group\nwithin a single split. That might be useful when you have an unbalanced\ndataset so that using just\nGroupKFold\nmight produce skewed splits.\nExample:\n>>>\nfrom\nsklearn.model_selection\nimport\nStratifiedGroupKFold\n>>>\nX\n=\nlist\n(\nrange\n(\n18\n))\n>>>\ny\n=\n[\n1\n]\n*\n6\n+\n[\n0\n]\n*\n12\n>>>\ngroups\n=\n[\n1\n,\n2\n,\n3\n,\n3\n,\n4\n,\n4\n,\n1\n,\n1\n,\n2\n,\n2\n,\n3\n,\n4\n,\n5\n,\n5\n,\n5\n,\n6\n,\n6\n,\n6\n]\n>>>\nsgkf\n=\nStratifiedGroupKFold\n(\nn_splits\n=\n3\n)\n>>>\nfor\ntrain\n,\ntest\nin\nsgkf\n.\nsplit\n(\nX\n,\ny\n,\ngroups\n=\ngroups\n):\n...\nprint\n(\n\"\n%s\n%s\n\"\n%\n(\ntrain\n,\ntest\n))\n[ 0  2  3  4  5  6  7 10 11 15 16 17] [ 1  8  9 12 13 14]\n[ 0  1  4  5  6  7  8  9 11 12 13 14] [ 2  3 10 15 16 17]\n[ 1  2  3  8  9 10 12 13 14 15 16 17] [ 0  4  5  6  7 11]\nImplementation notes\n#\nWith the current implementation full shuffle is not possible in most\nscenarios. When shuffle=True, the following happens:\nAll groups are shuffled.\nGroups are sorted by standard deviation of classes using stable sort.\nSorted groups are iterated over and assigned to folds.\nThat means that only groups with the same standard deviation of class\ndistribution will be shuffled, which might be useful when each group has only\na single class.\nThe algorithm greedily assigns each group to one of n_splits test sets,\nchoosing the test set that minimises the variance in class distribution\nacross test sets. Group assignment proceeds from groups with highest to\nlowest variance in class frequency, i.e. large groups peaked on one or few\nclasses are assigned first.\nThis split is suboptimal in a sense that it might produce imbalanced splits\neven if perfect stratification is possible. If you have relatively close\ndistribution of classes in each group, using\nGroupKFold\nis better.\nHere is a visualization of cross-validation behavior for uneven groups:\n3.1.2.4.3.\nLeave One Group Out\n#\nLeaveOneGroupOut\nis a cross-validation scheme where each split holds\nout samples belonging to one specific group. Group information is\nprovided via an array that encodes the group of each sample.\nEach training set is thus constituted by all the samples except the ones\nrelated to a specific group. This is the same as\nLeavePGroupsOut\nwith\nn_groups=1\nand the same as\nGroupKFold\nwith\nn_splits\nequal to the\nnumber of unique labels passed to the\ngroups\nparameter.\nFor example, in the cases of multiple experiments,\nLeaveOneGroupOut\ncan be used to create a cross-validation based on the different experiments:\nwe create a training set using the samples of all the experiments except one:\n>>>\nfrom\nsklearn.model_selection\nimport\nLeaveOneGroupOut\n>>>\nX\n=\n[\n1\n,\n5\n,\n10\n,\n50\n,\n60\n,\n70\n,\n80\n]\n>>>\ny\n=\n[\n0\n,\n1\n,\n1\n,\n2\n,\n2\n,\n2\n,\n2\n]\n>>>\ngroups\n=\n[\n1\n,\n1\n,\n2\n,\n2\n,\n3\n,\n3\n,\n3\n]\n>>>\nlogo\n=\nLeaveOneGroupOut\n()\n>>>\nfor\ntrain\n,\ntest\nin\nlogo\n.\nsplit\n(\nX\n,\ny\n,\ngroups\n=\ngroups\n):\n...\nprint\n(\n\"\n%s\n%s\n\"\n%\n(\ntrain\n,\ntest\n))\n[2 3 4 5 6] [0 1]\n[0 1 4 5 6] [2 3]\n[0 1 2 3] [4 5 6]\nAnother common application is to use time information: for instance the\ngroups could be the year of collection of the samples and thus allow\nfor cross-validation against time-based splits.\n3.1.2.4.4.\nLeave P Groups Out\n#\nLeavePGroupsOut\nis similar to\nLeaveOneGroupOut\n, but removes\nsamples related to\n\\(P\\)\ngroups for each training/test set. All possible\ncombinations of\n\\(P\\)\ngroups are left out, meaning test sets will overlap\nfor\n\\(P>1\\)\n.\nExample of Leave-2-Group Out:\n>>>\nfrom\nsklearn.model_selection\nimport\nLeavePGroupsOut\n>>>\nX\n=\nnp\n.\narange\n(\n6\n)\n>>>\ny\n=\n[\n1\n,\n1\n,\n1\n,\n2\n,\n2\n,\n2\n]\n>>>\ngroups\n=\n[\n1\n,\n1\n,\n2\n,\n2\n,\n3\n,\n3\n]\n>>>\nlpgo\n=\nLeavePGroupsOut\n(\nn_groups\n=\n2\n)\n>>>\nfor\ntrain\n,\ntest\nin\nlpgo\n.\nsplit\n(\nX\n,\ny\n,\ngroups\n=\ngroups\n):\n...\nprint\n(\n\"\n%s\n%s\n\"\n%\n(\ntrain\n,\ntest\n))\n[4 5] [0 1 2 3]\n[2 3] [0 1 4 5]\n[0 1] [2 3 4 5]\n3.1.2.4.5.\nGroup Shuffle Split\n#\nThe\nGroupShuffleSplit\niterator behaves as a combination of\nShuffleSplit\nand\nLeavePGroupsOut\n, and generates a\nsequence of randomized partitions in which a subset of groups are held\nout for each split. Each train/test split is performed independently meaning\nthere is no guaranteed relationship between successive test sets.\nHere is a usage example:\n>>>\nfrom\nsklearn.model_selection\nimport\nGroupShuffleSplit\n>>>\nX\n=\n[\n0.1\n,\n0.2\n,\n2.2\n,\n2.4\n,\n2.3\n,\n4.55\n,\n5.8\n,\n0.001\n]\n>>>\ny\n=\n[\n\"a\"\n,\n\"b\"\n,\n\"b\"\n,\n\"b\"\n,\n\"c\"\n,\n\"c\"\n,\n\"c\"\n,\n\"a\"\n]\n>>>\ngroups\n=\n[\n1\n,\n1\n,\n2\n,\n2\n,\n3\n,\n3\n,\n4\n,\n4\n]\n>>>\ngss\n=\nGroupShuffleSplit\n(\nn_splits\n=\n4\n,\ntest_size\n=\n0.5\n,\nrandom_state\n=\n0\n)\n>>>\nfor\ntrain\n,\ntest\nin\ngss\n.\nsplit\n(\nX\n,\ny\n,\ngroups\n=\ngroups\n):\n...\nprint\n(\n\"\n%s\n%s\n\"\n%\n(\ntrain\n,\ntest\n))\n...\n[0 1 2 3] [4 5 6 7]\n[2 3 6 7] [0 1 4 5]\n[2 3 4 5] [0 1 6 7]\n[4 5 6 7] [0 1 2 3]\nHere is a visualization of the cross-validation behavior.\nThis class is useful when the behavior of\nLeavePGroupsOut\nis\ndesired, but the number of groups is large enough that generating all\npossible partitions with\n\\(P\\)\ngroups withheld would be prohibitively\nexpensive. In such a scenario,\nGroupShuffleSplit\nprovides\na random sample (with replacement) of the train / test splits\ngenerated by\nLeavePGroupsOut\n.\n3.1.2.5.\nUsing cross-validation iterators to split train and test\n#\nThe above group cross-validation functions may also be useful for splitting a\ndataset into training and testing subsets. Note that the convenience\nfunction\ntrain_test_split\nis a wrapper around\nShuffleSplit\nand thus only allows for stratified splitting (using the class labels)\nand cannot account for groups.\nTo perform the train and test split, use the indices for the train and test\nsubsets yielded by the generator output by the\nsplit()\nmethod of the\ncross-validation splitter. For example:\n>>>\nimport\nnumpy\nas\nnp\n>>>\nfrom\nsklearn.model_selection\nimport\nGroupShuffleSplit\n>>>\nX\n=\nnp\n.\narray\n([\n0.1\n,\n0.2\n,\n2.2\n,\n2.4\n,\n2.3\n,\n4.55\n,\n5.8\n,\n0.001\n])\n>>>\ny\n=\nnp\n.\narray\n([\n\"a\"\n,\n\"b\"\n,\n\"b\"\n,\n\"b\"\n,\n\"c\"\n,\n\"c\"\n,\n\"c\"\n,\n\"a\"\n])\n>>>\ngroups\n=\nnp\n.\narray\n([\n1\n,\n1\n,\n2\n,\n2\n,\n3\n,\n3\n,\n4\n,\n4\n])\n>>>\ntrain_indx\n,\ntest_indx\n=\nnext\n(\n...\nGroupShuffleSplit\n(\nrandom_state\n=\n7\n)\n.\nsplit\n(\nX\n,\ny\n,\ngroups\n)\n...\n)\n>>>\nX_train\n,\nX_test\n,\ny_train\n,\ny_test\n=\n\\\n...\nX\n[\ntrain_indx\n],\nX\n[\ntest_indx\n],\ny\n[\ntrain_indx\n],\ny\n[\ntest_indx\n]\n>>>\nX_train\n.\nshape\n,\nX_test\n.\nshape\n((6,), (2,))\n>>>\nnp\n.\nunique\n(\ngroups\n[\ntrain_indx\n]),\nnp\n.\nunique\n(\ngroups\n[\ntest_indx\n])\n(array([1, 2, 4]), array([3]))\n3.1.2.6.\nCross validation of time series data\n#\nTime series data is characterized by the correlation between observations\nthat are near in time (\nautocorrelation\n). However, classical\ncross-validation techniques such as\nKFold\nand\nShuffleSplit\nassume the samples are independent and\nidentically distributed, and would result in unreasonable correlation\nbetween training and testing instances (yielding poor estimates of\ngeneralization error) on time series data. Therefore, it is very important\nto evaluate our model for time series data on the “future” observations\nleast like those that are used to train the model. To achieve this, one\nsolution is provided by\nTimeSeriesSplit\n.\n3.1.2.6.1.\nTime Series Split\n#\nTimeSeriesSplit\nis a variation of\nk-fold\nwhich\nreturns first\n\\(k\\)\nfolds as train set and the\n\\((k+1)\\)\nth\nfold as test set. Note that unlike standard cross-validation methods,\nsuccessive training sets are supersets of those that come before them.\nAlso, it adds all surplus data to the first training partition, which\nis always used to train the model.\nThis class can be used to cross-validate time series data samples\nthat are observed at fixed time intervals. Indeed, the folds must\nrepresent the same duration, in order to have comparable metrics across folds.\nExample of 3-split time series cross-validation on a dataset with 6 samples:\n>>>\nfrom\nsklearn.model_selection\nimport\nTimeSeriesSplit\n>>>\nX\n=\nnp\n.\narray\n([[\n1\n,\n2\n],\n[\n3\n,\n4\n],\n[\n1\n,\n2\n],\n[\n3\n,\n4\n],\n[\n1\n,\n2\n],\n[\n3\n,\n4\n]])\n>>>\ny\n=\nnp\n.\narray\n([\n1\n,\n2\n,\n3\n,\n4\n,\n5\n,\n6\n])\n>>>\ntscv\n=\nTimeSeriesSplit\n(\nn_splits\n=\n3\n)\n>>>\nprint\n(\ntscv\n)\nTimeSeriesSplit(gap=0, max_train_size=None, n_splits=3, test_size=None)\n>>>\nfor\ntrain\n,\ntest\nin\ntscv\n.\nsplit\n(\nX\n):\n...\nprint\n(\n\"\n%s\n%s\n\"\n%\n(\ntrain\n,\ntest\n))\n[0 1 2] [3]\n[0 1 2 3] [4]\n[0 1 2 3 4] [5]\nHere is a visualization of the cross-validation behavior.\n3.1.3.\nA note on shuffling\n#\nIf the data ordering is not arbitrary (e.g. samples with the same class label\nare contiguous), shuffling it first may be essential to get a meaningful\ncross-validation result. However, the opposite may be true if the samples are not\nindependently and identically distributed. For example, if samples correspond\nto news articles, and are ordered by their time of publication, then shuffling\nthe data will likely lead to a model that is overfit and an inflated validation\nscore: it will be tested on samples that are artificially similar (close in\ntime) to training samples.\nSome cross validation iterators, such as\nKFold\n, have an inbuilt option\nto shuffle the data indices before splitting them. Note that:\nThis consumes less memory than shuffling the data directly.\nBy default no shuffling occurs, including for the (stratified) K fold\ncross-validation performed by specifying\ncv=some_integer\nto\ncross_val_score\n, grid search, etc. Keep in mind that\ntrain_test_split\nstill returns a random split.\nThe\nrandom_state\nparameter defaults to\nNone\n, meaning that the\nshuffling will be different every time\nKFold(...,\nshuffle=True)\nis\niterated. However,\nGridSearchCV\nwill use the same shuffling for each set\nof parameters validated by a single call to its\nfit\nmethod.\nTo get identical results for each split, set\nrandom_state\nto an integer.\nFor more details on how to control the randomness of cv splitters and avoid\ncommon pitfalls, see\nControlling randomness\n.\n3.1.4.\nCross validation and model selection\n#\nCross validation iterators can also be used to directly perform model\nselection using Grid Search for the optimal hyperparameters of the\nmodel. This is the topic of the next section:\nTuning the hyper-parameters of an estimator\n.\n3.1.5.\nPermutation test score\n#\npermutation_test_score\noffers another way\nto evaluate the performance of a\npredictor\n. It provides a\npermutation-based p-value, which represents how likely an observed performance of the\nestimator would be obtained by chance. The null hypothesis in this test is\nthat the estimator fails to leverage any statistical dependency between the\nfeatures and the targets to make correct predictions on left-out data.\npermutation_test_score\ngenerates a null\ndistribution by calculating\nn_permutations\ndifferent permutations of the\ndata. In each permutation the target values are randomly shuffled, thereby removing\nany dependency between the features and the targets. The p-value output is the fraction\nof permutations whose cross-validation score is better or equal than the true score\nwithout permuting targets. For reliable results\nn_permutations\nshould typically be\nlarger than 100 and\ncv\nbetween 3-10 folds.\nA low p-value provides evidence that the dataset contains some real dependency between\nfeatures and targets\nand\nthat the estimator was able to utilize this dependency to\nobtain good results. A high p-value, in reverse, could be due to either one of these:\na lack of dependency between features and targets (i.e., there is no systematic\nrelationship and any observed patterns are likely due to random chance)\nor\nbecause the estimator was not able to use the dependency in the data (for\ninstance because it underfit).\nIn the latter case, using a more appropriate estimator that is able to use the\nstructure in the data, would result in a lower p-value.\nCross-validation provides information about how well an estimator generalizes\nby estimating the range of its expected scores. However, an\nestimator trained on a high dimensional dataset with no structure may still\nperform better than expected on cross-validation, just by chance.\nThis can typically happen with small datasets with less than a few hundred\nsamples.\npermutation_test_score\nprovides information\non whether the estimator has found a real dependency between features and targets and\ncan help in evaluating the performance of the estimator.\nIt is important to note that this test has been shown to produce low\np-values even if there is only weak structure in the data because in the\ncorresponding permutated datasets there is absolutely no structure. This\ntest is therefore only able to show whether the model reliably outperforms\nrandom guessing.\nFinally,\npermutation_test_score\nis computed\nusing brute force and internally fits\n(n_permutations\n+\n1)\n*\nn_cv\nmodels.\nIt is therefore only tractable with small datasets for which fitting an\nindividual model is very fast. Using the\nn_jobs\nparameter parallelizes the\ncomputation and thus speeds it up.\nExamples\nTest with permutations the significance of a classification score\nReferences\n#\nOjala and Garriga.\nPermutation Tests for Studying Classifier Performance\n.\nJ. Mach. Learn. Res. 2010.",
    "url": "https://scikit-learn.org/stable/modules/cross_validation.html"
  },
  {
    "title": "11. Common pitfalls and recommended practices — scikit-learn 1.7.0 documentation",
    "content": "11.\nCommon pitfalls and recommended practices\n#\nThe purpose of this chapter is to illustrate some common pitfalls and\nanti-patterns that occur when using scikit-learn. It provides\nexamples of what\nnot\nto do, along with a corresponding correct\nexample.\n11.1.\nInconsistent preprocessing\n#\nscikit-learn provides a library of\nDataset transformations\n, which\nmay clean (see\nPreprocessing data\n), reduce\n(see\nUnsupervised dimensionality reduction\n), expand (see\nKernel Approximation\n)\nor generate (see\nFeature extraction\n) feature representations.\nIf these data transforms are used when training a model, they also\nmust be used on subsequent datasets, whether it’s test data or\ndata in a production system. Otherwise, the feature space will change,\nand the model will not be able to perform effectively.\nFor the following example, let’s create a synthetic dataset with a\nsingle feature:\n>>>\nfrom\nsklearn.datasets\nimport\nmake_regression\n>>>\nfrom\nsklearn.model_selection\nimport\ntrain_test_split\n>>>\nrandom_state\n=\n42\n>>>\nX\n,\ny\n=\nmake_regression\n(\nrandom_state\n=\nrandom_state\n,\nn_features\n=\n1\n,\nnoise\n=\n1\n)\n>>>\nX_train\n,\nX_test\n,\ny_train\n,\ny_test\n=\ntrain_test_split\n(\n...\nX\n,\ny\n,\ntest_size\n=\n0.4\n,\nrandom_state\n=\nrandom_state\n)\nWrong\nThe train dataset is scaled, but not the test dataset, so model\nperformance on the test dataset is worse than expected:\n>>>\nfrom\nsklearn.metrics\nimport\nmean_squared_error\n>>>\nfrom\nsklearn.linear_model\nimport\nLinearRegression\n>>>\nfrom\nsklearn.preprocessing\nimport\nStandardScaler\n>>>\nscaler\n=\nStandardScaler\n()\n>>>\nX_train_transformed\n=\nscaler\n.\nfit_transform\n(\nX_train\n)\n>>>\nmodel\n=\nLinearRegression\n()\n.\nfit\n(\nX_train_transformed\n,\ny_train\n)\n>>>\nmean_squared_error\n(\ny_test\n,\nmodel\n.\npredict\n(\nX_test\n))\n62.80...\nRight\nInstead of passing the non-transformed\nX_test\nto\npredict\n, we should\ntransform the test data, the same way we transformed the training data:\n>>>\nX_test_transformed\n=\nscaler\n.\ntransform\n(\nX_test\n)\n>>>\nmean_squared_error\n(\ny_test\n,\nmodel\n.\npredict\n(\nX_test_transformed\n))\n0.90...\nAlternatively, we recommend using a\nPipeline\n, which makes it easier to chain transformations\nwith estimators, and reduces the possibility of forgetting a transformation:\n>>>\nfrom\nsklearn.pipeline\nimport\nmake_pipeline\n>>>\nmodel\n=\nmake_pipeline\n(\nStandardScaler\n(),\nLinearRegression\n())\n>>>\nmodel\n.\nfit\n(\nX_train\n,\ny_train\n)\nPipeline(steps=[('standardscaler', StandardScaler()),\n('linearregression', LinearRegression())])\n>>>\nmean_squared_error\n(\ny_test\n,\nmodel\n.\npredict\n(\nX_test\n))\n0.90...\nPipelines also help avoiding another common pitfall: leaking the test data\ninto the training data.\n11.2.\nData leakage\n#\nData leakage occurs when information that would not be available at prediction\ntime is used when building the model. This results in overly optimistic\nperformance estimates, for example from\ncross-validation\n, and thus poorer performance when the model is used\non actually novel data, for example during production.\nA common cause is not keeping the test and train data subsets separate.\nTest data should never be used to make choices about the model.\nThe general rule is to never call\nfit\non the test data\n. While this\nmay sound obvious, this is easy to miss in some cases, for example when\napplying certain pre-processing steps.\nAlthough both train and test data subsets should receive the same\npreprocessing transformation (as described in the previous section), it is\nimportant that these transformations are only learnt from the training data.\nFor example, if you have a\nnormalization step where you divide by the average value, the average should\nbe the average of the train subset,\nnot\nthe average of all the data. If the\ntest subset is included in the average calculation, information from the test\nsubset is influencing the model.\n11.2.1.\nHow to avoid data leakage\n#\nBelow are some tips on avoiding data leakage:\nAlways split the data into train and test subsets first, particularly\nbefore any preprocessing steps.\nNever include test data when using the\nfit\nand\nfit_transform\nmethods. Using all the data, e.g.,\nfit(X)\n, can result in overly optimistic\nscores.\nConversely, the\ntransform\nmethod should be used on both train and test\nsubsets as the same preprocessing should be applied to all the data.\nThis can be achieved by using\nfit_transform\non the train subset and\ntransform\non the test subset.\nThe scikit-learn\npipeline\nis a great way to prevent data\nleakage as it ensures that the appropriate method is performed on the\ncorrect data subset. The pipeline is ideal for use in cross-validation\nand hyper-parameter tuning functions.\nAn example of data leakage during preprocessing is detailed below.\n11.2.2.\nData leakage during pre-processing\n#\nNote\nWe here choose to illustrate data leakage with a feature selection step.\nThis risk of leakage is however relevant with almost all transformations\nin scikit-learn, including (but not limited to)\nStandardScaler\n,\nSimpleImputer\n, and\nPCA\n.\nA number of\nFeature selection\nfunctions are available in scikit-learn.\nThey can help remove irrelevant, redundant and noisy features as well as\nimprove your model build time and performance. As with any other type of\npreprocessing, feature selection should\nonly\nuse the training data.\nIncluding the test data in feature selection will optimistically bias your\nmodel.\nTo demonstrate we will create this binary classification problem with\n10,000 randomly generated features:\n>>>\nimport\nnumpy\nas\nnp\n>>>\nn_samples\n,\nn_features\n,\nn_classes\n=\n200\n,\n10000\n,\n2\n>>>\nrng\n=\nnp\n.\nrandom\n.\nRandomState\n(\n42\n)\n>>>\nX\n=\nrng\n.\nstandard_normal\n((\nn_samples\n,\nn_features\n))\n>>>\ny\n=\nrng\n.\nchoice\n(\nn_classes\n,\nn_samples\n)\nWrong\nUsing all the data to perform feature selection results in an accuracy score\nmuch higher than chance, even though our targets are completely random.\nThis randomness means that our\nX\nand\ny\nare independent and we thus expect\nthe accuracy to be around 0.5. However, since the feature selection step\n‘sees’ the test data, the model has an unfair advantage. In the incorrect\nexample below we first use all the data for feature selection and then split\nthe data into training and test subsets for model fitting. The result is a\nmuch higher than expected accuracy score:\n>>>\nfrom\nsklearn.model_selection\nimport\ntrain_test_split\n>>>\nfrom\nsklearn.feature_selection\nimport\nSelectKBest\n>>>\nfrom\nsklearn.ensemble\nimport\nHistGradientBoostingClassifier\n>>>\nfrom\nsklearn.metrics\nimport\naccuracy_score\n>>>\n# Incorrect preprocessing: the entire data is transformed\n>>>\nX_selected\n=\nSelectKBest\n(\nk\n=\n25\n)\n.\nfit_transform\n(\nX\n,\ny\n)\n>>>\nX_train\n,\nX_test\n,\ny_train\n,\ny_test\n=\ntrain_test_split\n(\n...\nX_selected\n,\ny\n,\nrandom_state\n=\n42\n)\n>>>\ngbc\n=\nHistGradientBoostingClassifier\n(\nrandom_state\n=\n1\n)\n>>>\ngbc\n.\nfit\n(\nX_train\n,\ny_train\n)\nHistGradientBoostingClassifier(random_state=1)\n>>>\ny_pred\n=\ngbc\n.\npredict\n(\nX_test\n)\n>>>\naccuracy_score\n(\ny_test\n,\ny_pred\n)\n0.76\nRight\nTo prevent data leakage, it is good practice to split your data into train\nand test subsets\nfirst\n. Feature selection can then be formed using just\nthe train dataset. Notice that whenever we use\nfit\nor\nfit_transform\n, we\nonly use the train dataset. The score is now what we would expect for the\ndata, close to chance:\n>>>\nX_train\n,\nX_test\n,\ny_train\n,\ny_test\n=\ntrain_test_split\n(\n...\nX\n,\ny\n,\nrandom_state\n=\n42\n)\n>>>\nselect\n=\nSelectKBest\n(\nk\n=\n25\n)\n>>>\nX_train_selected\n=\nselect\n.\nfit_transform\n(\nX_train\n,\ny_train\n)\n>>>\ngbc\n=\nHistGradientBoostingClassifier\n(\nrandom_state\n=\n1\n)\n>>>\ngbc\n.\nfit\n(\nX_train_selected\n,\ny_train\n)\nHistGradientBoostingClassifier(random_state=1)\n>>>\nX_test_selected\n=\nselect\n.\ntransform\n(\nX_test\n)\n>>>\ny_pred\n=\ngbc\n.\npredict\n(\nX_test_selected\n)\n>>>\naccuracy_score\n(\ny_test\n,\ny_pred\n)\n0.5\nHere again, we recommend using a\nPipeline\nto chain\ntogether the feature selection and model estimators. The pipeline ensures\nthat only the training data is used when performing\nfit\nand the test data\nis used only for calculating the accuracy score:\n>>>\nfrom\nsklearn.pipeline\nimport\nmake_pipeline\n>>>\nX_train\n,\nX_test\n,\ny_train\n,\ny_test\n=\ntrain_test_split\n(\n...\nX\n,\ny\n,\nrandom_state\n=\n42\n)\n>>>\npipeline\n=\nmake_pipeline\n(\nSelectKBest\n(\nk\n=\n25\n),\n...\nHistGradientBoostingClassifier\n(\nrandom_state\n=\n1\n))\n>>>\npipeline\n.\nfit\n(\nX_train\n,\ny_train\n)\nPipeline(steps=[('selectkbest', SelectKBest(k=25)),\n('histgradientboostingclassifier',\nHistGradientBoostingClassifier(random_state=1))])\n>>>\ny_pred\n=\npipeline\n.\npredict\n(\nX_test\n)\n>>>\naccuracy_score\n(\ny_test\n,\ny_pred\n)\n0.5\nThe pipeline can also be fed into a cross-validation\nfunction such as\ncross_val_score\n.\nAgain, the pipeline ensures that the correct data subset and estimator\nmethod is used during fitting and predicting:\n>>>\nfrom\nsklearn.model_selection\nimport\ncross_val_score\n>>>\nscores\n=\ncross_val_score\n(\npipeline\n,\nX\n,\ny\n)\n>>>\nprint\n(\nf\n\"Mean accuracy:\n{\nscores\n.\nmean\n()\n:\n.2f\n}\n+/-\n{\nscores\n.\nstd\n()\n:\n.2f\n}\n\"\n)\nMean accuracy: 0.43+/-0.05\n11.3.\nControlling randomness\n#\nSome scikit-learn objects are inherently random. These are usually estimators\n(e.g.\nRandomForestClassifier\n) and cross-validation\nsplitters (e.g.\nKFold\n). The randomness of\nthese objects is controlled via their\nrandom_state\nparameter, as described\nin the\nGlossary\n. This section expands on the glossary\nentry, and describes good practices and common pitfalls w.r.t. this\nsubtle parameter.\nNote\nRecommendation summary\nFor an optimal robustness of cross-validation (CV) results, pass\nRandomState\ninstances when creating estimators, or leave\nrandom_state\nto\nNone\n. Passing integers to CV splitters is usually the safest option\nand is preferable; passing\nRandomState\ninstances to splitters may\nsometimes be useful to achieve very specific use-cases.\nFor both estimators and splitters, passing an integer vs passing an\ninstance (or\nNone\n) leads to subtle but significant differences,\nespecially for CV procedures. These differences are important to\nunderstand when reporting results.\nFor reproducible results across executions, remove any use of\nrandom_state=None\n.\n11.3.1.\nUsing\nNone\nor\nRandomState\ninstances, and repeated calls to\nfit\nand\nsplit\n#\nThe\nrandom_state\nparameter determines whether multiple calls to\nfit\n(for estimators) or to\nsplit\n(for CV splitters) will produce the same\nresults, according to these rules:\nIf an integer is passed, calling\nfit\nor\nsplit\nmultiple times always\nyields the same results.\nIf\nNone\nor a\nRandomState\ninstance is passed:\nfit\nand\nsplit\nwill\nyield different results each time they are called, and the succession of\ncalls explores all sources of entropy.\nNone\nis the default value for all\nrandom_state\nparameters.\nWe here illustrate these rules for both estimators and CV splitters.\nNote\nSince passing\nrandom_state=None\nis equivalent to passing the global\nRandomState\ninstance from\nnumpy\n(\nrandom_state=np.random.mtrand._rand\n), we will not explicitly mention\nNone\nhere. Everything that applies to instances also applies to using\nNone\n.\n11.3.1.1.\nEstimators\n#\nPassing instances means that calling\nfit\nmultiple times will not yield the\nsame results, even if the estimator is fitted on the same data and with the\nsame hyper-parameters:\n>>>\nfrom\nsklearn.linear_model\nimport\nSGDClassifier\n>>>\nfrom\nsklearn.datasets\nimport\nmake_classification\n>>>\nimport\nnumpy\nas\nnp\n>>>\nrng\n=\nnp\n.\nrandom\n.\nRandomState\n(\n0\n)\n>>>\nX\n,\ny\n=\nmake_classification\n(\nn_features\n=\n5\n,\nrandom_state\n=\nrng\n)\n>>>\nsgd\n=\nSGDClassifier\n(\nrandom_state\n=\nrng\n)\n>>>\nsgd\n.\nfit\n(\nX\n,\ny\n)\n.\ncoef_\narray([[ 8.85418642,  4.79084103, -3.13077794,  8.11915045, -0.56479934]])\n>>>\nsgd\n.\nfit\n(\nX\n,\ny\n)\n.\ncoef_\narray([[ 6.70814003,  5.25291366, -7.55212743,  5.18197458,  1.37845099]])\nWe can see from the snippet above that repeatedly calling\nsgd.fit\nhas\nproduced different models, even if the data was the same. This is because the\nRandom Number Generator (RNG) of the estimator is consumed (i.e. mutated)\nwhen\nfit\nis called, and this mutated RNG will be used in the subsequent\ncalls to\nfit\n. In addition, the\nrng\nobject is shared across all objects\nthat use it, and as a consequence, these objects become somewhat\ninter-dependent. For example, two estimators that share the same\nRandomState\ninstance will influence each other, as we will see later when\nwe discuss cloning. This point is important to keep in mind when debugging.\nIf we had passed an integer to the\nrandom_state\nparameter of the\nSGDClassifier\n, we would have obtained the\nsame models, and thus the same scores each time. When we pass an integer, the\nsame RNG is used across all calls to\nfit\n. What internally happens is that\neven though the RNG is consumed when\nfit\nis called, it is always reset to\nits original state at the beginning of\nfit\n.\n11.3.1.2.\nCV splitters\n#\nRandomized CV splitters have a similar behavior when a\nRandomState\ninstance is passed; calling\nsplit\nmultiple times yields different data\nsplits:\n>>>\nfrom\nsklearn.model_selection\nimport\nKFold\n>>>\nimport\nnumpy\nas\nnp\n>>>\nX\n=\ny\n=\nnp\n.\narange\n(\n10\n)\n>>>\nrng\n=\nnp\n.\nrandom\n.\nRandomState\n(\n0\n)\n>>>\ncv\n=\nKFold\n(\nn_splits\n=\n2\n,\nshuffle\n=\nTrue\n,\nrandom_state\n=\nrng\n)\n>>>\nfor\ntrain\n,\ntest\nin\ncv\n.\nsplit\n(\nX\n,\ny\n):\n...\nprint\n(\ntrain\n,\ntest\n)\n[0 3 5 6 7] [1 2 4 8 9]\n[1 2 4 8 9] [0 3 5 6 7]\n>>>\nfor\ntrain\n,\ntest\nin\ncv\n.\nsplit\n(\nX\n,\ny\n):\n...\nprint\n(\ntrain\n,\ntest\n)\n[0 4 6 7 8] [1 2 3 5 9]\n[1 2 3 5 9] [0 4 6 7 8]\nWe can see that the splits are different from the second time\nsplit\nis\ncalled. This may lead to unexpected results if you compare the performance of\nmultiple estimators by calling\nsplit\nmany times, as we will see in the next\nsection.\n11.3.2.\nCommon pitfalls and subtleties\n#\nWhile the rules that govern the\nrandom_state\nparameter are seemingly simple,\nthey do however have some subtle implications. In some cases, this can even\nlead to wrong conclusions.\n11.3.2.1.\nEstimators\n#\nDifferent `random_state` types lead to different cross-validation\nprocedures\nDepending on the type of the\nrandom_state\nparameter, estimators will behave\ndifferently, especially in cross-validation procedures. Consider the\nfollowing snippet:\n>>>\nfrom\nsklearn.ensemble\nimport\nRandomForestClassifier\n>>>\nfrom\nsklearn.datasets\nimport\nmake_classification\n>>>\nfrom\nsklearn.model_selection\nimport\ncross_val_score\n>>>\nimport\nnumpy\nas\nnp\n>>>\nX\n,\ny\n=\nmake_classification\n(\nrandom_state\n=\n0\n)\n>>>\nrf_123\n=\nRandomForestClassifier\n(\nrandom_state\n=\n123\n)\n>>>\ncross_val_score\n(\nrf_123\n,\nX\n,\ny\n)\narray([0.85, 0.95, 0.95, 0.9 , 0.9 ])\n>>>\nrf_inst\n=\nRandomForestClassifier\n(\nrandom_state\n=\nnp\n.\nrandom\n.\nRandomState\n(\n0\n))\n>>>\ncross_val_score\n(\nrf_inst\n,\nX\n,\ny\n)\narray([0.9 , 0.95, 0.95, 0.9 , 0.9 ])\nWe see that the cross-validated scores of\nrf_123\nand\nrf_inst\nare\ndifferent, as should be expected since we didn’t pass the same\nrandom_state\nparameter. However, the difference between these scores is more subtle than\nit looks, and\nthe cross-validation procedures that were performed by\ncross_val_score\nsignificantly differ in\neach case\n:\nSince\nrf_123\nwas passed an integer, every call to\nfit\nuses the same RNG:\nthis means that all random characteristics of the random forest estimator\nwill be the same for each of the 5 folds of the CV procedure. In\nparticular, the (randomly chosen) subset of features of the estimator will\nbe the same across all folds.\nSince\nrf_inst\nwas passed a\nRandomState\ninstance, each call to\nfit\nstarts from a different RNG. As a result, the random subset of features\nwill be different for each fold.\nWhile having a constant estimator RNG across folds isn’t inherently wrong, we\nusually want CV results that are robust w.r.t. the estimator’s randomness. As\na result, passing an instance instead of an integer may be preferable, since\nit will allow the estimator RNG to vary for each fold.\nNote\nHere,\ncross_val_score\nwill use a\nnon-randomized CV splitter (as is the default), so both estimators will\nbe evaluated on the same splits. This section is not about variability in\nthe splits. Also, whether we pass an integer or an instance to\nmake_classification\nisn’t relevant for our\nillustration purpose: what matters is what we pass to the\nRandomForestClassifier\nestimator.\nCloning\n#\nAnother subtle side effect of passing\nRandomState\ninstances is how\nclone\nwill work:\n>>>\nfrom\nsklearn\nimport\nclone\n>>>\nfrom\nsklearn.ensemble\nimport\nRandomForestClassifier\n>>>\nimport\nnumpy\nas\nnp\n>>>\nrng\n=\nnp\n.\nrandom\n.\nRandomState\n(\n0\n)\n>>>\na\n=\nRandomForestClassifier\n(\nrandom_state\n=\nrng\n)\n>>>\nb\n=\nclone\n(\na\n)\nSince a\nRandomState\ninstance was passed to\na\n,\na\nand\nb\nare not clones\nin the strict sense, but rather clones in the statistical sense:\na\nand\nb\nwill still be different models, even when calling\nfit(X,\ny)\non the same\ndata. Moreover,\na\nand\nb\nwill influence each other since they share the\nsame internal RNG: calling\na.fit\nwill consume\nb\n’s RNG, and calling\nb.fit\nwill consume\na\n’s RNG, since they are the same. This bit is true for\nany estimators that share a\nrandom_state\nparameter; it is not specific to\nclones.\nIf an integer were passed,\na\nand\nb\nwould be exact clones and they would not\ninfluence each other.\nWarning\nEven though\nclone\nis rarely used in user code, it is\ncalled pervasively throughout scikit-learn codebase: in particular, most\nmeta-estimators that accept non-fitted estimators call\nclone\ninternally\n(\nGridSearchCV\n,\nStackingClassifier\n,\nCalibratedClassifierCV\n, etc.).\n11.3.2.2.\nCV splitters\n#\nWhen passed a\nRandomState\ninstance, CV splitters yield different splits\neach time\nsplit\nis called. When comparing different estimators, this can\nlead to overestimating the variance of the difference in performance between\nthe estimators:\n>>>\nfrom\nsklearn.naive_bayes\nimport\nGaussianNB\n>>>\nfrom\nsklearn.discriminant_analysis\nimport\nLinearDiscriminantAnalysis\n>>>\nfrom\nsklearn.datasets\nimport\nmake_classification\n>>>\nfrom\nsklearn.model_selection\nimport\nKFold\n>>>\nfrom\nsklearn.model_selection\nimport\ncross_val_score\n>>>\nimport\nnumpy\nas\nnp\n>>>\nrng\n=\nnp\n.\nrandom\n.\nRandomState\n(\n0\n)\n>>>\nX\n,\ny\n=\nmake_classification\n(\nrandom_state\n=\nrng\n)\n>>>\ncv\n=\nKFold\n(\nshuffle\n=\nTrue\n,\nrandom_state\n=\nrng\n)\n>>>\nlda\n=\nLinearDiscriminantAnalysis\n()\n>>>\nnb\n=\nGaussianNB\n()\n>>>\nfor\nest\nin\n(\nlda\n,\nnb\n):\n...\nprint\n(\ncross_val_score\n(\nest\n,\nX\n,\ny\n,\ncv\n=\ncv\n))\n[0.8  0.75 0.75 0.7  0.85]\n[0.85 0.95 0.95 0.85 0.95]\nDirectly comparing the performance of the\nLinearDiscriminantAnalysis\nestimator\nvs the\nGaussianNB\nestimator\non each fold\nwould\nbe a mistake:\nthe splits on which the estimators are evaluated are\ndifferent\n. Indeed,\ncross_val_score\nwill\ninternally call\ncv.split\non the same\nKFold\ninstance, but the splits will be\ndifferent each time. This is also true for any tool that performs model\nselection via cross-validation, e.g.\nGridSearchCV\nand\nRandomizedSearchCV\n: scores are not\ncomparable fold-to-fold across different calls to\nsearch.fit\n, since\ncv.split\nwould have been called multiple times. Within a single call to\nsearch.fit\n, however, fold-to-fold comparison is possible since the search\nestimator only calls\ncv.split\nonce.\nFor comparable fold-to-fold results in all scenarios, one should pass an\ninteger to the CV splitter:\ncv\n=\nKFold(shuffle=True,\nrandom_state=0)\n.\nNote\nWhile fold-to-fold comparison is not advisable with\nRandomState\ninstances, one can however expect that average scores allow to conclude\nwhether one estimator is better than another, as long as enough folds and\ndata are used.\nNote\nWhat matters in this example is what was passed to\nKFold\n. Whether we pass a\nRandomState\ninstance or an integer to\nmake_classification\nis not relevant for our illustration purpose. Also, neither\nLinearDiscriminantAnalysis\nnor\nGaussianNB\nare randomized estimators.\n11.3.3.\nGeneral recommendations\n#\n11.3.3.1.\nGetting reproducible results across multiple executions\n#\nIn order to obtain reproducible (i.e. constant) results across multiple\nprogram executions\n, we need to remove all uses of\nrandom_state=None\n, which\nis the default. The recommended way is to declare a\nrng\nvariable at the top\nof the program, and pass it down to any object that accepts a\nrandom_state\nparameter:\n>>>\nfrom\nsklearn.ensemble\nimport\nRandomForestClassifier\n>>>\nfrom\nsklearn.datasets\nimport\nmake_classification\n>>>\nfrom\nsklearn.model_selection\nimport\ntrain_test_split\n>>>\nimport\nnumpy\nas\nnp\n>>>\nrng\n=\nnp\n.\nrandom\n.\nRandomState\n(\n0\n)\n>>>\nX\n,\ny\n=\nmake_classification\n(\nrandom_state\n=\nrng\n)\n>>>\nrf\n=\nRandomForestClassifier\n(\nrandom_state\n=\nrng\n)\n>>>\nX_train\n,\nX_test\n,\ny_train\n,\ny_test\n=\ntrain_test_split\n(\nX\n,\ny\n,\n...\nrandom_state\n=\nrng\n)\n>>>\nrf\n.\nfit\n(\nX_train\n,\ny_train\n)\n.\nscore\n(\nX_test\n,\ny_test\n)\n0.84\nWe are now guaranteed that the result of this script will always be 0.84, no\nmatter how many times we run it. Changing the global\nrng\nvariable to a\ndifferent value should affect the results, as expected.\nIt is also possible to declare the\nrng\nvariable as an integer. This may\nhowever lead to less robust cross-validation results, as we will see in the\nnext section.\nNote\nWe do not recommend setting the global\nnumpy\nseed by calling\nnp.random.seed(0)\n. See\nhere\nfor a discussion.\n11.3.3.2.\nRobustness of cross-validation results\n#\nWhen we evaluate a randomized estimator performance by cross-validation, we\nwant to make sure that the estimator can yield accurate predictions for new\ndata, but we also want to make sure that the estimator is robust w.r.t. its\nrandom initialization. For example, we would like the random weights\ninitialization of an\nSGDClassifier\nto be\nconsistently good across all folds: otherwise, when we train that estimator\non new data, we might get unlucky and the random initialization may lead to\nbad performance. Similarly, we want a random forest to be robust w.r.t. the\nset of randomly selected features that each tree will be using.\nFor these reasons, it is preferable to evaluate the cross-validation\nperformance by letting the estimator use a different RNG on each fold. This\nis done by passing a\nRandomState\ninstance (or\nNone\n) to the estimator\ninitialization.\nWhen we pass an integer, the estimator will use the same RNG on each fold:\nif the estimator performs well (or bad), as evaluated by CV, it might just be\nbecause we got lucky (or unlucky) with that specific seed. Passing instances\nleads to more robust CV results, and makes the comparison between various\nalgorithms fairer. It also helps limiting the temptation to treat the\nestimator’s RNG as a hyper-parameter that can be tuned.\nWhether we pass\nRandomState\ninstances or integers to CV splitters has no\nimpact on robustness, as long as\nsplit\nis only called once. When\nsplit\nis called multiple times, fold-to-fold comparison isn’t possible anymore. As\na result, passing integer to CV splitters is usually safer and covers most\nuse-cases.",
    "url": "https://scikit-learn.org/stable/common_pitfalls.html"
  }
]