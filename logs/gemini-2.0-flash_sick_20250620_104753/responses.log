[2025-06-20 10:48:11] [Metadata: {'component': 'config'}] ```python
from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, InCondition

def get_configspace() -> ConfigurationSpace:
    """
    Creates a configuration space for machine learning hyperparameter optimization,
    specifically tailored for the 'sick' dataset, which is a tabular dataset
    with mixed categorical and numerical features and class imbalance.

    Returns:
        ConfigurationSpace: A configuration space object.
    """
    cs = ConfigurationSpace()

    # --- Random Forest Hyperparameters ---
    n_estimators = UniformIntegerHyperparameter(
        "n_estimators", lower=10, upper=200, default_value=100,
        docstring="The number of trees in the forest."
    )
    cs.add_hyperparameter(n_estimators)

    max_features = UniformFloatHyperparameter(
        "max_features", lower=0.1, upper=1.0, default_value=0.5,
        docstring="The number of features to consider when looking for the best split."
                  "Can be specified as a float (percentage of features) or an int (number of features)."
    )
    cs.add_hyperparameter(max_features)

    min_samples_split = UniformIntegerHyperparameter(
        "min_samples_split", lower=2, upper=20, default_value=2,
        docstring="The minimum number of samples required to split an internal node."
    )
    cs.add_hyperparameter(min_samples_split)

    min_samples_leaf = UniformIntegerHyperparameter(
        "min_samples_leaf", lower=1, upper=10, default_value=1,
        docstring="The minimum number of samples required to be at a leaf node."
    )
    cs.add_hyperparameter(min_samples_leaf)

    # --- Gradient Boosting Hyperparameters ---
    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-4, upper=1e-1, default_value=1e-2, log=True,
        docstring="Learning rate shrinks the contribution of each tree by learning_rate."
    )
    cs.add_hyperparameter(learning_rate)

    max_depth = UniformIntegerHyperparameter(
        "max_depth", lower=2, upper=10, default_value=3,
        docstring="Maximum depth of the individual regression estimators."
    )
    cs.add_hyperparameter(max_depth)

    # --- SVM Hyperparameters ---
    C = UniformFloatHyperparameter(
        "C", lower=0.001, upper=10, default_value=1.0, log=True,
        docstring="Regularization parameter. The strength of the regularization is inversely proportional to C."
    )
    cs.add_hyperparameter(C)

    kernel = CategoricalHyperparameter(
        "kernel", choices=["linear", "rbf", "poly", "sigmoid"], default_value="rbf",
        docstring="Specifies the kernel type to be used in the algorithm."
    )
    cs.add_hyperparameter(kernel)

    degree = UniformIntegerHyperparameter(
        "degree", lower=2, upper=5, default_value=3,
        docstring="Degree of the polynomial kernel function ('poly'). Ignored by all other kernels."
    )
    cs.add_hyperparameter(degree)
    degree_condition = InCondition(child=degree, parent=kernel, values=["poly"])
    cs.add_condition(degree_condition)


    gamma = CategoricalHyperparameter(
        "gamma", choices=["scale", "auto"], default_value="scale",
        docstring="Kernel coefficient for 'rbf', 'poly' and 'sigmoid'."
    )
    cs.add_hyperparameter(gamma)
    gamma_condition = InCondition(child=gamma, parent=kernel, values=["rbf", "poly", "sigmoid"])
    cs.add_condition(gamma_condition)


    # --- Logistic Regression Hyperparameters ---
    penalty = CategoricalHyperparameter(
        "penalty", choices=["l1", "l2", "elasticnet", "none"], default_value="l2",
        docstring="Used to specify the norm used in the penalization."
    )
    cs.add_hyperparameter(penalty)

    solver = CategoricalHyperparameter(
        "solver", choices=["newton-cg", "lbfgs", "liblinear", "sag", "saga"], default_value="lbfgs",
        docstring="Algorithm to use in the optimization problem."
    )
    cs.add_hyperparameter(solver)

    l1_ratio = UniformFloatHyperparameter(
        "l1_ratio", lower=0.0, upper=1.0, default_value=0.5,
        docstring="The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``."
    )
    cs.add_hyperparameter(l1_ratio)
    l1_ratio_condition = InCondition(child=l1_ratio, parent=penalty, values=["elasticnet"])
    cs.add_condition(l1_ratio_condition)

    # --- Data Preprocessing (Example - Not exhaustive) ---
    imputation_strategy_numerical = CategoricalHyperparameter(
        "imputation_strategy_numerical", choices=["mean", "median"], default_value="mean",
        docstring="Strategy to use for imputing missing numerical values."
    )
    cs.add_hyperparameter(imputation_strategy_numerical)

    imputation_strategy_categorical = CategoricalHyperparameter(
        "imputation_strategy_categorical", choices=["most_frequent", "constant"], default_value="most_frequent",
        docstring="Strategy to use for imputing missing categorical values."
    )
    cs.add_hyperparameter(imputation_strategy_categorical)


    return cs
```
--------------------------------------------------------------------------------
[2025-06-20 10:48:11] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, InCondition

def get_configspace() -> ConfigurationSpace:
    """
    Creates a configuration space for machine learning hyperparameter optimization,
    specifically tailored for the 'sick' dataset, which is a tabular dataset
    with mixed categorical and numerical features and class imbalance.

    Returns:
        ConfigurationSpace: A configuration space object.
    """
    cs = ConfigurationSpace()

    # --- Random Forest Hyperparameters ---
    n_estimators = UniformIntegerHyperparameter(
        "n_estimators", lower=10, upper=200, default_value=100,
        docstring="The number of trees in the forest."
    )
    cs.add_hyperparameter(n_estimators)

    max_features = UniformFloatHyperparameter(
        "max_features", lower=0.1, upper=1.0, default_value=0.5,
        docstring="The number of features to consider when looking for the best split."
                  "Can be specified as a float (percentage of features) or an int (number of features)."
    )
    cs.add_hyperparameter(max_features)

    min_samples_split = UniformIntegerHyperparameter(
        "min_samples_split", lower=2, upper=20, default_value=2,
        docstring="The minimum number of samples required to split an internal node."
    )
    cs.add_hyperparameter(min_samples_split)

    min_samples_leaf = UniformIntegerHyperparameter(
        "min_samples_leaf", lower=1, upper=10, default_value=1,
        docstring="The minimum number of samples required to be at a leaf node."
    )
    cs.add_hyperparameter(min_samples_leaf)

    # --- Gradient Boosting Hyperparameters ---
    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-4, upper=1e-1, default_value=1e-2, log=True,
        docstring="Learning rate shrinks the contribution of each tree by learning_rate."
    )
    cs.add_hyperparameter(learning_rate)

    max_depth = UniformIntegerHyperparameter(
        "max_depth", lower=2, upper=10, default_value=3,
        docstring="Maximum depth of the individual regression estimators."
    )
    cs.add_hyperparameter(max_depth)

    # --- SVM Hyperparameters ---
    C = UniformFloatHyperparameter(
        "C", lower=0.001, upper=10, default_value=1.0, log=True,
        docstring="Regularization parameter. The strength of the regularization is inversely proportional to C."
    )
    cs.add_hyperparameter(C)

    kernel = CategoricalHyperparameter(
        "kernel", choices=["linear", "rbf", "poly", "sigmoid"], default_value="rbf",
        docstring="Specifies the kernel type to be used in the algorithm."
    )
    cs.add_hyperparameter(kernel)

    degree = UniformIntegerHyperparameter(
        "degree", lower=2, upper=5, default_value=3,
        docstring="Degree of the polynomial kernel function ('poly'). Ignored by all other kernels."
    )
    cs.add_hyperparameter(degree)
    degree_condition = InCondition(child=degree, parent=kernel, values=["poly"])
    cs.add_condition(degree_condition)


    gamma = CategoricalHyperparameter(
        "gamma", choices=["scale", "auto"], default_value="scale",
        docstring="Kernel coefficient for 'rbf', 'poly' and 'sigmoid'."
    )
    cs.add_hyperparameter(gamma)
    gamma_condition = InCondition(child=gamma, parent=kernel, values=["rbf", "poly", "sigmoid"])
    cs.add_condition(gamma_condition)


    # --- Logistic Regression Hyperparameters ---
    penalty = CategoricalHyperparameter(
        "penalty", choices=["l1", "l2", "elasticnet", "none"], default_value="l2",
        docstring="Used to specify the norm used in the penalization."
    )
    cs.add_hyperparameter(penalty)

    solver = CategoricalHyperparameter(
        "solver", choices=["newton-cg", "lbfgs", "liblinear", "sag", "saga"], default_value="lbfgs",
        docstring="Algorithm to use in the optimization problem."
    )
    cs.add_hyperparameter(solver)

    l1_ratio = UniformFloatHyperparameter(
        "l1_ratio", lower=0.0, upper=1.0, default_value=0.5,
        docstring="The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``."
    )
    cs.add_hyperparameter(l1_ratio)
    l1_ratio_condition = InCondition(child=l1_ratio, parent=penalty, values=["elasticnet"])
    cs.add_condition(l1_ratio_condition)

    # --- Data Preprocessing (Example - Not exhaustive) ---
    imputation_strategy_numerical = CategoricalHyperparameter(
        "imputation_strategy_numerical", choices=["mean", "median"], default_value="mean",
        docstring="Strategy to use for imputing missing numerical values."
    )
    cs.add_hyperparameter(imputation_strategy_numerical)

    imputation_strategy_categorical = CategoricalHyperparameter(
        "imputation_strategy_categorical", choices=["most_frequent", "constant"], default_value="most_frequent",
        docstring="Strategy to use for imputing missing categorical values."
    )
    cs.add_hyperparameter(imputation_strategy_categorical)


    return cs

--------------------------------------------------------------------------------
[2025-06-20 10:48:11] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#1): UniformIntegerHyperparameter.__init__() got an unexpected keyword argument 'docstring'
--------------------------------------------------------------------------------
[2025-06-20 10:48:18] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, InCondition

def get_configspace() -> ConfigurationSpace:
    """
    Creates a configuration space for machine learning hyperparameter optimization,
    specifically tailored for the 'sick' dataset, which is a tabular dataset
    with mixed categorical and numerical features and class imbalance.

    Returns:
        ConfigurationSpace: A configuration space object.
    """
    cs = ConfigurationSpace()

    # --- Random Forest Hyperparameters ---
    n_estimators = UniformIntegerHyperparameter(
        "n_estimators", lower=10, upper=200, default_value=100
    )
    cs.add_hyperparameter(n_estimators)
    n_estimators.doc = "The number of trees in the forest."

    max_features = UniformFloatHyperparameter(
        "max_features", lower=0.1, upper=1.0, default_value=0.5
    )
    cs.add_hyperparameter(max_features)
    max_features.doc = "The number of features to consider when looking for the best split. Can be specified as a float (percentage of features) or an int (number of features)."

    min_samples_split = UniformIntegerHyperparameter(
        "min_samples_split", lower=2, upper=20, default_value=2
    )
    cs.add_hyperparameter(min_samples_split)
    min_samples_split.doc = "The minimum number of samples required to split an internal node."

    min_samples_leaf = UniformIntegerHyperparameter(
        "min_samples_leaf", lower=1, upper=10, default_value=1
    )
    cs.add_hyperparameter(min_samples_leaf)
    min_samples_leaf.doc = "The minimum number of samples required to be at a leaf node."

    # --- Gradient Boosting Hyperparameters ---
    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-4, upper=1e-1, default_value=1e-2, log=True
    )
    cs.add_hyperparameter(learning_rate)
    learning_rate.doc = "Learning rate shrinks the contribution of each tree by learning_rate."

    max_depth = UniformIntegerHyperparameter(
        "max_depth", lower=2, upper=10, default_value=3
    )
    cs.add_hyperparameter(max_depth)
    max_depth.doc = "Maximum depth of the individual regression estimators."

    # --- SVM Hyperparameters ---
    C = UniformFloatHyperparameter(
        "C", lower=0.001, upper=10, default_value=1.0, log=True
    )
    cs.add_hyperparameter(C)
    C.doc = "Regularization parameter. The strength of the regularization is inversely proportional to C."

    kernel = CategoricalHyperparameter(
        "kernel", choices=["linear", "rbf", "poly", "sigmoid"], default_value="rbf"
    )
    cs.add_hyperparameter(kernel)
    kernel.doc = "Specifies the kernel type to be used in the algorithm."

    degree = UniformIntegerHyperparameter(
        "degree", lower=2, upper=5, default_value=3
    )
    cs.add_hyperparameter(degree)
    degree.doc = "Degree of the polynomial kernel function ('poly'). Ignored by all other kernels."
    degree_condition = InCondition(child=degree, parent=kernel, values=["poly"])
    cs.add_condition(degree_condition)


    gamma = CategoricalHyperparameter(
        "gamma", choices=["scale", "auto"], default_value="scale"
    )
    cs.add_hyperparameter(gamma)
    gamma.doc = "Kernel coefficient for 'rbf', 'poly' and 'sigmoid'."
    gamma_condition = InCondition(child=gamma, parent=kernel, values=["rbf", "poly", "sigmoid"])
    cs.add_condition(gamma_condition)


    # --- Logistic Regression Hyperparameters ---
    penalty = CategoricalHyperparameter(
        "penalty", choices=["l1", "l2", "elasticnet", "none"], default_value="l2"
    )
    cs.add_hyperparameter(penalty)
    penalty.doc = "Used to specify the norm used in the penalization."

    solver = CategoricalHyperparameter(
        "solver", choices=["newton-cg", "lbfgs", "liblinear", "sag", "saga"], default_value="lbfgs"
    )
    cs.add_hyperparameter(solver)
    solver.doc = "Algorithm to use in the optimization problem."

    l1_ratio = UniformFloatHyperparameter(
        "l1_ratio", lower=0.0, upper=1.0, default_value=0.5
    )
    cs.add_hyperparameter(l1_ratio)
    l1_ratio.doc = "The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``."
    l1_ratio_condition = InCondition(child=l1_ratio, parent=penalty, values=["elasticnet"])
    cs.add_condition(l1_ratio_condition)

    # --- Data Preprocessing (Example - Not exhaustive) ---
    imputation_strategy_numerical = CategoricalHyperparameter(
        "imputation_strategy_numerical", choices=["mean", "median"], default_value="mean"
    )
    cs.add_hyperparameter(imputation_strategy_numerical)
    imputation_strategy_numerical.doc = "Strategy to use for imputing missing numerical values."

    imputation_strategy_categorical = CategoricalHyperparameter(
        "imputation_strategy_categorical", choices=["most_frequent", "constant"], default_value="most_frequent"
    )
    cs.add_hyperparameter(imputation_strategy_categorical)
    imputation_strategy_categorical.doc = "Strategy to use for imputing missing categorical values."


    return cs
```
--------------------------------------------------------------------------------
[2025-06-20 10:48:18] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, InCondition

def get_configspace() -> ConfigurationSpace:
    """
    Creates a configuration space for machine learning hyperparameter optimization,
    specifically tailored for the 'sick' dataset, which is a tabular dataset
    with mixed categorical and numerical features and class imbalance.

    Returns:
        ConfigurationSpace: A configuration space object.
    """
    cs = ConfigurationSpace()

    # --- Random Forest Hyperparameters ---
    n_estimators = UniformIntegerHyperparameter(
        "n_estimators", lower=10, upper=200, default_value=100
    )
    cs.add_hyperparameter(n_estimators)
    n_estimators.doc = "The number of trees in the forest."

    max_features = UniformFloatHyperparameter(
        "max_features", lower=0.1, upper=1.0, default_value=0.5
    )
    cs.add_hyperparameter(max_features)
    max_features.doc = "The number of features to consider when looking for the best split. Can be specified as a float (percentage of features) or an int (number of features)."

    min_samples_split = UniformIntegerHyperparameter(
        "min_samples_split", lower=2, upper=20, default_value=2
    )
    cs.add_hyperparameter(min_samples_split)
    min_samples_split.doc = "The minimum number of samples required to split an internal node."

    min_samples_leaf = UniformIntegerHyperparameter(
        "min_samples_leaf", lower=1, upper=10, default_value=1
    )
    cs.add_hyperparameter(min_samples_leaf)
    min_samples_leaf.doc = "The minimum number of samples required to be at a leaf node."

    # --- Gradient Boosting Hyperparameters ---
    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-4, upper=1e-1, default_value=1e-2, log=True
    )
    cs.add_hyperparameter(learning_rate)
    learning_rate.doc = "Learning rate shrinks the contribution of each tree by learning_rate."

    max_depth = UniformIntegerHyperparameter(
        "max_depth", lower=2, upper=10, default_value=3
    )
    cs.add_hyperparameter(max_depth)
    max_depth.doc = "Maximum depth of the individual regression estimators."

    # --- SVM Hyperparameters ---
    C = UniformFloatHyperparameter(
        "C", lower=0.001, upper=10, default_value=1.0, log=True
    )
    cs.add_hyperparameter(C)
    C.doc = "Regularization parameter. The strength of the regularization is inversely proportional to C."

    kernel = CategoricalHyperparameter(
        "kernel", choices=["linear", "rbf", "poly", "sigmoid"], default_value="rbf"
    )
    cs.add_hyperparameter(kernel)
    kernel.doc = "Specifies the kernel type to be used in the algorithm."

    degree = UniformIntegerHyperparameter(
        "degree", lower=2, upper=5, default_value=3
    )
    cs.add_hyperparameter(degree)
    degree.doc = "Degree of the polynomial kernel function ('poly'). Ignored by all other kernels."
    degree_condition = InCondition(child=degree, parent=kernel, values=["poly"])
    cs.add_condition(degree_condition)


    gamma = CategoricalHyperparameter(
        "gamma", choices=["scale", "auto"], default_value="scale"
    )
    cs.add_hyperparameter(gamma)
    gamma.doc = "Kernel coefficient for 'rbf', 'poly' and 'sigmoid'."
    gamma_condition = InCondition(child=gamma, parent=kernel, values=["rbf", "poly", "sigmoid"])
    cs.add_condition(gamma_condition)


    # --- Logistic Regression Hyperparameters ---
    penalty = CategoricalHyperparameter(
        "penalty", choices=["l1", "l2", "elasticnet", "none"], default_value="l2"
    )
    cs.add_hyperparameter(penalty)
    penalty.doc = "Used to specify the norm used in the penalization."

    solver = CategoricalHyperparameter(
        "solver", choices=["newton-cg", "lbfgs", "liblinear", "sag", "saga"], default_value="lbfgs"
    )
    cs.add_hyperparameter(solver)
    solver.doc = "Algorithm to use in the optimization problem."

    l1_ratio = UniformFloatHyperparameter(
        "l1_ratio", lower=0.0, upper=1.0, default_value=0.5
    )
    cs.add_hyperparameter(l1_ratio)
    l1_ratio.doc = "The Elastic-Net mixing parameter, with ``0 <= l1_ratio <= 1``."
    l1_ratio_condition = InCondition(child=l1_ratio, parent=penalty, values=["elasticnet"])
    cs.add_condition(l1_ratio_condition)

    # --- Data Preprocessing (Example - Not exhaustive) ---
    imputation_strategy_numerical = CategoricalHyperparameter(
        "imputation_strategy_numerical", choices=["mean", "median"], default_value="mean"
    )
    cs.add_hyperparameter(imputation_strategy_numerical)
    imputation_strategy_numerical.doc = "Strategy to use for imputing missing numerical values."

    imputation_strategy_categorical = CategoricalHyperparameter(
        "imputation_strategy_categorical", choices=["most_frequent", "constant"], default_value="most_frequent"
    )
    cs.add_hyperparameter(imputation_strategy_categorical)
    imputation_strategy_categorical.doc = "Strategy to use for imputing missing categorical values."


    return cs

--------------------------------------------------------------------------------
[2025-06-20 10:48:18] [Metadata: {'component': 'config', 'status': 'success'}] Configuration space generated successfully
--------------------------------------------------------------------------------
[2025-06-20 10:48:19] [Metadata: {'component': 'scenario'}] ```python
from smac import Scenario
from ConfigSpace import ConfigurationSpace

def generate_scenario(cs: ConfigurationSpace) -> Scenario:
    scenario = Scenario(
        configspace=cs,
        name="HyperparameterOptimization",
        output_directory="./logs/gemini-2.0-flash_sick_20250620_104753",
        deterministic=False,
        n_trials=10,
        n_workers=1,
    )
    return scenario
```
--------------------------------------------------------------------------------
[2025-06-20 10:48:19] [Metadata: {'component': 'scenario', 'action': 'run'}] from smac import Scenario
from ConfigSpace import ConfigurationSpace

def generate_scenario(cs: ConfigurationSpace) -> Scenario:
    scenario = Scenario(
        configspace=cs,
        name="HyperparameterOptimization",
        output_directory="./logs/gemini-2.0-flash_sick_20250620_104753",
        deterministic=False,
        n_trials=10,
        n_workers=1,
    )
    return scenario

--------------------------------------------------------------------------------
[2025-06-20 10:48:19] [Metadata: {'component': 'scenario', 'status': 'success'}] Scenario generated successfully
--------------------------------------------------------------------------------
[2025-06-20 10:48:25] [Metadata: {'component': 'train_function'}] ```python
import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, make_scorer
from sklearn.utils import resample
from ConfigSpace import Configuration
from typing import Any

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model on the given dataset using the provided configuration.

    Args:
        cfg (Configuration): A configuration object containing the hyperparameters.
        dataset (Any): A dictionary containing the training data ('X' for features, 'y' for labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: The negative accuracy score on the validation set.
    """
    np.random.seed(seed)

    X = dataset['X']
    y = dataset['y']

    # Identify categorical and numerical features
    categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()
    numerical_features = X.select_dtypes(include=['number']).columns.tolist()

    # Create preprocessor
    numerical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy=cfg.get('imputation_strategy_numerical'))),
        ('scaler', StandardScaler())
    ])

    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy=cfg.get('imputation_strategy_categorical'), fill_value='missing')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numerical_transformer, numerical_features),
            ('cat', categorical_transformer, categorical_features)
        ])

    # Define the model based on hyperparameters (example: RandomForest)
    model_type = "random_forest" # Simplified - all configurations now use Random Forest
    if model_type == "random_forest":
        model = RandomForestClassifier(
            n_estimators=cfg.get('n_estimators'),
            max_features=cfg.get('max_features'),
            min_samples_split=cfg.get('min_samples_split'),
            min_samples_leaf=cfg.get('min_samples_leaf'),
            random_state=seed,
            class_weight='balanced'  # Handle class imbalance
        )
    elif model_type == "gradient_boosting":  # Example: Gradient Boosting
        model = GradientBoostingClassifier(
            learning_rate=cfg.get('learning_rate'),
            max_depth=cfg.get('max_depth'),
            random_state=seed
        )
    elif model_type == "svm":
        model = SVC(C=cfg.get('C'), kernel=cfg.get('kernel'), gamma=cfg.get('gamma') if 'gamma' in cfg else 'scale', degree=cfg.get('degree') if 'degree' in cfg else 3, random_state=seed, probability=True)
    elif model_type == "logistic_regression":
        model = LogisticRegression(penalty=cfg.get('penalty'), solver=cfg.get('solver'), l1_ratio=cfg.get('l1_ratio') if 'l1_ratio' in cfg else None, random_state=seed, max_iter=1000)
    else:
        raise ValueError(f"Unknown model type: {model_type}")

    # Create pipeline
    pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                               ('classifier', model)])

    # Stratified K-Fold Cross Validation
    n_splits = 3
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)
    accuracy_scores = []

    for train_index, val_index in skf.split(X, y):
        X_train, X_val = X.iloc[train_index], X.iloc[val_index]
        y_train, y_val = y.iloc[train_index], y.iloc[val_index]

        # Train the model
        pipeline.fit(X_train, y_train)

        # Make predictions on the validation set
        y_pred = pipeline.predict(X_val)

        # Calculate accuracy and append to the list
        accuracy = accuracy_score(y_val, y_pred)
        accuracy_scores.append(accuracy)

    # Calculate the average accuracy across all folds
    avg_accuracy = np.mean(accuracy_scores)

    return -avg_accuracy  # Return negative accuracy as SMAC minimizes
```
--------------------------------------------------------------------------------
[2025-06-20 10:48:25] [Metadata: {'component': 'train_function', 'action': 'run'}] import pandas as pd
import numpy as np
from sklearn.model_selection import StratifiedKFold
from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, make_scorer
from sklearn.utils import resample
from ConfigSpace import Configuration
from typing import Any

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model on the given dataset using the provided configuration.

    Args:
        cfg (Configuration): A configuration object containing the hyperparameters.
        dataset (Any): A dictionary containing the training data ('X' for features, 'y' for labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: The negative accuracy score on the validation set.
    """
    np.random.seed(seed)

    X = dataset['X']
    y = dataset['y']

    # Identify categorical and numerical features
    categorical_features = X.select_dtypes(include=['object', 'category']).columns.tolist()
    numerical_features = X.select_dtypes(include=['number']).columns.tolist()

    # Create preprocessor
    numerical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy=cfg.get('imputation_strategy_numerical'))),
        ('scaler', StandardScaler())
    ])

    categorical_transformer = Pipeline(steps=[
        ('imputer', SimpleImputer(strategy=cfg.get('imputation_strategy_categorical'), fill_value='missing')),
        ('onehot', OneHotEncoder(handle_unknown='ignore'))
    ])

    preprocessor = ColumnTransformer(
        transformers=[
            ('num', numerical_transformer, numerical_features),
            ('cat', categorical_transformer, categorical_features)
        ])

    # Define the model based on hyperparameters (example: RandomForest)
    model_type = "random_forest" # Simplified - all configurations now use Random Forest
    if model_type == "random_forest":
        model = RandomForestClassifier(
            n_estimators=cfg.get('n_estimators'),
            max_features=cfg.get('max_features'),
            min_samples_split=cfg.get('min_samples_split'),
            min_samples_leaf=cfg.get('min_samples_leaf'),
            random_state=seed,
            class_weight='balanced'  # Handle class imbalance
        )
    elif model_type == "gradient_boosting":  # Example: Gradient Boosting
        model = GradientBoostingClassifier(
            learning_rate=cfg.get('learning_rate'),
            max_depth=cfg.get('max_depth'),
            random_state=seed
        )
    elif model_type == "svm":
        model = SVC(C=cfg.get('C'), kernel=cfg.get('kernel'), gamma=cfg.get('gamma') if 'gamma' in cfg else 'scale', degree=cfg.get('degree') if 'degree' in cfg else 3, random_state=seed, probability=True)
    elif model_type == "logistic_regression":
        model = LogisticRegression(penalty=cfg.get('penalty'), solver=cfg.get('solver'), l1_ratio=cfg.get('l1_ratio') if 'l1_ratio' in cfg else None, random_state=seed, max_iter=1000)
    else:
        raise ValueError(f"Unknown model type: {model_type}")

    # Create pipeline
    pipeline = Pipeline(steps=[('preprocessor', preprocessor),
                               ('classifier', model)])

    # Stratified K-Fold Cross Validation
    n_splits = 3
    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=seed)
    accuracy_scores = []

    for train_index, val_index in skf.split(X, y):
        X_train, X_val = X.iloc[train_index], X.iloc[val_index]
        y_train, y_val = y.iloc[train_index], y.iloc[val_index]

        # Train the model
        pipeline.fit(X_train, y_train)

        # Make predictions on the validation set
        y_pred = pipeline.predict(X_val)

        # Calculate accuracy and append to the list
        accuracy = accuracy_score(y_val, y_pred)
        accuracy_scores.append(accuracy)

    # Calculate the average accuracy across all folds
    avg_accuracy = np.mean(accuracy_scores)

    return -avg_accuracy  # Return negative accuracy as SMAC minimizes

--------------------------------------------------------------------------------
[2025-06-20 10:48:25] [Metadata: {'component': 'train_function', 'status': 'success', 'loss': -0.9771299236092567}] Training executed successfully, loss: -0.9771299236092567
--------------------------------------------------------------------------------
