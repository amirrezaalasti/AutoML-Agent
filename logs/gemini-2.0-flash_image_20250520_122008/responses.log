[2025-05-20 12:20:11] [Metadata: {'component': 'config'}] ```python
from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause
def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    optimizer = Categorical("optimizer", ["Adam", "SGD", "RMSprop"], default="Adam")
    learning_rate = Float("learning_rate", (1e-5, 1e-2), default=1e-3, log=True)
    batch_size = Categorical("batch_size", [32, 64, 128], default=32)
    momentum = Float("momentum", (0.0, 0.99), default=0.9, log=False)
    num_layers = Integer("num_layers", (1, 5), default=2)

    # Add hyperparameters to the configuration space
    cs.add([optimizer, learning_rate, batch_size, momentum, num_layers])

    # Add forbidden clauses
    forbidden_momentum_adam = ForbiddenAndConjunction(
        ForbiddenEqualsClause(optimizer, "Adam"),
        ForbiddenEqualsClause(momentum, 0.9)
    )
    forbidden_momentum_rmsprop = ForbiddenAndConjunction(
        ForbiddenEqualsClause(optimizer, "RMSprop"),
        ForbiddenEqualsClause(momentum, 0.9)
    )

    cs.add_forbidden_clause(forbidden_momentum_adam)
    cs.add_forbidden_clause(forbidden_momentum_rmsprop)

    return cs
```
--------------------------------------------------------------------------------
[2025-05-20 12:20:11] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause
def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    optimizer = Categorical("optimizer", ["Adam", "SGD", "RMSprop"], default="Adam")
    learning_rate = Float("learning_rate", (1e-5, 1e-2), default=1e-3, log=True)
    batch_size = Categorical("batch_size", [32, 64, 128], default=32)
    momentum = Float("momentum", (0.0, 0.99), default=0.9, log=False)
    num_layers = Integer("num_layers", (1, 5), default=2)

    # Add hyperparameters to the configuration space
    cs.add([optimizer, learning_rate, batch_size, momentum, num_layers])

    # Add forbidden clauses
    forbidden_momentum_adam = ForbiddenAndConjunction(
        ForbiddenEqualsClause(optimizer, "Adam"),
        ForbiddenEqualsClause(momentum, 0.9)
    )
    forbidden_momentum_rmsprop = ForbiddenAndConjunction(
        ForbiddenEqualsClause(optimizer, "RMSprop"),
        ForbiddenEqualsClause(momentum, 0.9)
    )

    cs.add_forbidden_clause(forbidden_momentum_adam)
    cs.add_forbidden_clause(forbidden_momentum_rmsprop)

    return cs

--------------------------------------------------------------------------------
[2025-05-20 12:20:11] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#1): Given vector violates forbidden clause: (Forbidden: momentum == 0.9 && Forbidden: optimizer == 'Adam')
--------------------------------------------------------------------------------
[2025-05-20 12:20:13] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause
def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    optimizer = Categorical("optimizer", ["Adam", "SGD", "RMSprop"], default="Adam")
    learning_rate = Float("learning_rate", (1e-5, 1e-2), default=1e-3, log=True)
    batch_size = Categorical("batch_size", [32, 64, 128], default=32)
    momentum = Float("momentum", (0.0, 0.9), default=0.9, log=False)
    num_layers = Integer("num_layers", (1, 5), default=2)

    # Add hyperparameters to the configuration space
    cs.add([optimizer, learning_rate, batch_size, momentum, num_layers])

    # Add forbidden clauses
    forbidden_momentum_adam = ForbiddenAndConjunction(
        ForbiddenEqualsClause(optimizer, "Adam"),
        ForbiddenEqualsClause(momentum, 0.9)
    )

    cs.add_forbidden_clause(forbidden_momentum_adam)

    return cs
```
--------------------------------------------------------------------------------
[2025-05-20 12:20:13] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause
def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    optimizer = Categorical("optimizer", ["Adam", "SGD", "RMSprop"], default="Adam")
    learning_rate = Float("learning_rate", (1e-5, 1e-2), default=1e-3, log=True)
    batch_size = Categorical("batch_size", [32, 64, 128], default=32)
    momentum = Float("momentum", (0.0, 0.9), default=0.9, log=False)
    num_layers = Integer("num_layers", (1, 5), default=2)

    # Add hyperparameters to the configuration space
    cs.add([optimizer, learning_rate, batch_size, momentum, num_layers])

    # Add forbidden clauses
    forbidden_momentum_adam = ForbiddenAndConjunction(
        ForbiddenEqualsClause(optimizer, "Adam"),
        ForbiddenEqualsClause(momentum, 0.9)
    )

    cs.add_forbidden_clause(forbidden_momentum_adam)

    return cs

--------------------------------------------------------------------------------
[2025-05-20 12:20:13] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#2): Given vector violates forbidden clause: (Forbidden: momentum == 0.9 && Forbidden: optimizer == 'Adam')
--------------------------------------------------------------------------------
[2025-05-20 12:20:14] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause
def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    optimizer = Categorical("optimizer", ["Adam", "SGD", "RMSprop"], default="Adam")
    learning_rate = Float("learning_rate", (1e-5, 1e-2), default=1e-3, log=True)
    batch_size = Categorical("batch_size", [32, 64, 128], default=32)
    momentum = Float("momentum", (0.0, 0.9), default=0.0, log=False)
    num_layers = Integer("num_layers", (1, 5), default=2)

    # Add hyperparameters to the configuration space
    cs.add([optimizer, learning_rate, batch_size, momentum, num_layers])

    # Add forbidden clauses
    forbidden_momentum_adam = ForbiddenAndConjunction(
        ForbiddenEqualsClause(optimizer, "Adam"),
        ForbiddenEqualsClause(momentum, 0.9)
    )

    cs.add_forbidden_clause(forbidden_momentum_adam)

    return cs
```
--------------------------------------------------------------------------------
[2025-05-20 12:20:14] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause
def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    optimizer = Categorical("optimizer", ["Adam", "SGD", "RMSprop"], default="Adam")
    learning_rate = Float("learning_rate", (1e-5, 1e-2), default=1e-3, log=True)
    batch_size = Categorical("batch_size", [32, 64, 128], default=32)
    momentum = Float("momentum", (0.0, 0.9), default=0.0, log=False)
    num_layers = Integer("num_layers", (1, 5), default=2)

    # Add hyperparameters to the configuration space
    cs.add([optimizer, learning_rate, batch_size, momentum, num_layers])

    # Add forbidden clauses
    forbidden_momentum_adam = ForbiddenAndConjunction(
        ForbiddenEqualsClause(optimizer, "Adam"),
        ForbiddenEqualsClause(momentum, 0.9)
    )

    cs.add_forbidden_clause(forbidden_momentum_adam)

    return cs

--------------------------------------------------------------------------------
[2025-05-20 12:20:14] [Metadata: {'component': 'config', 'status': 'success'}] Configuration space generated successfully
--------------------------------------------------------------------------------
[2025-05-20 12:20:15] [Metadata: {'component': 'scenario'}] ```python
from smac import Scenario
from ConfigSpace import ConfigurationSpace

def generate_scenario(cs):
    scenario = Scenario(
        configspace=cs,
        output_directory="./automl_results",
        deterministic=False,
        min_budget=1,
        max_budget=10,
        n_workers=2
    )
    return scenario
```
--------------------------------------------------------------------------------
[2025-05-20 12:20:15] [Metadata: {'component': 'scenario', 'action': 'run'}] from smac import Scenario
from ConfigSpace import ConfigurationSpace

def generate_scenario(cs):
    scenario = Scenario(
        configspace=cs,
        output_directory="./automl_results",
        deterministic=False,
        min_budget=1,
        max_budget=10,
        n_workers=2
    )
    return scenario

--------------------------------------------------------------------------------
[2025-05-20 12:20:15] [Metadata: {'component': 'scenario', 'status': 'success'}] Scenario generated successfully
--------------------------------------------------------------------------------
[2025-05-20 12:20:23] [Metadata: {'component': 'train_function'}] ```python
from typing import Any
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from ConfigSpace import Configuration
import math

def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a PyTorch model on the given dataset using the provided configuration.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing 'X' (features) and 'y' (labels).

    Returns:
        float: The average training loss over 10 epochs.
    """

    X = dataset['X']
    y = dataset['y']

    # Convert to numpy arrays if not already
    if not isinstance(X, np.ndarray):
        X = np.array(X)
    if not isinstance(y, np.ndarray):
        y = np.array(y)

    # Data Processing and Reshaping
    n_samples = X.shape[0]
    n_features = X.shape[1]

    height = int(math.sqrt(n_features))
    if height * height == n_features:
        X = X.reshape(n_samples, 1, height, height)
    elif len(X.shape) == 3:  # (N, H, W)
        X = X.reshape(n_samples, 1, X.shape[1], X.shape[2]) # (N, C, H, W)
    # If already 4D, assume correct (N, C, H, W) for PyTorch (or adjust if TF).

    # Normalization
    X = X / 255.0

    # Convert to PyTorch tensors
    X = torch.tensor(X, dtype=torch.float32)
    y = torch.tensor(y, dtype=torch.long)

    # Create DataLoader
    batch_size = int(cfg.get("batch_size"))
    train_dataset = TensorDataset(X, y)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    # Infer input and output dimensions
    input_channels = X.shape[1]
    input_height = X.shape[2]
    input_width = X.shape[3]
    num_classes = len(torch.unique(y))

    # Define the Model
    class SimpleCNN(nn.Module):
        def __init__(self, input_channels, num_classes, num_layers):
            super(SimpleCNN, self).__init__()
            self.layers = nn.ModuleList()

            in_channels = input_channels
            for i in range(num_layers):
                out_channels = 16 * (i + 1)
                self.layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))
                self.layers.append(nn.ReLU())
                self.layers.append(nn.MaxPool2d(2, 2))
                in_channels = out_channels

            self.flatten = nn.Flatten()
            # Dynamically determine the size of the flattened layer by passing a sample input
            with torch.no_grad():
                sample_input = torch.randn(1, input_channels, input_height, input_width)
                for layer in self.layers:
                    sample_input = layer(sample_input)
                flattened_size = sample_input.view(1, -1).size(1)
            
            self.fc = nn.Linear(flattened_size, num_classes)

        def forward(self, x):
            for layer in self.layers:
                x = layer(x)
            x = self.flatten(x)
            x = self.fc(x)
            return x

    num_layers = int(cfg.get("num_layers"))
    model = SimpleCNN(input_channels, num_classes, num_layers)

    # Define Optimizer
    optimizer_name = cfg.get("optimizer")
    learning_rate = cfg.get("learning_rate")
    if optimizer_name == "Adam":
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == "SGD":
        momentum = cfg.get("momentum")
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    elif optimizer_name == "RMSprop":
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Unknown optimizer: {optimizer_name}")

    # Define Loss Function
    criterion = nn.CrossEntropyLoss()

    # Training Loop
    num_epochs = 10
    total_loss = 0.0
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for i, (inputs, labels) in enumerate(train_loader):
            # Zero the parameter gradients
            optimizer.zero_grad()

            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            # Backward and optimize
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
        total_loss += epoch_loss / len(train_loader)

    avg_loss = total_loss / num_epochs

    return float(avg_loss)
```
--------------------------------------------------------------------------------
[2025-05-20 12:20:23] [Metadata: {'component': 'train_function', 'action': 'run'}] from typing import Any
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from ConfigSpace import Configuration
import math

def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a PyTorch model on the given dataset using the provided configuration.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing 'X' (features) and 'y' (labels).

    Returns:
        float: The average training loss over 10 epochs.
    """

    X = dataset['X']
    y = dataset['y']

    # Convert to numpy arrays if not already
    if not isinstance(X, np.ndarray):
        X = np.array(X)
    if not isinstance(y, np.ndarray):
        y = np.array(y)

    # Data Processing and Reshaping
    n_samples = X.shape[0]
    n_features = X.shape[1]

    height = int(math.sqrt(n_features))
    if height * height == n_features:
        X = X.reshape(n_samples, 1, height, height)
    elif len(X.shape) == 3:  # (N, H, W)
        X = X.reshape(n_samples, 1, X.shape[1], X.shape[2]) # (N, C, H, W)
    # If already 4D, assume correct (N, C, H, W) for PyTorch (or adjust if TF).

    # Normalization
    X = X / 255.0

    # Convert to PyTorch tensors
    X = torch.tensor(X, dtype=torch.float32)
    y = torch.tensor(y, dtype=torch.long)

    # Create DataLoader
    batch_size = int(cfg.get("batch_size"))
    train_dataset = TensorDataset(X, y)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    # Infer input and output dimensions
    input_channels = X.shape[1]
    input_height = X.shape[2]
    input_width = X.shape[3]
    num_classes = len(torch.unique(y))

    # Define the Model
    class SimpleCNN(nn.Module):
        def __init__(self, input_channels, num_classes, num_layers):
            super(SimpleCNN, self).__init__()
            self.layers = nn.ModuleList()

            in_channels = input_channels
            for i in range(num_layers):
                out_channels = 16 * (i + 1)
                self.layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1))
                self.layers.append(nn.ReLU())
                self.layers.append(nn.MaxPool2d(2, 2))
                in_channels = out_channels

            self.flatten = nn.Flatten()
            # Dynamically determine the size of the flattened layer by passing a sample input
            with torch.no_grad():
                sample_input = torch.randn(1, input_channels, input_height, input_width)
                for layer in self.layers:
                    sample_input = layer(sample_input)
                flattened_size = sample_input.view(1, -1).size(1)
            
            self.fc = nn.Linear(flattened_size, num_classes)

        def forward(self, x):
            for layer in self.layers:
                x = layer(x)
            x = self.flatten(x)
            x = self.fc(x)
            return x

    num_layers = int(cfg.get("num_layers"))
    model = SimpleCNN(input_channels, num_classes, num_layers)

    # Define Optimizer
    optimizer_name = cfg.get("optimizer")
    learning_rate = cfg.get("learning_rate")
    if optimizer_name == "Adam":
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == "SGD":
        momentum = cfg.get("momentum")
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    elif optimizer_name == "RMSprop":
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Unknown optimizer: {optimizer_name}")

    # Define Loss Function
    criterion = nn.CrossEntropyLoss()

    # Training Loop
    num_epochs = 10
    total_loss = 0.0
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for i, (inputs, labels) in enumerate(train_loader):
            # Zero the parameter gradients
            optimizer.zero_grad()

            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            # Backward and optimize
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
        total_loss += epoch_loss / len(train_loader)

    avg_loss = total_loss / num_epochs

    return float(avg_loss)

--------------------------------------------------------------------------------
