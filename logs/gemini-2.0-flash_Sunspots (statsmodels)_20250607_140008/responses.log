[2025-06-07 14:00:12] [Metadata: {'error_type': 'DATASET_NOT_FOUND'}] [DATASET_NOT_FOUND] No related datasets found for Sunspots (statsmodels)
--------------------------------------------------------------------------------
[2025-06-07 14:00:20] [Metadata: {'component': 'config'}] ```python
from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause
from ConfigSpace import EqualsCondition, InCondition

def get_configspace():
    cs = ConfigurationSpace()

    model_type = Categorical("model_type", ["LSTM", "GRU", "MLP"])
    cs.add_hyperparameter(model_type)

    # LSTM Hyperparameters
    lstm_units = Integer("lstm_units", (32, 256), default=64)
    lstm_layers = Integer("lstm_layers", (1, 3), default=1)
    lstm_dropout = Float("lstm_dropout", (0.0, 0.5), default=0.0)
    cs.add_hyperparameters([lstm_units, lstm_layers, lstm_dropout])

    # GRU Hyperparameters
    gru_units = Integer("gru_units", (32, 256), default=64)
    gru_layers = Integer("gru_layers", (1, 3), default=1)
    gru_dropout = Float("gru_dropout", (0.0, 0.5), default=0.0)
    cs.add_hyperparameters([gru_units, gru_layers, gru_dropout])

    # MLP Hyperparameters
    mlp_layers = Integer("mlp_layers", (1, 3), default=2)
    mlp_units = Integer("mlp_units", (32, 256), default=128)
    mlp_dropout = Float("mlp_dropout", (0.0, 0.5), default=0.0)
    cs.add_hyperparameters([mlp_layers, mlp_units, mlp_dropout])

    # Learning Rate
    learning_rate = Float("learning_rate", (1e-5, 1e-2), default=1e-3, log=True)
    cs.add_hyperparameter(learning_rate)

    # Conditions for LSTM
    cond_lstm_units = EqualsCondition(lstm_units, model_type, "LSTM")
    cond_lstm_layers = EqualsCondition(lstm_layers, model_type, "LSTM")
    cond_lstm_dropout = EqualsCondition(lstm_dropout, model_type, "LSTM")
    cs.add_conditions([cond_lstm_units, cond_lstm_layers, cond_lstm_dropout])

    # Conditions for GRU
    cond_gru_units = EqualsCondition(gru_units, model_type, "GRU")
    cond_gru_layers = EqualsCondition(gru_layers, model_type, "GRU")
    cond_gru_dropout = EqualsCondition(gru_dropout, model_type, "GRU")
    cs.add_conditions([cond_gru_units, cond_gru_layers, cond_gru_dropout])

    # Conditions for MLP
    cond_mlp_layers = EqualsCondition(mlp_layers, model_type, "MLP")
    cond_mlp_units = EqualsCondition(mlp_units, model_type, "MLP")
    cond_mlp_dropout = EqualsCondition(mlp_dropout, model_type, "MLP")
    cs.add_conditions([cond_mlp_layers, cond_mlp_units, cond_mlp_dropout])

    # Forbidden Clauses - Example: LSTM with no layers
    forbidden_lstm_no_layers = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "LSTM"),
        ForbiddenEqualsClause(lstm_layers, 1)
    )

    # Forbidden Clauses - Example: GRU with no layers
    forbidden_gru_no_layers = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "GRU"),
        ForbiddenEqualsClause(gru_layers, 1)
    )
    
    cs.add_forbidden_clause(forbidden_lstm_no_layers)
    cs.add_forbidden_clause(forbidden_gru_no_layers)

    return cs
```
--------------------------------------------------------------------------------
[2025-06-07 14:00:20] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause
from ConfigSpace import EqualsCondition, InCondition

def get_configspace():
    cs = ConfigurationSpace()

    model_type = Categorical("model_type", ["LSTM", "GRU", "MLP"])
    cs.add_hyperparameter(model_type)

    # LSTM Hyperparameters
    lstm_units = Integer("lstm_units", (32, 256), default=64)
    lstm_layers = Integer("lstm_layers", (1, 3), default=1)
    lstm_dropout = Float("lstm_dropout", (0.0, 0.5), default=0.0)
    cs.add_hyperparameters([lstm_units, lstm_layers, lstm_dropout])

    # GRU Hyperparameters
    gru_units = Integer("gru_units", (32, 256), default=64)
    gru_layers = Integer("gru_layers", (1, 3), default=1)
    gru_dropout = Float("gru_dropout", (0.0, 0.5), default=0.0)
    cs.add_hyperparameters([gru_units, gru_layers, gru_dropout])

    # MLP Hyperparameters
    mlp_layers = Integer("mlp_layers", (1, 3), default=2)
    mlp_units = Integer("mlp_units", (32, 256), default=128)
    mlp_dropout = Float("mlp_dropout", (0.0, 0.5), default=0.0)
    cs.add_hyperparameters([mlp_layers, mlp_units, mlp_dropout])

    # Learning Rate
    learning_rate = Float("learning_rate", (1e-5, 1e-2), default=1e-3, log=True)
    cs.add_hyperparameter(learning_rate)

    # Conditions for LSTM
    cond_lstm_units = EqualsCondition(lstm_units, model_type, "LSTM")
    cond_lstm_layers = EqualsCondition(lstm_layers, model_type, "LSTM")
    cond_lstm_dropout = EqualsCondition(lstm_dropout, model_type, "LSTM")
    cs.add_conditions([cond_lstm_units, cond_lstm_layers, cond_lstm_dropout])

    # Conditions for GRU
    cond_gru_units = EqualsCondition(gru_units, model_type, "GRU")
    cond_gru_layers = EqualsCondition(gru_layers, model_type, "GRU")
    cond_gru_dropout = EqualsCondition(gru_dropout, model_type, "GRU")
    cs.add_conditions([cond_gru_units, cond_gru_layers, cond_gru_dropout])

    # Conditions for MLP
    cond_mlp_layers = EqualsCondition(mlp_layers, model_type, "MLP")
    cond_mlp_units = EqualsCondition(mlp_units, model_type, "MLP")
    cond_mlp_dropout = EqualsCondition(mlp_dropout, model_type, "MLP")
    cs.add_conditions([cond_mlp_layers, cond_mlp_units, cond_mlp_dropout])

    # Forbidden Clauses - Example: LSTM with no layers
    forbidden_lstm_no_layers = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "LSTM"),
        ForbiddenEqualsClause(lstm_layers, 1)
    )

    # Forbidden Clauses - Example: GRU with no layers
    forbidden_gru_no_layers = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "GRU"),
        ForbiddenEqualsClause(gru_layers, 1)
    )
    
    cs.add_forbidden_clause(forbidden_lstm_no_layers)
    cs.add_forbidden_clause(forbidden_gru_no_layers)

    return cs

--------------------------------------------------------------------------------
[2025-06-07 14:00:20] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#1): Given vector violates forbidden clause: (Forbidden: model_type == 'LSTM' && Forbidden: lstm_layers == 1)
--------------------------------------------------------------------------------
[2025-06-07 14:00:25] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause
from ConfigSpace import EqualsCondition, InCondition

def get_configspace():
    cs = ConfigurationSpace()

    model_type = Categorical("model_type", ["LSTM", "GRU", "MLP"])
    cs.add_hyperparameter(model_type)

    # LSTM Hyperparameters
    lstm_units = Integer("lstm_units", (32, 256), default=64)
    lstm_layers = Integer("lstm_layers", (1, 3), default=1)
    lstm_dropout = Float("lstm_dropout", (0.0, 0.5), default=0.0)
    cs.add_hyperparameters([lstm_units, lstm_layers, lstm_dropout])

    # GRU Hyperparameters
    gru_units = Integer("gru_units", (32, 256), default=64)
    gru_layers = Integer("gru_layers", (1, 3), default=1)
    gru_dropout = Float("gru_dropout", (0.0, 0.5), default=0.0)
    cs.add_hyperparameters([gru_units, gru_layers, gru_dropout])

    # MLP Hyperparameters
    mlp_layers = Integer("mlp_layers", (1, 3), default=2)
    mlp_units = Integer("mlp_units", (32, 256), default=128)
    mlp_dropout = Float("mlp_dropout", (0.0, 0.5), default=0.0)
    cs.add_hyperparameters([mlp_layers, mlp_units, mlp_dropout])

    # Learning Rate
    learning_rate = Float("learning_rate", (1e-5, 1e-2), default=1e-3, log=True)
    cs.add_hyperparameter(learning_rate)

    # Conditions for LSTM
    cond_lstm_units = EqualsCondition(lstm_units, model_type, "LSTM")
    cond_lstm_layers = EqualsCondition(lstm_layers, model_type, "LSTM")
    cond_lstm_dropout = EqualsCondition(lstm_dropout, model_type, "LSTM")
    cs.add_conditions([cond_lstm_units, cond_lstm_layers, cond_lstm_dropout])

    # Conditions for GRU
    cond_gru_units = EqualsCondition(gru_units, model_type, "GRU")
    cond_gru_layers = EqualsCondition(gru_layers, model_type, "GRU")
    cond_gru_dropout = EqualsCondition(gru_dropout, model_type, "GRU")
    cs.add_conditions([cond_gru_units, cond_gru_layers, cond_gru_dropout])

    # Conditions for MLP
    cond_mlp_layers = EqualsCondition(mlp_layers, model_type, "MLP")
    cond_mlp_units = EqualsCondition(mlp_units, model_type, "MLP")
    cond_mlp_dropout = EqualsCondition(mlp_dropout, model_type, "MLP")
    cs.add_conditions([cond_mlp_layers, cond_mlp_units, cond_mlp_dropout])

    # Forbidden Clauses - Example: LSTM with no layers
    forbidden_lstm_no_layers = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "LSTM"),
        ForbiddenEqualsClause(lstm_layers, 1)
    )

    cs.add_forbidden_clause(forbidden_lstm_no_layers)

    return cs
```
--------------------------------------------------------------------------------
[2025-06-07 14:00:25] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause
from ConfigSpace import EqualsCondition, InCondition

def get_configspace():
    cs = ConfigurationSpace()

    model_type = Categorical("model_type", ["LSTM", "GRU", "MLP"])
    cs.add_hyperparameter(model_type)

    # LSTM Hyperparameters
    lstm_units = Integer("lstm_units", (32, 256), default=64)
    lstm_layers = Integer("lstm_layers", (1, 3), default=1)
    lstm_dropout = Float("lstm_dropout", (0.0, 0.5), default=0.0)
    cs.add_hyperparameters([lstm_units, lstm_layers, lstm_dropout])

    # GRU Hyperparameters
    gru_units = Integer("gru_units", (32, 256), default=64)
    gru_layers = Integer("gru_layers", (1, 3), default=1)
    gru_dropout = Float("gru_dropout", (0.0, 0.5), default=0.0)
    cs.add_hyperparameters([gru_units, gru_layers, gru_dropout])

    # MLP Hyperparameters
    mlp_layers = Integer("mlp_layers", (1, 3), default=2)
    mlp_units = Integer("mlp_units", (32, 256), default=128)
    mlp_dropout = Float("mlp_dropout", (0.0, 0.5), default=0.0)
    cs.add_hyperparameters([mlp_layers, mlp_units, mlp_dropout])

    # Learning Rate
    learning_rate = Float("learning_rate", (1e-5, 1e-2), default=1e-3, log=True)
    cs.add_hyperparameter(learning_rate)

    # Conditions for LSTM
    cond_lstm_units = EqualsCondition(lstm_units, model_type, "LSTM")
    cond_lstm_layers = EqualsCondition(lstm_layers, model_type, "LSTM")
    cond_lstm_dropout = EqualsCondition(lstm_dropout, model_type, "LSTM")
    cs.add_conditions([cond_lstm_units, cond_lstm_layers, cond_lstm_dropout])

    # Conditions for GRU
    cond_gru_units = EqualsCondition(gru_units, model_type, "GRU")
    cond_gru_layers = EqualsCondition(gru_layers, model_type, "GRU")
    cond_gru_dropout = EqualsCondition(gru_dropout, model_type, "GRU")
    cs.add_conditions([cond_gru_units, cond_gru_layers, cond_gru_dropout])

    # Conditions for MLP
    cond_mlp_layers = EqualsCondition(mlp_layers, model_type, "MLP")
    cond_mlp_units = EqualsCondition(mlp_units, model_type, "MLP")
    cond_mlp_dropout = EqualsCondition(mlp_dropout, model_type, "MLP")
    cs.add_conditions([cond_mlp_layers, cond_mlp_units, cond_mlp_dropout])

    # Forbidden Clauses - Example: LSTM with no layers
    forbidden_lstm_no_layers = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "LSTM"),
        ForbiddenEqualsClause(lstm_layers, 1)
    )

    cs.add_forbidden_clause(forbidden_lstm_no_layers)

    return cs

--------------------------------------------------------------------------------
[2025-06-07 14:00:25] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#2): Given vector violates forbidden clause: (Forbidden: model_type == 'LSTM' && Forbidden: lstm_layers == 1)
--------------------------------------------------------------------------------
[2025-06-07 14:00:29] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause
from ConfigSpace import EqualsCondition, InCondition

def get_configspace():
    cs = ConfigurationSpace()

    model_type = Categorical("model_type", ["LSTM", "GRU", "MLP"])
    cs.add_hyperparameter(model_type)

    # LSTM Hyperparameters
    lstm_units = Integer("lstm_units", (32, 256), default=64)
    lstm_layers = Integer("lstm_layers", (1, 3), default=1)
    lstm_dropout = Float("lstm_dropout", (0.0, 0.5), default=0.0)
    cs.add_hyperparameters([lstm_units, lstm_layers, lstm_dropout])

    # GRU Hyperparameters
    gru_units = Integer("gru_units", (32, 256), default=64)
    gru_layers = Integer("gru_layers", (1, 3), default=1)
    gru_dropout = Float("gru_dropout", (0.0, 0.5), default=0.0)
    cs.add_hyperparameters([gru_units, gru_layers, gru_dropout])

    # MLP Hyperparameters
    mlp_layers = Integer("mlp_layers", (1, 3), default=2)
    mlp_units = Integer("mlp_units", (32, 256), default=128)
    mlp_dropout = Float("mlp_dropout", (0.0, 0.5), default=0.0)
    cs.add_hyperparameters([mlp_layers, mlp_units, mlp_dropout])

    # Learning Rate
    learning_rate = Float("learning_rate", (1e-5, 1e-2), default=1e-3, log=True)
    cs.add_hyperparameter(learning_rate)

    # Conditions for LSTM
    cond_lstm_units = EqualsCondition(lstm_units, model_type, "LSTM")
    cond_lstm_layers = EqualsCondition(lstm_layers, model_type, "LSTM")
    cond_lstm_dropout = EqualsCondition(lstm_dropout, model_type, "LSTM")
    cs.add_conditions([cond_lstm_units, cond_lstm_layers, cond_lstm_dropout])

    # Conditions for GRU
    cond_gru_units = EqualsCondition(gru_units, model_type, "GRU")
    cond_gru_layers = EqualsCondition(gru_layers, model_type, "GRU")
    cond_gru_dropout = EqualsCondition(gru_dropout, model_type, "GRU")
    cs.add_conditions([cond_gru_units, cond_gru_layers, cond_gru_dropout])

    # Conditions for MLP
    cond_mlp_layers = EqualsCondition(mlp_layers, model_type, "MLP")
    cond_mlp_units = EqualsCondition(mlp_units, model_type, "MLP")
    cond_mlp_dropout = EqualsCondition(mlp_dropout, model_type, "MLP")
    cs.add_conditions([cond_mlp_layers, cond_mlp_units, cond_mlp_dropout])

    # Forbidden Clauses - Example: LSTM with no layers
    forbidden_lstm_no_layers = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "LSTM"),
        ForbiddenEqualsClause(lstm_layers, 1)
    )

    #cs.add_forbidden_clause(forbidden_lstm_no_layers) # Removing forbidden clause.

    return cs
```
--------------------------------------------------------------------------------
[2025-06-07 14:00:29] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause
from ConfigSpace import EqualsCondition, InCondition

def get_configspace():
    cs = ConfigurationSpace()

    model_type = Categorical("model_type", ["LSTM", "GRU", "MLP"])
    cs.add_hyperparameter(model_type)

    # LSTM Hyperparameters
    lstm_units = Integer("lstm_units", (32, 256), default=64)
    lstm_layers = Integer("lstm_layers", (1, 3), default=1)
    lstm_dropout = Float("lstm_dropout", (0.0, 0.5), default=0.0)
    cs.add_hyperparameters([lstm_units, lstm_layers, lstm_dropout])

    # GRU Hyperparameters
    gru_units = Integer("gru_units", (32, 256), default=64)
    gru_layers = Integer("gru_layers", (1, 3), default=1)
    gru_dropout = Float("gru_dropout", (0.0, 0.5), default=0.0)
    cs.add_hyperparameters([gru_units, gru_layers, gru_dropout])

    # MLP Hyperparameters
    mlp_layers = Integer("mlp_layers", (1, 3), default=2)
    mlp_units = Integer("mlp_units", (32, 256), default=128)
    mlp_dropout = Float("mlp_dropout", (0.0, 0.5), default=0.0)
    cs.add_hyperparameters([mlp_layers, mlp_units, mlp_dropout])

    # Learning Rate
    learning_rate = Float("learning_rate", (1e-5, 1e-2), default=1e-3, log=True)
    cs.add_hyperparameter(learning_rate)

    # Conditions for LSTM
    cond_lstm_units = EqualsCondition(lstm_units, model_type, "LSTM")
    cond_lstm_layers = EqualsCondition(lstm_layers, model_type, "LSTM")
    cond_lstm_dropout = EqualsCondition(lstm_dropout, model_type, "LSTM")
    cs.add_conditions([cond_lstm_units, cond_lstm_layers, cond_lstm_dropout])

    # Conditions for GRU
    cond_gru_units = EqualsCondition(gru_units, model_type, "GRU")
    cond_gru_layers = EqualsCondition(gru_layers, model_type, "GRU")
    cond_gru_dropout = EqualsCondition(gru_dropout, model_type, "GRU")
    cs.add_conditions([cond_gru_units, cond_gru_layers, cond_gru_dropout])

    # Conditions for MLP
    cond_mlp_layers = EqualsCondition(mlp_layers, model_type, "MLP")
    cond_mlp_units = EqualsCondition(mlp_units, model_type, "MLP")
    cond_mlp_dropout = EqualsCondition(mlp_dropout, model_type, "MLP")
    cs.add_conditions([cond_mlp_layers, cond_mlp_units, cond_mlp_dropout])

    # Forbidden Clauses - Example: LSTM with no layers
    forbidden_lstm_no_layers = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "LSTM"),
        ForbiddenEqualsClause(lstm_layers, 1)
    )

    #cs.add_forbidden_clause(forbidden_lstm_no_layers) # Removing forbidden clause.

    return cs

--------------------------------------------------------------------------------
[2025-06-07 14:00:29] [Metadata: {'component': 'config', 'status': 'success'}] Configuration space generated successfully
--------------------------------------------------------------------------------
[2025-06-07 14:00:31] [Metadata: {'component': 'scenario'}] ```python
from smac import Scenario
from ConfigSpace import ConfigurationSpace

def generate_scenario(cs):
    """
    Generates a Scenario object for SMAC.

    Args:
        cs (ConfigurationSpace): The configuration space to be used.

    Returns:
        Scenario: A configured Scenario object.
    """
    scenario = Scenario(
        configspace=cs,
        name="gemini-2.0-flashsunspots_(statsmodels)20250607_140029",
        output_directory="./automl_results",
        deterministic=False,
        n_workers=2,
        min_budget=1,
        max_budget=9,
        n_trials=10
    )
    return scenario
```
--------------------------------------------------------------------------------
[2025-06-07 14:00:31] [Metadata: {'component': 'scenario', 'action': 'run'}] from smac import Scenario
from ConfigSpace import ConfigurationSpace

def generate_scenario(cs):
    """
    Generates a Scenario object for SMAC.

    Args:
        cs (ConfigurationSpace): The configuration space to be used.

    Returns:
        Scenario: A configured Scenario object.
    """
    scenario = Scenario(
        configspace=cs,
        name="gemini-2.0-flashsunspots_(statsmodels)20250607_140029",
        output_directory="./automl_results",
        deterministic=False,
        n_workers=2,
        min_budget=1,
        max_budget=9,
        n_trials=10
    )
    return scenario

--------------------------------------------------------------------------------
[2025-06-07 14:00:31] [Metadata: {'component': 'scenario', 'status': 'success'}] Scenario generated successfully
--------------------------------------------------------------------------------
[2025-06-07 14:00:41] [Metadata: {'component': 'train_function'}] ```python
from typing import Any
from ConfigSpace import Configuration
import numpy as np

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): The configuration object specifying hyperparameters.
        dataset (Any): A dictionary containing the training data with keys 'X' and 'y'.
        seed (int): The random seed for reproducibility.

    Returns:
        float: The average training loss over 10 epochs.
    """

    torch.manual_seed(seed)
    np.random.seed(seed)

    X = dataset['X']
    y = dataset['y']

    # Convert data to tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.float32)  # Ensure y is float32

    # Create TensorDataset and DataLoader
    dataset_torch = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset_torch, batch_size=32, shuffle=True)  # Adjust batch_size as needed

    model_type = cfg.get('model_type')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        input_dim = X_tensor.shape[2]  # num_features
        sequence_length = X_tensor.shape[1]
    elif X_tensor.ndim == 2:
        input_dim = X_tensor.shape[1]
        sequence_length = 1 # Not applicable
    else:
        raise ValueError(f"Unexpected input dimension: {X_tensor.ndim}. Expected 2 or 3.")

    output_dim = 1  # Assuming regression task with a single output

    # Define the model
    if model_type == 'LSTM':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for LSTM, got {X_tensor.shape}")
        lstm_units = cfg.get('lstm_units')
        lstm_layers = cfg.get('lstm_layers')
        lstm_dropout = cfg.get('lstm_dropout')
        class LSTMModel(nn.Module):
            def __init__(self, input_dim, lstm_units, lstm_layers, output_dim, lstm_dropout):
                super(LSTMModel, self).__init__()
                self.lstm = nn.LSTM(input_dim, lstm_units, num_layers=lstm_layers, batch_first=True, dropout=lstm_dropout if lstm_layers > 1 else 0)
                self.linear = nn.Linear(lstm_units, output_dim)

            def forward(self, x):
                out, _ = self.lstm(x)
                out = self.linear(out[:, -1, :]) # many-to-one: take last time step output
                return out

        model = LSTMModel(input_dim, lstm_units, lstm_layers, output_dim, lstm_dropout)

    elif model_type == 'GRU':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for GRU, got {X_tensor.shape}")

        gru_units = cfg.get('gru_units')
        gru_layers = cfg.get('gru_layers')
        gru_dropout = cfg.get('gru_dropout')

        class GRUModel(nn.Module):
            def __init__(self, input_dim, gru_units, gru_layers, output_dim, gru_dropout):
                super(GRUModel, self).__init__()
                self.gru = nn.GRU(input_dim, gru_units, num_layers=gru_layers, batch_first=True, dropout=gru_dropout if gru_layers > 1 else 0)
                self.linear = nn.Linear(gru_units, output_dim)

            def forward(self, x):
                out, _ = self.gru(x)
                out = self.linear(out[:, -1, :])  # many-to-one: take last time step output
                return out

        model = GRUModel(input_dim, gru_units, gru_layers, output_dim, gru_dropout)

    elif model_type == 'MLP':
        mlp_layers = cfg.get('mlp_layers')
        mlp_units = cfg.get('mlp_units')
        mlp_dropout = cfg.get('mlp_dropout')

        class MLPModel(nn.Module):
            def __init__(self, input_dim, mlp_layers, mlp_units, output_dim, mlp_dropout):
                super(MLPModel, self).__init__()
                layers = []
                if mlp_layers == 1:
                   layers.append(nn.Linear(input_dim, mlp_units))
                   layers.append(nn.ReLU())
                   layers.append(nn.Dropout(mlp_dropout))
                   layers.append(nn.Linear(mlp_units, output_dim))
                elif mlp_layers == 2:
                    layers.append(nn.Linear(input_dim, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, output_dim))
                elif mlp_layers == 3:
                    layers.append(nn.Linear(input_dim, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, output_dim))

                self.layers = nn.Sequential(*layers)

            def forward(self, x):
                return self.layers(x)

        model = MLPModel(input_dim, mlp_layers, mlp_units, output_dim, mlp_dropout)
        if X_tensor.ndim == 3:
            X_tensor = X_tensor.reshape(X_tensor.shape[0], -1) # Flatten for MLP


    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    learning_rate = cfg.get('learning_rate')
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    criterion = nn.MSELoss()  # Mean Squared Error for regression

    # Train the model
    num_epochs = 10
    total_loss = 0.0

    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for X_batch, y_batch in dataloader:

            if model_type == 'MLP' and X_batch.ndim == 3:
                X_batch = X_batch.reshape(X_batch.shape[0], -1) # Flatten batch for MLP

            optimizer.zero_grad()
            outputs = model(X_batch)

            loss = criterion(outputs.squeeze(), y_batch)  # Ensure outputs are same shape as targets

            loss.backward()
            optimizer.step()
            epoch_loss += loss.item() * X_batch.size(0)  # Accumulate batch loss

        epoch_loss = epoch_loss / len(dataloader.dataset) # Average loss over dataset
        total_loss += epoch_loss
        #print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}") # Can be uncommented for debugging.


    avg_loss = total_loss / num_epochs

    return float(avg_loss)
```
--------------------------------------------------------------------------------
[2025-06-07 14:00:41] [Metadata: {'component': 'train_function', 'action': 'run'}] from typing import Any
from ConfigSpace import Configuration
import numpy as np

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): The configuration object specifying hyperparameters.
        dataset (Any): A dictionary containing the training data with keys 'X' and 'y'.
        seed (int): The random seed for reproducibility.

    Returns:
        float: The average training loss over 10 epochs.
    """

    torch.manual_seed(seed)
    np.random.seed(seed)

    X = dataset['X']
    y = dataset['y']

    # Convert data to tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.float32)  # Ensure y is float32

    # Create TensorDataset and DataLoader
    dataset_torch = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset_torch, batch_size=32, shuffle=True)  # Adjust batch_size as needed

    model_type = cfg.get('model_type')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        input_dim = X_tensor.shape[2]  # num_features
        sequence_length = X_tensor.shape[1]
    elif X_tensor.ndim == 2:
        input_dim = X_tensor.shape[1]
        sequence_length = 1 # Not applicable
    else:
        raise ValueError(f"Unexpected input dimension: {X_tensor.ndim}. Expected 2 or 3.")

    output_dim = 1  # Assuming regression task with a single output

    # Define the model
    if model_type == 'LSTM':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for LSTM, got {X_tensor.shape}")
        lstm_units = cfg.get('lstm_units')
        lstm_layers = cfg.get('lstm_layers')
        lstm_dropout = cfg.get('lstm_dropout')
        class LSTMModel(nn.Module):
            def __init__(self, input_dim, lstm_units, lstm_layers, output_dim, lstm_dropout):
                super(LSTMModel, self).__init__()
                self.lstm = nn.LSTM(input_dim, lstm_units, num_layers=lstm_layers, batch_first=True, dropout=lstm_dropout if lstm_layers > 1 else 0)
                self.linear = nn.Linear(lstm_units, output_dim)

            def forward(self, x):
                out, _ = self.lstm(x)
                out = self.linear(out[:, -1, :]) # many-to-one: take last time step output
                return out

        model = LSTMModel(input_dim, lstm_units, lstm_layers, output_dim, lstm_dropout)

    elif model_type == 'GRU':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for GRU, got {X_tensor.shape}")

        gru_units = cfg.get('gru_units')
        gru_layers = cfg.get('gru_layers')
        gru_dropout = cfg.get('gru_dropout')

        class GRUModel(nn.Module):
            def __init__(self, input_dim, gru_units, gru_layers, output_dim, gru_dropout):
                super(GRUModel, self).__init__()
                self.gru = nn.GRU(input_dim, gru_units, num_layers=gru_layers, batch_first=True, dropout=gru_dropout if gru_layers > 1 else 0)
                self.linear = nn.Linear(gru_units, output_dim)

            def forward(self, x):
                out, _ = self.gru(x)
                out = self.linear(out[:, -1, :])  # many-to-one: take last time step output
                return out

        model = GRUModel(input_dim, gru_units, gru_layers, output_dim, gru_dropout)

    elif model_type == 'MLP':
        mlp_layers = cfg.get('mlp_layers')
        mlp_units = cfg.get('mlp_units')
        mlp_dropout = cfg.get('mlp_dropout')

        class MLPModel(nn.Module):
            def __init__(self, input_dim, mlp_layers, mlp_units, output_dim, mlp_dropout):
                super(MLPModel, self).__init__()
                layers = []
                if mlp_layers == 1:
                   layers.append(nn.Linear(input_dim, mlp_units))
                   layers.append(nn.ReLU())
                   layers.append(nn.Dropout(mlp_dropout))
                   layers.append(nn.Linear(mlp_units, output_dim))
                elif mlp_layers == 2:
                    layers.append(nn.Linear(input_dim, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, output_dim))
                elif mlp_layers == 3:
                    layers.append(nn.Linear(input_dim, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, output_dim))

                self.layers = nn.Sequential(*layers)

            def forward(self, x):
                return self.layers(x)

        model = MLPModel(input_dim, mlp_layers, mlp_units, output_dim, mlp_dropout)
        if X_tensor.ndim == 3:
            X_tensor = X_tensor.reshape(X_tensor.shape[0], -1) # Flatten for MLP


    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    learning_rate = cfg.get('learning_rate')
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    criterion = nn.MSELoss()  # Mean Squared Error for regression

    # Train the model
    num_epochs = 10
    total_loss = 0.0

    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for X_batch, y_batch in dataloader:

            if model_type == 'MLP' and X_batch.ndim == 3:
                X_batch = X_batch.reshape(X_batch.shape[0], -1) # Flatten batch for MLP

            optimizer.zero_grad()
            outputs = model(X_batch)

            loss = criterion(outputs.squeeze(), y_batch)  # Ensure outputs are same shape as targets

            loss.backward()
            optimizer.step()
            epoch_loss += loss.item() * X_batch.size(0)  # Accumulate batch loss

        epoch_loss = epoch_loss / len(dataloader.dataset) # Average loss over dataset
        total_loss += epoch_loss
        #print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}") # Can be uncommented for debugging.


    avg_loss = total_loss / num_epochs

    return float(avg_loss)

--------------------------------------------------------------------------------
[2025-06-07 14:00:41] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#1): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-06-07 14:00:51] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
from typing import Any
from ConfigSpace import Configuration
import numpy as np

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): The configuration object specifying hyperparameters.
        dataset (Any): A dictionary containing the training data with keys 'X' and 'y'.
        seed (int): The random seed for reproducibility.

    Returns:
        float: The average training loss over 10 epochs.
    """

    torch.manual_seed(seed)
    np.random.seed(seed)

    X = dataset['X']
    y = dataset['y']

    # Convert data to tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.float32)

    # Create TensorDataset and DataLoader
    dataset_torch = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset_torch, batch_size=32, shuffle=True)

    model_type = cfg.get('model_type')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        input_dim = X_tensor.shape[2]  # num_features
        sequence_length = X_tensor.shape[1]
    elif X_tensor.ndim == 2:
        input_dim = X_tensor.shape[1]
        sequence_length = 1 # Not applicable
    else:
        raise ValueError(f"Unexpected input dimension: {X_tensor.ndim}. Expected 2 or 3.")

    output_dim = 1  # Assuming regression task with a single output

    # Define the model
    if model_type == 'LSTM':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for LSTM, got {X_tensor.shape}")
        lstm_units = cfg.get('lstm_units')
        lstm_layers = cfg.get('lstm_layers')
        lstm_dropout = cfg.get('lstm_dropout')
        class LSTMModel(nn.Module):
            def __init__(self, input_dim, lstm_units, lstm_layers, output_dim, lstm_dropout):
                super(LSTMModel, self).__init__()
                self.lstm = nn.LSTM(input_dim, lstm_units, num_layers=lstm_layers, batch_first=True, dropout=lstm_dropout if lstm_layers > 1 else 0)
                self.linear = nn.Linear(lstm_units, output_dim)

            def forward(self, x):
                out, _ = self.lstm(x)
                out = self.linear(out[:, -1, :]) # many-to-one: take last time step output
                return out

        model = LSTMModel(input_dim, lstm_units, lstm_layers, output_dim, lstm_dropout)

    elif model_type == 'GRU':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for GRU, got {X_tensor.shape}")

        gru_units = cfg.get('gru_units')
        gru_layers = cfg.get('gru_layers')
        gru_dropout = cfg.get('gru_dropout')

        class GRUModel(nn.Module):
            def __init__(self, input_dim, gru_units, gru_layers, output_dim, gru_dropout):
                super(GRUModel, self).__init__()
                self.gru = nn.GRU(input_dim, gru_units, num_layers=gru_layers, batch_first=True, dropout=gru_dropout if gru_layers > 1 else 0)
                self.linear = nn.Linear(gru_units, output_dim)

            def forward(self, x):
                out, _ = self.gru(x)
                out = self.linear(out[:, -1, :])  # many-to-one: take last time step output
                return out

        model = GRUModel(input_dim, gru_units, gru_layers, output_dim, gru_dropout)

    elif model_type == 'MLP':
        mlp_layers = cfg.get('mlp_layers')
        mlp_units = cfg.get('mlp_units')
        mlp_dropout = cfg.get('mlp_dropout')

        class MLPModel(nn.Module):
            def __init__(self, input_dim, mlp_layers, mlp_units, output_dim, mlp_dropout):
                super(MLPModel, self).__init__()
                layers = []
                if mlp_layers == 1:
                   layers.append(nn.Linear(input_dim, mlp_units))
                   layers.append(nn.ReLU())
                   layers.append(nn.Dropout(mlp_dropout))
                   layers.append(nn.Linear(mlp_units, output_dim))
                elif mlp_layers == 2:
                    layers.append(nn.Linear(input_dim, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, output_dim))
                elif mlp_layers == 3:
                    layers.append(nn.Linear(input_dim, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, output_dim))

                self.layers = nn.Sequential(*layers)

            def forward(self, x):
                return self.layers(x)

        model = MLPModel(input_dim, mlp_layers, mlp_units, output_dim, mlp_dropout)
        if X_tensor.ndim == 3:
            X_tensor = X_tensor.reshape(X_tensor.shape[0], -1) # Flatten for MLP


    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    learning_rate = cfg.get('learning_rate')
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    criterion = nn.MSELoss()  # Mean Squared Error for regression

    # Train the model
    num_epochs = 10
    total_loss = 0.0

    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for X_batch, y_batch in dataloader:

            if model_type == 'MLP' and len(X_batch.shape) == 3:
                X_batch = X_batch.reshape(X_batch.shape[0], -1) # Flatten batch for MLP

            optimizer.zero_grad()
            outputs = model(X_batch)

            loss = criterion(outputs.squeeze(), y_batch.squeeze())  # Ensure outputs are same shape as targets

            loss.backward()
            optimizer.step()
            epoch_loss += loss.item() * X_batch.size(0)  # Accumulate batch loss

        epoch_loss = epoch_loss / len(dataloader.dataset) # Average loss over dataset
        total_loss += epoch_loss
        #print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}") # Can be uncommented for debugging.


    avg_loss = total_loss / num_epochs

    return float(avg_loss)
```
--------------------------------------------------------------------------------
[2025-06-07 14:00:51] [Metadata: {'component': 'train_function', 'action': 'run'}] from typing import Any
from ConfigSpace import Configuration
import numpy as np

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): The configuration object specifying hyperparameters.
        dataset (Any): A dictionary containing the training data with keys 'X' and 'y'.
        seed (int): The random seed for reproducibility.

    Returns:
        float: The average training loss over 10 epochs.
    """

    torch.manual_seed(seed)
    np.random.seed(seed)

    X = dataset['X']
    y = dataset['y']

    # Convert data to tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.float32)

    # Create TensorDataset and DataLoader
    dataset_torch = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset_torch, batch_size=32, shuffle=True)

    model_type = cfg.get('model_type')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        input_dim = X_tensor.shape[2]  # num_features
        sequence_length = X_tensor.shape[1]
    elif X_tensor.ndim == 2:
        input_dim = X_tensor.shape[1]
        sequence_length = 1 # Not applicable
    else:
        raise ValueError(f"Unexpected input dimension: {X_tensor.ndim}. Expected 2 or 3.")

    output_dim = 1  # Assuming regression task with a single output

    # Define the model
    if model_type == 'LSTM':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for LSTM, got {X_tensor.shape}")
        lstm_units = cfg.get('lstm_units')
        lstm_layers = cfg.get('lstm_layers')
        lstm_dropout = cfg.get('lstm_dropout')
        class LSTMModel(nn.Module):
            def __init__(self, input_dim, lstm_units, lstm_layers, output_dim, lstm_dropout):
                super(LSTMModel, self).__init__()
                self.lstm = nn.LSTM(input_dim, lstm_units, num_layers=lstm_layers, batch_first=True, dropout=lstm_dropout if lstm_layers > 1 else 0)
                self.linear = nn.Linear(lstm_units, output_dim)

            def forward(self, x):
                out, _ = self.lstm(x)
                out = self.linear(out[:, -1, :]) # many-to-one: take last time step output
                return out

        model = LSTMModel(input_dim, lstm_units, lstm_layers, output_dim, lstm_dropout)

    elif model_type == 'GRU':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for GRU, got {X_tensor.shape}")

        gru_units = cfg.get('gru_units')
        gru_layers = cfg.get('gru_layers')
        gru_dropout = cfg.get('gru_dropout')

        class GRUModel(nn.Module):
            def __init__(self, input_dim, gru_units, gru_layers, output_dim, gru_dropout):
                super(GRUModel, self).__init__()
                self.gru = nn.GRU(input_dim, gru_units, num_layers=gru_layers, batch_first=True, dropout=gru_dropout if gru_layers > 1 else 0)
                self.linear = nn.Linear(gru_units, output_dim)

            def forward(self, x):
                out, _ = self.gru(x)
                out = self.linear(out[:, -1, :])  # many-to-one: take last time step output
                return out

        model = GRUModel(input_dim, gru_units, gru_layers, output_dim, gru_dropout)

    elif model_type == 'MLP':
        mlp_layers = cfg.get('mlp_layers')
        mlp_units = cfg.get('mlp_units')
        mlp_dropout = cfg.get('mlp_dropout')

        class MLPModel(nn.Module):
            def __init__(self, input_dim, mlp_layers, mlp_units, output_dim, mlp_dropout):
                super(MLPModel, self).__init__()
                layers = []
                if mlp_layers == 1:
                   layers.append(nn.Linear(input_dim, mlp_units))
                   layers.append(nn.ReLU())
                   layers.append(nn.Dropout(mlp_dropout))
                   layers.append(nn.Linear(mlp_units, output_dim))
                elif mlp_layers == 2:
                    layers.append(nn.Linear(input_dim, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, output_dim))
                elif mlp_layers == 3:
                    layers.append(nn.Linear(input_dim, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, output_dim))

                self.layers = nn.Sequential(*layers)

            def forward(self, x):
                return self.layers(x)

        model = MLPModel(input_dim, mlp_layers, mlp_units, output_dim, mlp_dropout)
        if X_tensor.ndim == 3:
            X_tensor = X_tensor.reshape(X_tensor.shape[0], -1) # Flatten for MLP


    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    learning_rate = cfg.get('learning_rate')
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    criterion = nn.MSELoss()  # Mean Squared Error for regression

    # Train the model
    num_epochs = 10
    total_loss = 0.0

    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for X_batch, y_batch in dataloader:

            if model_type == 'MLP' and len(X_batch.shape) == 3:
                X_batch = X_batch.reshape(X_batch.shape[0], -1) # Flatten batch for MLP

            optimizer.zero_grad()
            outputs = model(X_batch)

            loss = criterion(outputs.squeeze(), y_batch.squeeze())  # Ensure outputs are same shape as targets

            loss.backward()
            optimizer.step()
            epoch_loss += loss.item() * X_batch.size(0)  # Accumulate batch loss

        epoch_loss = epoch_loss / len(dataloader.dataset) # Average loss over dataset
        total_loss += epoch_loss
        #print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}") # Can be uncommented for debugging.


    avg_loss = total_loss / num_epochs

    return float(avg_loss)

--------------------------------------------------------------------------------
[2025-06-07 14:00:51] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#2): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-06-07 14:01:00] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
from typing import Any
from ConfigSpace import Configuration
import numpy as np

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): The configuration object specifying hyperparameters.
        dataset (Any): A dictionary containing the training data with keys 'X' and 'y'.
        seed (int): The random seed for reproducibility.

    Returns:
        float: The average training loss over 10 epochs.
    """

    torch.manual_seed(seed)
    np.random.seed(seed)

    X = dataset['X']
    y = dataset['y']

    # Convert data to tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.float32)

    # Create TensorDataset and DataLoader
    dataset_torch = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset_torch, batch_size=32, shuffle=True)

    model_type = cfg.get('model_type')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        input_dim = X_tensor.shape[2]  # num_features
        sequence_length = X_tensor.shape[1]
    elif X_tensor.ndim == 2:
        input_dim = X_tensor.shape[1]
        sequence_length = 1 # Not applicable
    else:
        raise ValueError(f"Unexpected input dimension: {X_tensor.ndim}. Expected 2 or 3.")

    output_dim = 1  # Assuming regression task with a single output

    # Define the model
    if model_type == 'LSTM':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for LSTM, got {X_tensor.shape}")
        lstm_units = cfg.get('lstm_units')
        lstm_layers = cfg.get('lstm_layers')
        lstm_dropout = cfg.get('lstm_dropout')
        class LSTMModel(nn.Module):
            def __init__(self, input_dim, lstm_units, lstm_layers, output_dim, lstm_dropout):
                super(LSTMModel, self).__init__()
                self.lstm = nn.LSTM(input_dim, lstm_units, num_layers=lstm_layers, batch_first=True, dropout=lstm_dropout if lstm_layers > 1 else 0)
                self.linear = nn.Linear(lstm_units, output_dim)

            def forward(self, x):
                out, _ = self.lstm(x)
                out = self.linear(out[:, -1, :]) # many-to-one: take last time step output
                return out

        model = LSTMModel(input_dim, lstm_units, lstm_layers, output_dim, lstm_dropout)

    elif model_type == 'GRU':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for GRU, got {X_tensor.shape}")

        gru_units = cfg.get('gru_units')
        gru_layers = cfg.get('gru_layers')
        gru_dropout = cfg.get('gru_dropout')

        class GRUModel(nn.Module):
            def __init__(self, input_dim, gru_units, gru_layers, output_dim, gru_dropout):
                super(GRUModel, self).__init__()
                self.gru = nn.GRU(input_dim, gru_units, num_layers=gru_layers, batch_first=True, dropout=gru_dropout if gru_layers > 1 else 0)
                self.linear = nn.Linear(gru_units, output_dim)

            def forward(self, x):
                out, _ = self.gru(x)
                out = self.linear(out[:, -1, :])  # many-to-one: take last time step output
                return out

        model = GRUModel(input_dim, gru_units, gru_layers, output_dim, gru_dropout)

    elif model_type == 'MLP':
        mlp_layers = cfg.get('mlp_layers')
        mlp_units = cfg.get('mlp_units')
        mlp_dropout = cfg.get('mlp_dropout')

        class MLPModel(nn.Module):
            def __init__(self, input_dim, mlp_layers, mlp_units, output_dim, mlp_dropout):
                super(MLPModel, self).__init__()
                layers = []
                if mlp_layers == 1:
                   layers.append(nn.Linear(input_dim, mlp_units))
                   layers.append(nn.ReLU())
                   layers.append(nn.Dropout(mlp_dropout))
                   layers.append(nn.Linear(mlp_units, output_dim))
                elif mlp_layers == 2:
                    layers.append(nn.Linear(input_dim, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, output_dim))
                elif mlp_layers == 3:
                    layers.append(nn.Linear(input_dim, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, output_dim))

                self.layers = nn.Sequential(*layers)

            def forward(self, x):
                return self.layers(x)

        model = MLPModel(input_dim, mlp_layers, mlp_units, output_dim, mlp_dropout)
        if X_tensor.ndim == 3:
            X_tensor = X_tensor.reshape(X_tensor.shape[0], -1) # Flatten for MLP


    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    learning_rate = cfg.get('learning_rate')
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    criterion = nn.MSELoss()  # Mean Squared Error for regression

    # Train the model
    num_epochs = 10
    total_loss = 0.0

    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for X_batch, y_batch in dataloader:

            if model_type == 'MLP' and len(X_batch.shape) == 3:
                X_batch = X_batch.reshape(X_batch.shape[0], -1) # Flatten batch for MLP

            optimizer.zero_grad()
            outputs = model(X_batch)

            loss = criterion(outputs.squeeze(), y_batch.squeeze())  # Ensure outputs are same shape as targets

            loss.backward()
            optimizer.step()
            epoch_loss += loss.item() * X_batch.size(0)  # Accumulate batch loss

        epoch_loss = epoch_loss / len(dataloader.dataset) # Average loss over dataset
        total_loss += epoch_loss
        #print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}") # Can be uncommented for debugging.


    avg_loss = total_loss / num_epochs

    return float(avg_loss)
```
--------------------------------------------------------------------------------
[2025-06-07 14:01:00] [Metadata: {'component': 'train_function', 'action': 'run'}] from typing import Any
from ConfigSpace import Configuration
import numpy as np

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): The configuration object specifying hyperparameters.
        dataset (Any): A dictionary containing the training data with keys 'X' and 'y'.
        seed (int): The random seed for reproducibility.

    Returns:
        float: The average training loss over 10 epochs.
    """

    torch.manual_seed(seed)
    np.random.seed(seed)

    X = dataset['X']
    y = dataset['y']

    # Convert data to tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.float32)

    # Create TensorDataset and DataLoader
    dataset_torch = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset_torch, batch_size=32, shuffle=True)

    model_type = cfg.get('model_type')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        input_dim = X_tensor.shape[2]  # num_features
        sequence_length = X_tensor.shape[1]
    elif X_tensor.ndim == 2:
        input_dim = X_tensor.shape[1]
        sequence_length = 1 # Not applicable
    else:
        raise ValueError(f"Unexpected input dimension: {X_tensor.ndim}. Expected 2 or 3.")

    output_dim = 1  # Assuming regression task with a single output

    # Define the model
    if model_type == 'LSTM':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for LSTM, got {X_tensor.shape}")
        lstm_units = cfg.get('lstm_units')
        lstm_layers = cfg.get('lstm_layers')
        lstm_dropout = cfg.get('lstm_dropout')
        class LSTMModel(nn.Module):
            def __init__(self, input_dim, lstm_units, lstm_layers, output_dim, lstm_dropout):
                super(LSTMModel, self).__init__()
                self.lstm = nn.LSTM(input_dim, lstm_units, num_layers=lstm_layers, batch_first=True, dropout=lstm_dropout if lstm_layers > 1 else 0)
                self.linear = nn.Linear(lstm_units, output_dim)

            def forward(self, x):
                out, _ = self.lstm(x)
                out = self.linear(out[:, -1, :]) # many-to-one: take last time step output
                return out

        model = LSTMModel(input_dim, lstm_units, lstm_layers, output_dim, lstm_dropout)

    elif model_type == 'GRU':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for GRU, got {X_tensor.shape}")

        gru_units = cfg.get('gru_units')
        gru_layers = cfg.get('gru_layers')
        gru_dropout = cfg.get('gru_dropout')

        class GRUModel(nn.Module):
            def __init__(self, input_dim, gru_units, gru_layers, output_dim, gru_dropout):
                super(GRUModel, self).__init__()
                self.gru = nn.GRU(input_dim, gru_units, num_layers=gru_layers, batch_first=True, dropout=gru_dropout if gru_layers > 1 else 0)
                self.linear = nn.Linear(gru_units, output_dim)

            def forward(self, x):
                out, _ = self.gru(x)
                out = self.linear(out[:, -1, :])  # many-to-one: take last time step output
                return out

        model = GRUModel(input_dim, gru_units, gru_layers, output_dim, gru_dropout)

    elif model_type == 'MLP':
        mlp_layers = cfg.get('mlp_layers')
        mlp_units = cfg.get('mlp_units')
        mlp_dropout = cfg.get('mlp_dropout')

        class MLPModel(nn.Module):
            def __init__(self, input_dim, mlp_layers, mlp_units, output_dim, mlp_dropout):
                super(MLPModel, self).__init__()
                layers = []
                if mlp_layers == 1:
                   layers.append(nn.Linear(input_dim, mlp_units))
                   layers.append(nn.ReLU())
                   layers.append(nn.Dropout(mlp_dropout))
                   layers.append(nn.Linear(mlp_units, output_dim))
                elif mlp_layers == 2:
                    layers.append(nn.Linear(input_dim, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, output_dim))
                elif mlp_layers == 3:
                    layers.append(nn.Linear(input_dim, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, output_dim))

                self.layers = nn.Sequential(*layers)

            def forward(self, x):
                return self.layers(x)

        model = MLPModel(input_dim, mlp_layers, mlp_units, output_dim, mlp_dropout)
        if X_tensor.ndim == 3:
            X_tensor = X_tensor.reshape(X_tensor.shape[0], -1) # Flatten for MLP


    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    learning_rate = cfg.get('learning_rate')
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    criterion = nn.MSELoss()  # Mean Squared Error for regression

    # Train the model
    num_epochs = 10
    total_loss = 0.0

    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for X_batch, y_batch in dataloader:

            if model_type == 'MLP' and len(X_batch.shape) == 3:
                X_batch = X_batch.reshape(X_batch.shape[0], -1) # Flatten batch for MLP

            optimizer.zero_grad()
            outputs = model(X_batch)

            loss = criterion(outputs.squeeze(), y_batch.squeeze())  # Ensure outputs are same shape as targets

            loss.backward()
            optimizer.step()
            epoch_loss += loss.item() * X_batch.size(0)  # Accumulate batch loss

        epoch_loss = epoch_loss / len(dataloader.dataset) # Average loss over dataset
        total_loss += epoch_loss
        #print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}") # Can be uncommented for debugging.


    avg_loss = total_loss / num_epochs

    return float(avg_loss)

--------------------------------------------------------------------------------
[2025-06-07 14:01:00] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#3): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-06-07 14:01:09] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
from typing import Any
from ConfigSpace import Configuration
import numpy as np

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): The configuration object specifying hyperparameters.
        dataset (Any): A dictionary containing the training data with keys 'X' and 'y'.
        seed (int): The random seed for reproducibility.

    Returns:
        float: The average training loss over 10 epochs.
    """

    torch.manual_seed(seed)
    np.random.seed(seed)

    X = dataset['X']
    y = dataset['y']

    # Convert data to tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.float32)

    # Create TensorDataset and DataLoader
    dataset_torch = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset_torch, batch_size=32, shuffle=True)

    model_type = cfg.get('model_type')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        input_dim = X_tensor.shape[2]  # num_features
        sequence_length = X_tensor.shape[1]
    elif X_tensor.ndim == 2:
        input_dim = X_tensor.shape[1]
        sequence_length = 1 # Not applicable
    else:
        raise ValueError(f"Unexpected input dimension: {X_tensor.ndim}. Expected 2 or 3.")

    output_dim = 1  # Assuming regression task with a single output

    # Define the model
    if model_type == 'LSTM':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for LSTM, got {X_tensor.shape}")
        lstm_units = cfg.get('lstm_units')
        lstm_layers = cfg.get('lstm_layers')
        lstm_dropout = cfg.get('lstm_dropout')
        class LSTMModel(nn.Module):
            def __init__(self, input_dim, lstm_units, lstm_layers, output_dim, lstm_dropout):
                super(LSTMModel, self).__init__()
                self.lstm = nn.LSTM(input_dim, lstm_units, num_layers=lstm_layers, batch_first=True, dropout=lstm_dropout if lstm_layers > 1 else 0)
                self.linear = nn.Linear(lstm_units, output_dim)

            def forward(self, x):
                out, _ = self.lstm(x)
                out = self.linear(out[:, -1, :]) # many-to-one: take last time step output
                return out

        model = LSTMModel(input_dim, lstm_units, lstm_layers, output_dim, lstm_dropout)

    elif model_type == 'GRU':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for GRU, got {X_tensor.shape}")

        gru_units = cfg.get('gru_units')
        gru_layers = cfg.get('gru_layers')
        gru_dropout = cfg.get('gru_dropout')

        class GRUModel(nn.Module):
            def __init__(self, input_dim, gru_units, gru_layers, output_dim, gru_dropout):
                super(GRUModel, self).__init__()
                self.gru = nn.GRU(input_dim, gru_units, num_layers=gru_layers, batch_first=True, dropout=gru_dropout if gru_layers > 1 else 0)
                self.linear = nn.Linear(gru_units, output_dim)

            def forward(self, x):
                out, _ = self.gru(x)
                out = self.linear(out[:, -1, :])  # many-to-one: take last time step output
                return out

        model = GRUModel(input_dim, gru_units, gru_layers, output_dim, gru_dropout)

    elif model_type == 'MLP':
        mlp_layers = cfg.get('mlp_layers')
        mlp_units = cfg.get('mlp_units')
        mlp_dropout = cfg.get('mlp_dropout')

        class MLPModel(nn.Module):
            def __init__(self, input_dim, mlp_layers, mlp_units, output_dim, mlp_dropout):
                super(MLPModel, self).__init__()
                layers = []
                if mlp_layers == 1:
                   layers.append(nn.Linear(input_dim, mlp_units))
                   layers.append(nn.ReLU())
                   layers.append(nn.Dropout(mlp_dropout))
                   layers.append(nn.Linear(mlp_units, output_dim))
                elif mlp_layers == 2:
                    layers.append(nn.Linear(input_dim, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, output_dim))
                elif mlp_layers == 3:
                    layers.append(nn.Linear(input_dim, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, output_dim))

                self.layers = nn.Sequential(*layers)

            def forward(self, x):
                return self.layers(x)

        model = MLPModel(input_dim, mlp_layers, mlp_units, output_dim, mlp_dropout)
        if X_tensor.ndim == 3:
            X_tensor = X_tensor.reshape(X_tensor.shape[0], -1) # Flatten for MLP


    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    learning_rate = cfg.get('learning_rate')
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    criterion = nn.MSELoss()  # Mean Squared Error for regression

    # Train the model
    num_epochs = 10
    total_loss = 0.0

    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for X_batch, y_batch in dataloader:

            if model_type == 'MLP' and len(X_batch.shape) == 3:
                X_batch = X_batch.reshape(X_batch.shape[0], -1) # Flatten batch for MLP

            optimizer.zero_grad()
            outputs = model(X_batch)

            loss = criterion(outputs.squeeze(), y_batch.squeeze())  # Ensure outputs are same shape as targets

            loss.backward()
            optimizer.step()
            epoch_loss += loss.item() * X_batch.size(0)  # Accumulate batch loss

        epoch_loss = epoch_loss / len(dataloader.dataset) # Average loss over dataset
        total_loss += epoch_loss
        #print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}") # Can be uncommented for debugging.


    avg_loss = total_loss / num_epochs

    return float(avg_loss)
```
--------------------------------------------------------------------------------
[2025-06-07 14:01:09] [Metadata: {'component': 'train_function', 'action': 'run'}] from typing import Any
from ConfigSpace import Configuration
import numpy as np

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): The configuration object specifying hyperparameters.
        dataset (Any): A dictionary containing the training data with keys 'X' and 'y'.
        seed (int): The random seed for reproducibility.

    Returns:
        float: The average training loss over 10 epochs.
    """

    torch.manual_seed(seed)
    np.random.seed(seed)

    X = dataset['X']
    y = dataset['y']

    # Convert data to tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.float32)

    # Create TensorDataset and DataLoader
    dataset_torch = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset_torch, batch_size=32, shuffle=True)

    model_type = cfg.get('model_type')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        input_dim = X_tensor.shape[2]  # num_features
        sequence_length = X_tensor.shape[1]
    elif X_tensor.ndim == 2:
        input_dim = X_tensor.shape[1]
        sequence_length = 1 # Not applicable
    else:
        raise ValueError(f"Unexpected input dimension: {X_tensor.ndim}. Expected 2 or 3.")

    output_dim = 1  # Assuming regression task with a single output

    # Define the model
    if model_type == 'LSTM':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for LSTM, got {X_tensor.shape}")
        lstm_units = cfg.get('lstm_units')
        lstm_layers = cfg.get('lstm_layers')
        lstm_dropout = cfg.get('lstm_dropout')
        class LSTMModel(nn.Module):
            def __init__(self, input_dim, lstm_units, lstm_layers, output_dim, lstm_dropout):
                super(LSTMModel, self).__init__()
                self.lstm = nn.LSTM(input_dim, lstm_units, num_layers=lstm_layers, batch_first=True, dropout=lstm_dropout if lstm_layers > 1 else 0)
                self.linear = nn.Linear(lstm_units, output_dim)

            def forward(self, x):
                out, _ = self.lstm(x)
                out = self.linear(out[:, -1, :]) # many-to-one: take last time step output
                return out

        model = LSTMModel(input_dim, lstm_units, lstm_layers, output_dim, lstm_dropout)

    elif model_type == 'GRU':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for GRU, got {X_tensor.shape}")

        gru_units = cfg.get('gru_units')
        gru_layers = cfg.get('gru_layers')
        gru_dropout = cfg.get('gru_dropout')

        class GRUModel(nn.Module):
            def __init__(self, input_dim, gru_units, gru_layers, output_dim, gru_dropout):
                super(GRUModel, self).__init__()
                self.gru = nn.GRU(input_dim, gru_units, num_layers=gru_layers, batch_first=True, dropout=gru_dropout if gru_layers > 1 else 0)
                self.linear = nn.Linear(gru_units, output_dim)

            def forward(self, x):
                out, _ = self.gru(x)
                out = self.linear(out[:, -1, :])  # many-to-one: take last time step output
                return out

        model = GRUModel(input_dim, gru_units, gru_layers, output_dim, gru_dropout)

    elif model_type == 'MLP':
        mlp_layers = cfg.get('mlp_layers')
        mlp_units = cfg.get('mlp_units')
        mlp_dropout = cfg.get('mlp_dropout')

        class MLPModel(nn.Module):
            def __init__(self, input_dim, mlp_layers, mlp_units, output_dim, mlp_dropout):
                super(MLPModel, self).__init__()
                layers = []
                if mlp_layers == 1:
                   layers.append(nn.Linear(input_dim, mlp_units))
                   layers.append(nn.ReLU())
                   layers.append(nn.Dropout(mlp_dropout))
                   layers.append(nn.Linear(mlp_units, output_dim))
                elif mlp_layers == 2:
                    layers.append(nn.Linear(input_dim, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, output_dim))
                elif mlp_layers == 3:
                    layers.append(nn.Linear(input_dim, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                    layers.append(nn.Linear(mlp_units, output_dim))

                self.layers = nn.Sequential(*layers)

            def forward(self, x):
                return self.layers(x)

        model = MLPModel(input_dim, mlp_layers, mlp_units, output_dim, mlp_dropout)
        if X_tensor.ndim == 3:
            X_tensor = X_tensor.reshape(X_tensor.shape[0], -1) # Flatten for MLP


    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    learning_rate = cfg.get('learning_rate')
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    criterion = nn.MSELoss()  # Mean Squared Error for regression

    # Train the model
    num_epochs = 10
    total_loss = 0.0

    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for X_batch, y_batch in dataloader:

            if model_type == 'MLP' and len(X_batch.shape) == 3:
                X_batch = X_batch.reshape(X_batch.shape[0], -1) # Flatten batch for MLP

            optimizer.zero_grad()
            outputs = model(X_batch)

            loss = criterion(outputs.squeeze(), y_batch.squeeze())  # Ensure outputs are same shape as targets

            loss.backward()
            optimizer.step()
            epoch_loss += loss.item() * X_batch.size(0)  # Accumulate batch loss

        epoch_loss = epoch_loss / len(dataloader.dataset) # Average loss over dataset
        total_loss += epoch_loss
        #print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}") # Can be uncommented for debugging.


    avg_loss = total_loss / num_epochs

    return float(avg_loss)

--------------------------------------------------------------------------------
[2025-06-07 14:01:09] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#4): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-06-07 14:01:18] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
from typing import Any
from ConfigSpace import Configuration
import numpy as np

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): The configuration object specifying hyperparameters.
        dataset (Any): A dictionary containing the training data with keys 'X' and 'y'.
        seed (int): The random seed for reproducibility.

    Returns:
        float: The average training loss over 10 epochs.
    """

    torch.manual_seed(seed)
    np.random.seed(seed)

    X = dataset['X']
    y = dataset['y']

    # Convert data to tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.float32)

    # Create TensorDataset and DataLoader
    dataset_torch = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset_torch, batch_size=32, shuffle=True)

    model_type = cfg.get('model_type')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        input_dim = X_tensor.shape[2]  # num_features
        sequence_length = X_tensor.shape[1]
    elif X_tensor.ndim == 2:
        input_dim = X_tensor.shape[1]
        sequence_length = 1 # Not applicable
    else:
        raise ValueError(f"Unexpected input dimension: {X_tensor.ndim}. Expected 2 or 3.")

    output_dim = 1  # Assuming regression task with a single output

    # Define the model
    if model_type == 'LSTM':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for LSTM, got {X_tensor.shape}")
        lstm_units = cfg.get('lstm_units')
        lstm_layers = cfg.get('lstm_layers')
        lstm_dropout = cfg.get('lstm_dropout')
        class LSTMModel(nn.Module):
            def __init__(self, input_dim, lstm_units, lstm_layers, output_dim, lstm_dropout):
                super(LSTMModel, self).__init__()
                self.lstm = nn.LSTM(input_dim, lstm_units, num_layers=lstm_layers, batch_first=True, dropout=lstm_dropout if lstm_layers > 1 else 0)
                self.linear = nn.Linear(lstm_units, output_dim)

            def forward(self, x):
                out, _ = self.lstm(x)
                out = self.linear(out[:, -1, :]) # many-to-one: take last time step output
                return out

        model = LSTMModel(input_dim, lstm_units, lstm_layers, output_dim, lstm_dropout)

    elif model_type == 'GRU':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for GRU, got {X_tensor.shape}")

        gru_units = cfg.get('gru_units')
        gru_layers = cfg.get('gru_layers')
        gru_dropout = cfg.get('gru_dropout')

        class GRUModel(nn.Module):
            def __init__(self, input_dim, gru_units, gru_layers, output_dim, gru_dropout):
                super(GRUModel, self).__init__()
                self.gru = nn.GRU(input_dim, gru_units, num_layers=gru_layers, batch_first=True, dropout=gru_dropout if gru_layers > 1 else 0)
                self.linear = nn.Linear(gru_units, output_dim)

            def forward(self, x):
                out, _ = self.gru(x)
                out = self.linear(out[:, -1, :])  # many-to-one: take last time step output
                return out

        model = GRUModel(input_dim, gru_units, gru_layers, output_dim, gru_dropout)

    elif model_type == 'MLP':
        mlp_layers = cfg.get('mlp_layers')
        mlp_units = cfg.get('mlp_units')
        mlp_dropout = cfg.get('mlp_dropout')

        class MLPModel(nn.Module):
            def __init__(self, input_dim, mlp_layers, mlp_units, output_dim, mlp_dropout):
                super(MLPModel, self).__init__()
                layers = []
                layers.append(nn.Linear(input_dim, mlp_units))
                layers.append(nn.ReLU())
                layers.append(nn.Dropout(mlp_dropout))
                for _ in range(mlp_layers - 1):
                    layers.append(nn.Linear(mlp_units, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                layers.append(nn.Linear(mlp_units, output_dim))

                self.layers = nn.Sequential(*layers)

            def forward(self, x):
                return self.layers(x)

        model = MLPModel(input_dim, mlp_layers, mlp_units, output_dim, mlp_dropout)
        if X_tensor.ndim == 3:
            X_tensor = X_tensor.reshape(X_tensor.shape[0], -1) # Flatten for MLP


    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    learning_rate = cfg.get('learning_rate')
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    criterion = nn.MSELoss()  # Mean Squared Error for regression

    # Train the model
    num_epochs = 10
    total_loss = 0.0

    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for X_batch, y_batch in dataloader:

            if model_type == 'MLP' and len(X_batch.shape) == 3:
                X_batch = X_batch.reshape(X_batch.shape[0], -1) # Flatten batch for MLP

            optimizer.zero_grad()
            outputs = model(X_batch)

            loss = criterion(outputs.squeeze(), y_batch.squeeze())  # Ensure outputs are same shape as targets

            loss.backward()
            optimizer.step()
            epoch_loss += loss.item() * X_batch.size(0)  # Accumulate batch loss

        epoch_loss = epoch_loss / len(dataloader.dataset) # Average loss over dataset
        total_loss += epoch_loss
        #print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}") # Can be uncommented for debugging.


    avg_loss = total_loss / num_epochs

    return float(avg_loss)
```
--------------------------------------------------------------------------------
[2025-06-07 14:01:18] [Metadata: {'component': 'train_function', 'action': 'run'}] from typing import Any
from ConfigSpace import Configuration
import numpy as np

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): The configuration object specifying hyperparameters.
        dataset (Any): A dictionary containing the training data with keys 'X' and 'y'.
        seed (int): The random seed for reproducibility.

    Returns:
        float: The average training loss over 10 epochs.
    """

    torch.manual_seed(seed)
    np.random.seed(seed)

    X = dataset['X']
    y = dataset['y']

    # Convert data to tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.float32)

    # Create TensorDataset and DataLoader
    dataset_torch = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset_torch, batch_size=32, shuffle=True)

    model_type = cfg.get('model_type')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        input_dim = X_tensor.shape[2]  # num_features
        sequence_length = X_tensor.shape[1]
    elif X_tensor.ndim == 2:
        input_dim = X_tensor.shape[1]
        sequence_length = 1 # Not applicable
    else:
        raise ValueError(f"Unexpected input dimension: {X_tensor.ndim}. Expected 2 or 3.")

    output_dim = 1  # Assuming regression task with a single output

    # Define the model
    if model_type == 'LSTM':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for LSTM, got {X_tensor.shape}")
        lstm_units = cfg.get('lstm_units')
        lstm_layers = cfg.get('lstm_layers')
        lstm_dropout = cfg.get('lstm_dropout')
        class LSTMModel(nn.Module):
            def __init__(self, input_dim, lstm_units, lstm_layers, output_dim, lstm_dropout):
                super(LSTMModel, self).__init__()
                self.lstm = nn.LSTM(input_dim, lstm_units, num_layers=lstm_layers, batch_first=True, dropout=lstm_dropout if lstm_layers > 1 else 0)
                self.linear = nn.Linear(lstm_units, output_dim)

            def forward(self, x):
                out, _ = self.lstm(x)
                out = self.linear(out[:, -1, :]) # many-to-one: take last time step output
                return out

        model = LSTMModel(input_dim, lstm_units, lstm_layers, output_dim, lstm_dropout)

    elif model_type == 'GRU':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for GRU, got {X_tensor.shape}")

        gru_units = cfg.get('gru_units')
        gru_layers = cfg.get('gru_layers')
        gru_dropout = cfg.get('gru_dropout')

        class GRUModel(nn.Module):
            def __init__(self, input_dim, gru_units, gru_layers, output_dim, gru_dropout):
                super(GRUModel, self).__init__()
                self.gru = nn.GRU(input_dim, gru_units, num_layers=gru_layers, batch_first=True, dropout=gru_dropout if gru_layers > 1 else 0)
                self.linear = nn.Linear(gru_units, output_dim)

            def forward(self, x):
                out, _ = self.gru(x)
                out = self.linear(out[:, -1, :])  # many-to-one: take last time step output
                return out

        model = GRUModel(input_dim, gru_units, gru_layers, output_dim, gru_dropout)

    elif model_type == 'MLP':
        mlp_layers = cfg.get('mlp_layers')
        mlp_units = cfg.get('mlp_units')
        mlp_dropout = cfg.get('mlp_dropout')

        class MLPModel(nn.Module):
            def __init__(self, input_dim, mlp_layers, mlp_units, output_dim, mlp_dropout):
                super(MLPModel, self).__init__()
                layers = []
                layers.append(nn.Linear(input_dim, mlp_units))
                layers.append(nn.ReLU())
                layers.append(nn.Dropout(mlp_dropout))
                for _ in range(mlp_layers - 1):
                    layers.append(nn.Linear(mlp_units, mlp_units))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(mlp_dropout))
                layers.append(nn.Linear(mlp_units, output_dim))

                self.layers = nn.Sequential(*layers)

            def forward(self, x):
                return self.layers(x)

        model = MLPModel(input_dim, mlp_layers, mlp_units, output_dim, mlp_dropout)
        if X_tensor.ndim == 3:
            X_tensor = X_tensor.reshape(X_tensor.shape[0], -1) # Flatten for MLP


    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    learning_rate = cfg.get('learning_rate')
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    criterion = nn.MSELoss()  # Mean Squared Error for regression

    # Train the model
    num_epochs = 10
    total_loss = 0.0

    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for X_batch, y_batch in dataloader:

            if model_type == 'MLP' and len(X_batch.shape) == 3:
                X_batch = X_batch.reshape(X_batch.shape[0], -1) # Flatten batch for MLP

            optimizer.zero_grad()
            outputs = model(X_batch)

            loss = criterion(outputs.squeeze(), y_batch.squeeze())  # Ensure outputs are same shape as targets

            loss.backward()
            optimizer.step()
            epoch_loss += loss.item() * X_batch.size(0)  # Accumulate batch loss

        epoch_loss = epoch_loss / len(dataloader.dataset) # Average loss over dataset
        total_loss += epoch_loss
        #print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}") # Can be uncommented for debugging.


    avg_loss = total_loss / num_epochs

    return float(avg_loss)

--------------------------------------------------------------------------------
[2025-06-07 14:01:18] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#5): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-06-07 14:01:24] [Metadata: {'component': 'train_function', 'action': 'run'}] from typing import Any
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from ConfigSpace import Configuration

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing the training data ('X' and 'y').
        seed (int): Random seed for reproducibility.

    Returns:
        float: The average training loss over 10 epochs.
    """

    torch.manual_seed(seed)
    np.random.seed(seed)

    X = dataset['X']
    y = dataset['y']

    # Convert data to PyTorch tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.float32)

    model_type = cfg.get('model_type')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        input_dim = X_tensor.shape[2]
        seq_len = X_tensor.shape[1]
    elif X_tensor.ndim == 2:
      input_dim = X_tensor.shape[1]
      seq_len = 1 # Assume seq_len is 1 for MLP with 2D input
    else:
        raise ValueError(f"Unsupported input dimension: {X_tensor.ndim}")
    output_dim = 1  # Regression task

    # Define the model
    if model_type == 'LSTM':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for LSTM, got {X_tensor.shape}")

        lstm_units = cfg.get('lstm_units')
        lstm_layers = cfg.get('lstm_layers')
        lstm_dropout = cfg.get('lstm_dropout')
        model = nn.LSTM(input_dim, lstm_units, num_layers=lstm_layers, dropout=lstm_dropout, batch_first=True)
        model = nn.Sequential(
            model,
            nn.Linear(lstm_units, output_dim)
        )

    elif model_type == 'GRU':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for GRU, got {X_tensor.shape}")

        gru_units = cfg.get('gru_units')
        gru_layers = cfg.get('gru_layers')
        gru_dropout = cfg.get('gru_dropout')
        model = nn.GRU(input_dim, gru_units, num_layers=gru_layers, dropout=gru_dropout, batch_first=True)
        model = nn.Sequential(
            model,
            nn.Linear(gru_units, output_dim)
        )

    elif model_type == 'MLP':
        mlp_layers = cfg.get('mlp_layers')
        mlp_units = cfg.get('mlp_units')
        mlp_dropout = cfg.get('mlp_dropout')

        if X_tensor.ndim == 3: # Reshape if input is 3D for time series
            X_tensor = X_tensor.reshape(X_tensor.shape[0], -1)
            input_dim = X_tensor.shape[1]

        layers = []
        layers.append(nn.Linear(input_dim, mlp_units))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(mlp_dropout))

        for _ in range(mlp_layers - 1):
            layers.append(nn.Linear(mlp_units, mlp_units))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(mlp_dropout))

        layers.append(nn.Linear(mlp_units, output_dim))
        model = nn.Sequential(*layers)

    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Define the loss function and optimizer
    learning_rate = cfg.get('learning_rate')
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # Training loop
    epochs = 10
    total_loss = 0.0
    model.train()  # Set the model to training mode

    for epoch in range(epochs):
        optimizer.zero_grad()
        if model_type in ['LSTM', 'GRU']:
            outputs = model(X_tensor)
        else:
            outputs = model(X_tensor)
        loss = criterion(outputs, y_tensor.unsqueeze(1)) # Reshape y_tensor to (batch_size, 1)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    avg_loss = total_loss / epochs
    return avg_loss

--------------------------------------------------------------------------------
[2025-06-07 14:01:24] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#1): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-06-07 14:01:30] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
from typing import Any
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from ConfigSpace import Configuration

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing the training data ('X' and 'y').
        seed (int): Random seed for reproducibility.

    Returns:
        float: The average training loss over 10 epochs.
    """

    torch.manual_seed(seed)
    np.random.seed(seed)

    X = dataset['X']
    y = dataset['y']

    # Convert data to PyTorch tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.float32)

    model_type = cfg.get('model_type')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        input_dim = X_tensor.shape[2]
        seq_len = X_tensor.shape[1]
    elif X_tensor.ndim == 2:
      input_dim = X_tensor.shape[1]
      seq_len = 1 # Assume seq_len is 1 for MLP with 2D input
    else:
        raise ValueError(f"Unsupported input dimension: {X_tensor.ndim}")
    output_dim = 1  # Regression task

    # Define the model
    if model_type == 'LSTM':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for LSTM, got {X_tensor.shape}")

        lstm_units = cfg.get('lstm_units')
        lstm_layers = cfg.get('lstm_layers')
        lstm_dropout = cfg.get('lstm_dropout')
        model = nn.LSTM(input_dim, lstm_units, num_layers=lstm_layers, dropout=lstm_dropout, batch_first=True)
        model = nn.Sequential(
            model,
            nn.Linear(lstm_units, output_dim)
        )

    elif model_type == 'GRU':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for GRU, got {X_tensor.shape}")

        gru_units = cfg.get('gru_units')
        gru_layers = cfg.get('gru_layers')
        gru_dropout = cfg.get('gru_dropout')
        model = nn.GRU(input_dim, gru_units, num_layers=gru_layers, dropout=gru_dropout, batch_first=True)
        model = nn.Sequential(
            model,
            nn.Linear(gru_units, output_dim)
        )

    elif model_type == 'MLP':
        mlp_layers = cfg.get('mlp_layers')
        mlp_units = cfg.get('mlp_units')
        mlp_dropout = cfg.get('mlp_dropout')

        if X_tensor.ndim == 3: # Reshape if input is 3D for time series
            X_tensor = X_tensor.reshape(X_tensor.shape[0], -1)
            input_dim = X_tensor.shape[1]

        layers = []
        layers.append(nn.Linear(input_dim, mlp_units))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(mlp_dropout))

        for _ in range(mlp_layers - 1):
            layers.append(nn.Linear(mlp_units, mlp_units))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(mlp_dropout))

        layers.append(nn.Linear(mlp_units, output_dim))
        model = nn.Sequential(*layers)

    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Define the loss function and optimizer
    learning_rate = cfg.get('learning_rate')
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # Training loop
    epochs = 10
    total_loss = 0.0
    model.train()  # Set the model to training mode

    for epoch in range(epochs):
        optimizer.zero_grad()
        if model_type in ['LSTM', 'GRU']:
            outputs = model(X_tensor)
        else:
            outputs = model(X_tensor)
        loss = criterion(outputs, y_tensor.reshape(-1, 1)) # Reshape y_tensor to (batch_size, 1)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    avg_loss = total_loss / epochs
    return avg_loss
```
--------------------------------------------------------------------------------
[2025-06-07 14:01:30] [Metadata: {'component': 'train_function', 'action': 'run'}] from typing import Any
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from ConfigSpace import Configuration

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing the training data ('X' and 'y').
        seed (int): Random seed for reproducibility.

    Returns:
        float: The average training loss over 10 epochs.
    """

    torch.manual_seed(seed)
    np.random.seed(seed)

    X = dataset['X']
    y = dataset['y']

    # Convert data to PyTorch tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.float32)

    model_type = cfg.get('model_type')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        input_dim = X_tensor.shape[2]
        seq_len = X_tensor.shape[1]
    elif X_tensor.ndim == 2:
      input_dim = X_tensor.shape[1]
      seq_len = 1 # Assume seq_len is 1 for MLP with 2D input
    else:
        raise ValueError(f"Unsupported input dimension: {X_tensor.ndim}")
    output_dim = 1  # Regression task

    # Define the model
    if model_type == 'LSTM':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for LSTM, got {X_tensor.shape}")

        lstm_units = cfg.get('lstm_units')
        lstm_layers = cfg.get('lstm_layers')
        lstm_dropout = cfg.get('lstm_dropout')
        model = nn.LSTM(input_dim, lstm_units, num_layers=lstm_layers, dropout=lstm_dropout, batch_first=True)
        model = nn.Sequential(
            model,
            nn.Linear(lstm_units, output_dim)
        )

    elif model_type == 'GRU':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for GRU, got {X_tensor.shape}")

        gru_units = cfg.get('gru_units')
        gru_layers = cfg.get('gru_layers')
        gru_dropout = cfg.get('gru_dropout')
        model = nn.GRU(input_dim, gru_units, num_layers=gru_layers, dropout=gru_dropout, batch_first=True)
        model = nn.Sequential(
            model,
            nn.Linear(gru_units, output_dim)
        )

    elif model_type == 'MLP':
        mlp_layers = cfg.get('mlp_layers')
        mlp_units = cfg.get('mlp_units')
        mlp_dropout = cfg.get('mlp_dropout')

        if X_tensor.ndim == 3: # Reshape if input is 3D for time series
            X_tensor = X_tensor.reshape(X_tensor.shape[0], -1)
            input_dim = X_tensor.shape[1]

        layers = []
        layers.append(nn.Linear(input_dim, mlp_units))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(mlp_dropout))

        for _ in range(mlp_layers - 1):
            layers.append(nn.Linear(mlp_units, mlp_units))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(mlp_dropout))

        layers.append(nn.Linear(mlp_units, output_dim))
        model = nn.Sequential(*layers)

    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Define the loss function and optimizer
    learning_rate = cfg.get('learning_rate')
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # Training loop
    epochs = 10
    total_loss = 0.0
    model.train()  # Set the model to training mode

    for epoch in range(epochs):
        optimizer.zero_grad()
        if model_type in ['LSTM', 'GRU']:
            outputs = model(X_tensor)
        else:
            outputs = model(X_tensor)
        loss = criterion(outputs, y_tensor.reshape(-1, 1)) # Reshape y_tensor to (batch_size, 1)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    avg_loss = total_loss / epochs
    return avg_loss

--------------------------------------------------------------------------------
[2025-06-07 14:01:30] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#2): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-06-07 14:01:35] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
from typing import Any
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from ConfigSpace import Configuration

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing the training data ('X' and 'y').
        seed (int): Random seed for reproducibility.

    Returns:
        float: The average training loss over 10 epochs.
    """

    torch.manual_seed(seed)
    np.random.seed(seed)

    X = dataset['X']
    y = dataset['y']

    # Convert data to PyTorch tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.float32)

    model_type = cfg.get('model_type')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        input_dim = X_tensor.shape[2]
        seq_len = X_tensor.shape[1]
    elif X_tensor.ndim == 2:
      input_dim = X_tensor.shape[1]
      seq_len = 1 # Assume seq_len is 1 for MLP with 2D input
    else:
        raise ValueError(f"Unsupported input dimension: {X_tensor.ndim}")
    output_dim = 1  # Regression task

    # Define the model
    if model_type == 'LSTM':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for LSTM, got {X_tensor.shape}")

        lstm_units = cfg.get('lstm_units')
        lstm_layers = cfg.get('lstm_layers')
        lstm_dropout = cfg.get('lstm_dropout')
        model = nn.LSTM(input_dim, lstm_units, num_layers=lstm_layers, dropout=lstm_dropout, batch_first=True)
        model = nn.Sequential(
            model,
            nn.Linear(lstm_units, output_dim)
        )

    elif model_type == 'GRU':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for GRU, got {X_tensor.shape}")

        gru_units = cfg.get('gru_units')
        gru_layers = cfg.get('gru_layers')
        gru_dropout = cfg.get('gru_dropout')
        model = nn.GRU(input_dim, gru_units, num_layers=gru_layers, dropout=gru_dropout, batch_first=True)
        model = nn.Sequential(
            model,
            nn.Linear(gru_units, output_dim)
        )

    elif model_type == 'MLP':
        mlp_layers = cfg.get('mlp_layers')
        mlp_units = cfg.get('mlp_units')
        mlp_dropout = cfg.get('mlp_dropout')

        if X_tensor.ndim == 3: # Reshape if input is 3D for time series
            X_tensor = X_tensor.reshape(X_tensor.shape[0], -1)
            input_dim = X_tensor.shape[1]

        layers = []
        layers.append(nn.Linear(input_dim, mlp_units))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(mlp_dropout))

        for _ in range(mlp_layers - 1):
            layers.append(nn.Linear(mlp_units, mlp_units))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(mlp_dropout))

        layers.append(nn.Linear(mlp_units, output_dim))
        model = nn.Sequential(*layers)

    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Define the loss function and optimizer
    learning_rate = cfg.get('learning_rate')
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # Training loop
    epochs = 10
    total_loss = 0.0
    model.train()  # Set the model to training mode

    for epoch in range(epochs):
        optimizer.zero_grad()
        if model_type in ['LSTM', 'GRU']:
            outputs = model(X_tensor)
            outputs = outputs[:, -1, :]
        else:
            outputs = model(X_tensor)
        loss = criterion(outputs, y_tensor.reshape(-1, 1)) # Reshape y_tensor to (batch_size, 1)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    avg_loss = total_loss / epochs
    return avg_loss
```
--------------------------------------------------------------------------------
[2025-06-07 14:01:35] [Metadata: {'component': 'train_function', 'action': 'run'}] from typing import Any
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from ConfigSpace import Configuration

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing the training data ('X' and 'y').
        seed (int): Random seed for reproducibility.

    Returns:
        float: The average training loss over 10 epochs.
    """

    torch.manual_seed(seed)
    np.random.seed(seed)

    X = dataset['X']
    y = dataset['y']

    # Convert data to PyTorch tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.float32)

    model_type = cfg.get('model_type')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        input_dim = X_tensor.shape[2]
        seq_len = X_tensor.shape[1]
    elif X_tensor.ndim == 2:
      input_dim = X_tensor.shape[1]
      seq_len = 1 # Assume seq_len is 1 for MLP with 2D input
    else:
        raise ValueError(f"Unsupported input dimension: {X_tensor.ndim}")
    output_dim = 1  # Regression task

    # Define the model
    if model_type == 'LSTM':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for LSTM, got {X_tensor.shape}")

        lstm_units = cfg.get('lstm_units')
        lstm_layers = cfg.get('lstm_layers')
        lstm_dropout = cfg.get('lstm_dropout')
        model = nn.LSTM(input_dim, lstm_units, num_layers=lstm_layers, dropout=lstm_dropout, batch_first=True)
        model = nn.Sequential(
            model,
            nn.Linear(lstm_units, output_dim)
        )

    elif model_type == 'GRU':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for GRU, got {X_tensor.shape}")

        gru_units = cfg.get('gru_units')
        gru_layers = cfg.get('gru_layers')
        gru_dropout = cfg.get('gru_dropout')
        model = nn.GRU(input_dim, gru_units, num_layers=gru_layers, dropout=gru_dropout, batch_first=True)
        model = nn.Sequential(
            model,
            nn.Linear(gru_units, output_dim)
        )

    elif model_type == 'MLP':
        mlp_layers = cfg.get('mlp_layers')
        mlp_units = cfg.get('mlp_units')
        mlp_dropout = cfg.get('mlp_dropout')

        if X_tensor.ndim == 3: # Reshape if input is 3D for time series
            X_tensor = X_tensor.reshape(X_tensor.shape[0], -1)
            input_dim = X_tensor.shape[1]

        layers = []
        layers.append(nn.Linear(input_dim, mlp_units))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(mlp_dropout))

        for _ in range(mlp_layers - 1):
            layers.append(nn.Linear(mlp_units, mlp_units))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(mlp_dropout))

        layers.append(nn.Linear(mlp_units, output_dim))
        model = nn.Sequential(*layers)

    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Define the loss function and optimizer
    learning_rate = cfg.get('learning_rate')
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # Training loop
    epochs = 10
    total_loss = 0.0
    model.train()  # Set the model to training mode

    for epoch in range(epochs):
        optimizer.zero_grad()
        if model_type in ['LSTM', 'GRU']:
            outputs = model(X_tensor)
            outputs = outputs[:, -1, :]
        else:
            outputs = model(X_tensor)
        loss = criterion(outputs, y_tensor.reshape(-1, 1)) # Reshape y_tensor to (batch_size, 1)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    avg_loss = total_loss / epochs
    return avg_loss

--------------------------------------------------------------------------------
[2025-06-07 14:01:35] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#3): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-06-07 14:01:44] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
from typing import Any
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from ConfigSpace import Configuration

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing the training data ('X' and 'y').
        seed (int): Random seed for reproducibility.

    Returns:
        float: The average training loss over 10 epochs.
    """

    torch.manual_seed(seed)
    np.random.seed(seed)

    X = dataset['X']
    y = dataset['y']

    # Convert data to PyTorch tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.float32)

    model_type = cfg.get('model_type')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        input_dim = X_tensor.shape[2]
        seq_len = X_tensor.shape[1]
    elif X_tensor.ndim == 2:
      input_dim = X_tensor.shape[1]
      seq_len = 1 # Assume seq_len is 1 for MLP with 2D input
    else:
        raise ValueError(f"Unsupported input dimension: {X_tensor.ndim}")
    output_dim = 1  # Regression task

    # Define the model
    if model_type == 'LSTM':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for LSTM, got {X_tensor.shape}")

        lstm_units = cfg.get('lstm_units')
        lstm_layers = cfg.get('lstm_layers')
        lstm_dropout = cfg.get('lstm_dropout')
        model = nn.LSTM(input_dim, lstm_units, num_layers=lstm_layers, dropout=lstm_dropout, batch_first=True)
        model = nn.Sequential(
            model,
            nn.Linear(lstm_units, output_dim)
        )

    elif model_type == 'GRU':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for GRU, got {X_tensor.shape}")

        gru_units = cfg.get('gru_units')
        gru_layers = cfg.get('gru_layers')
        gru_dropout = cfg.get('gru_dropout')
        model = nn.GRU(input_dim, gru_units, num_layers=gru_layers, dropout=gru_dropout, batch_first=True)
        model = nn.Sequential(
            model,
            nn.Linear(gru_units, output_dim)
        )

    elif model_type == 'MLP':
        mlp_layers = cfg.get('mlp_layers')
        mlp_units = cfg.get('mlp_units')
        mlp_dropout = cfg.get('mlp_dropout')

        if X_tensor.ndim == 3: # Reshape if input is 3D for time series
            X_tensor = X_tensor.reshape(X_tensor.shape[0], -1)
            input_dim = X_tensor.shape[1]

        layers = []
        layers.append(nn.Linear(input_dim, mlp_units))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(mlp_dropout))

        for _ in range(mlp_layers - 1):
            layers.append(nn.Linear(mlp_units, mlp_units))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(mlp_dropout))

        layers.append(nn.Linear(mlp_units, output_dim))
        model = nn.Sequential(*layers)

    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Define the loss function and optimizer
    learning_rate = cfg.get('learning_rate')
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # Training loop
    epochs = 10
    total_loss = 0.0
    model.train()  # Set the model to training mode

    for epoch in range(epochs):
        optimizer.zero_grad()
        if model_type in ['LSTM', 'GRU']:
            outputs = model(X_tensor)
            outputs = outputs[:, -1, :]
        else:
            outputs = model(X_tensor)
        loss = criterion(outputs, y_tensor.reshape(-1, 1)) # Reshape y_tensor to (batch_size, 1)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    avg_loss = total_loss / epochs
    return avg_loss
```
--------------------------------------------------------------------------------
[2025-06-07 14:01:44] [Metadata: {'component': 'train_function', 'action': 'run'}] from typing import Any
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from ConfigSpace import Configuration

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing the training data ('X' and 'y').
        seed (int): Random seed for reproducibility.

    Returns:
        float: The average training loss over 10 epochs.
    """

    torch.manual_seed(seed)
    np.random.seed(seed)

    X = dataset['X']
    y = dataset['y']

    # Convert data to PyTorch tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.float32)

    model_type = cfg.get('model_type')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        input_dim = X_tensor.shape[2]
        seq_len = X_tensor.shape[1]
    elif X_tensor.ndim == 2:
      input_dim = X_tensor.shape[1]
      seq_len = 1 # Assume seq_len is 1 for MLP with 2D input
    else:
        raise ValueError(f"Unsupported input dimension: {X_tensor.ndim}")
    output_dim = 1  # Regression task

    # Define the model
    if model_type == 'LSTM':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for LSTM, got {X_tensor.shape}")

        lstm_units = cfg.get('lstm_units')
        lstm_layers = cfg.get('lstm_layers')
        lstm_dropout = cfg.get('lstm_dropout')
        model = nn.LSTM(input_dim, lstm_units, num_layers=lstm_layers, dropout=lstm_dropout, batch_first=True)
        model = nn.Sequential(
            model,
            nn.Linear(lstm_units, output_dim)
        )

    elif model_type == 'GRU':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for GRU, got {X_tensor.shape}")

        gru_units = cfg.get('gru_units')
        gru_layers = cfg.get('gru_layers')
        gru_dropout = cfg.get('gru_dropout')
        model = nn.GRU(input_dim, gru_units, num_layers=gru_layers, dropout=gru_dropout, batch_first=True)
        model = nn.Sequential(
            model,
            nn.Linear(gru_units, output_dim)
        )

    elif model_type == 'MLP':
        mlp_layers = cfg.get('mlp_layers')
        mlp_units = cfg.get('mlp_units')
        mlp_dropout = cfg.get('mlp_dropout')

        if X_tensor.ndim == 3: # Reshape if input is 3D for time series
            X_tensor = X_tensor.reshape(X_tensor.shape[0], -1)
            input_dim = X_tensor.shape[1]

        layers = []
        layers.append(nn.Linear(input_dim, mlp_units))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(mlp_dropout))

        for _ in range(mlp_layers - 1):
            layers.append(nn.Linear(mlp_units, mlp_units))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(mlp_dropout))

        layers.append(nn.Linear(mlp_units, output_dim))
        model = nn.Sequential(*layers)

    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Define the loss function and optimizer
    learning_rate = cfg.get('learning_rate')
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # Training loop
    epochs = 10
    total_loss = 0.0
    model.train()  # Set the model to training mode

    for epoch in range(epochs):
        optimizer.zero_grad()
        if model_type in ['LSTM', 'GRU']:
            outputs = model(X_tensor)
            outputs = outputs[:, -1, :]
        else:
            outputs = model(X_tensor)
        loss = criterion(outputs, y_tensor.reshape(-1, 1)) # Reshape y_tensor to (batch_size, 1)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    avg_loss = total_loss / epochs
    return avg_loss

--------------------------------------------------------------------------------
[2025-06-07 14:01:44] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#4): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-06-07 14:01:50] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
from typing import Any
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from ConfigSpace import Configuration

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing the training data ('X' and 'y').
        seed (int): Random seed for reproducibility.

    Returns:
        float: The average training loss over 10 epochs.
    """

    torch.manual_seed(seed)
    np.random.seed(seed)

    X = dataset['X']
    y = dataset['y']

    # Convert data to PyTorch tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.float32)

    model_type = cfg.get('model_type')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        input_dim = X_tensor.shape[2]
        seq_len = X_tensor.shape[1]
    elif X_tensor.ndim == 2:
      input_dim = X_tensor.shape[1]
      seq_len = 1 # Assume seq_len is 1 for MLP with 2D input
    else:
        raise ValueError(f"Unsupported input dimension: {X_tensor.ndim}")
    output_dim = 1  # Regression task

    # Define the model
    if model_type == 'LSTM':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for LSTM, got {X_tensor.shape}")

        lstm_units = cfg.get('lstm_units')
        lstm_layers = cfg.get('lstm_layers')
        lstm_dropout = cfg.get('lstm_dropout')
        model = nn.LSTM(input_dim, lstm_units, num_layers=lstm_layers, dropout=lstm_dropout, batch_first=True)
        model = nn.Sequential(
            model,
            nn.Linear(lstm_units, output_dim)
        )

    elif model_type == 'GRU':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for GRU, got {X_tensor.shape}")

        gru_units = cfg.get('gru_units')
        gru_layers = cfg.get('gru_layers')
        gru_dropout = cfg.get('gru_dropout')
        model = nn.GRU(input_dim, gru_units, num_layers=gru_layers, dropout=gru_dropout, batch_first=True)
        model = nn.Sequential(
            model,
            nn.Linear(gru_units, output_dim)
        )

    elif model_type == 'MLP':
        mlp_layers = cfg.get('mlp_layers')
        mlp_units = cfg.get('mlp_units')
        mlp_dropout = cfg.get('mlp_dropout')

        if X_tensor.ndim == 3: # Reshape if input is 3D for time series
            X_tensor = X_tensor.reshape(X_tensor.shape[0], -1)
            input_dim = X_tensor.shape[1]

        layers = []
        layers.append(nn.Linear(input_dim, mlp_units))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(mlp_dropout))

        for _ in range(mlp_layers - 1):
            layers.append(nn.Linear(mlp_units, mlp_units))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(mlp_dropout))

        layers.append(nn.Linear(mlp_units, output_dim))
        model = nn.Sequential(*layers)

    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Define the loss function and optimizer
    learning_rate = cfg.get('learning_rate')
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # Training loop
    epochs = 10
    total_loss = 0.0
    model.train()  # Set the model to training mode

    for epoch in range(epochs):
        optimizer.zero_grad()
        if model_type in ['LSTM', 'GRU']:
            outputs = model(X_tensor)
            outputs = outputs[:, -1, :]
        else:
            outputs = model(X_tensor)
        loss = criterion(outputs, y_tensor.reshape(-1, 1)) # Reshape y_tensor to (batch_size, 1)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    avg_loss = total_loss / epochs
    return avg_loss
```
--------------------------------------------------------------------------------
[2025-06-07 14:01:50] [Metadata: {'component': 'train_function', 'action': 'run'}] from typing import Any
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from ConfigSpace import Configuration

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing the training data ('X' and 'y').
        seed (int): Random seed for reproducibility.

    Returns:
        float: The average training loss over 10 epochs.
    """

    torch.manual_seed(seed)
    np.random.seed(seed)

    X = dataset['X']
    y = dataset['y']

    # Convert data to PyTorch tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.float32)

    model_type = cfg.get('model_type')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        input_dim = X_tensor.shape[2]
        seq_len = X_tensor.shape[1]
    elif X_tensor.ndim == 2:
      input_dim = X_tensor.shape[1]
      seq_len = 1 # Assume seq_len is 1 for MLP with 2D input
    else:
        raise ValueError(f"Unsupported input dimension: {X_tensor.ndim}")
    output_dim = 1  # Regression task

    # Define the model
    if model_type == 'LSTM':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for LSTM, got {X_tensor.shape}")

        lstm_units = cfg.get('lstm_units')
        lstm_layers = cfg.get('lstm_layers')
        lstm_dropout = cfg.get('lstm_dropout')
        model = nn.LSTM(input_dim, lstm_units, num_layers=lstm_layers, dropout=lstm_dropout, batch_first=True)
        model = nn.Sequential(
            model,
            nn.Linear(lstm_units, output_dim)
        )

    elif model_type == 'GRU':
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for GRU, got {X_tensor.shape}")

        gru_units = cfg.get('gru_units')
        gru_layers = cfg.get('gru_layers')
        gru_dropout = cfg.get('gru_dropout')
        model = nn.GRU(input_dim, gru_units, num_layers=gru_layers, dropout=gru_dropout, batch_first=True)
        model = nn.Sequential(
            model,
            nn.Linear(gru_units, output_dim)
        )

    elif model_type == 'MLP':
        mlp_layers = cfg.get('mlp_layers')
        mlp_units = cfg.get('mlp_units')
        mlp_dropout = cfg.get('mlp_dropout')

        if X_tensor.ndim == 3: # Reshape if input is 3D for time series
            X_tensor = X_tensor.reshape(X_tensor.shape[0], -1)
            input_dim = X_tensor.shape[1]

        layers = []
        layers.append(nn.Linear(input_dim, mlp_units))
        layers.append(nn.ReLU())
        layers.append(nn.Dropout(mlp_dropout))

        for _ in range(mlp_layers - 1):
            layers.append(nn.Linear(mlp_units, mlp_units))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(mlp_dropout))

        layers.append(nn.Linear(mlp_units, output_dim))
        model = nn.Sequential(*layers)

    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Define the loss function and optimizer
    learning_rate = cfg.get('learning_rate')
    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # Training loop
    epochs = 10
    total_loss = 0.0
    model.train()  # Set the model to training mode

    for epoch in range(epochs):
        optimizer.zero_grad()
        if model_type in ['LSTM', 'GRU']:
            outputs = model(X_tensor)
            outputs = outputs[:, -1, :]
        else:
            outputs = model(X_tensor)
        loss = criterion(outputs, y_tensor.reshape(-1, 1)) # Reshape y_tensor to (batch_size, 1)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    avg_loss = total_loss / epochs
    return avg_loss

--------------------------------------------------------------------------------
[2025-06-07 14:01:50] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#5): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-06-07 14:01:58] [Metadata: {'component': 'train_function', 'action': 'run'}] from typing import Any
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from ConfigSpace import Configuration

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dataset dictionary with 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: Average training loss over 10 epochs.
    """
    X = dataset['X']
    y = dataset['y']

    # Convert data to numpy arrays if it is a pandas DataFrame
    if isinstance(X, pd.DataFrame):
        X = X.values
    if isinstance(y, pd.Series):
        y = y.values

    model_type = cfg.get("model_type")
    learning_rate = cfg.get("learning_rate")

    # PyTorch-related setup
    if model_type in ['LSTM', 'GRU', 'MLP']:
        X_tensor = torch.tensor(X, dtype=torch.float32)
        y_tensor = torch.tensor(y, dtype=torch.float32)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")
    
    if X_tensor.ndim == 1:
        X_tensor = X_tensor.unsqueeze(1)
    if y_tensor.ndim == 1:
        y_tensor = y_tensor.unsqueeze(1)
    
    if model_type in ['LSTM', 'GRU'] and X_tensor.ndim != 3:
        raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")

    if model_type == 'LSTM':
        input_size = X_tensor.shape[2] if X_tensor.ndim == 3 else X_tensor.shape[1]
        hidden_size = cfg.get("lstm_units")
        num_layers = cfg.get("lstm_layers")
        dropout = cfg.get("lstm_dropout")
        output_size = 1  # Regression task
        
        class LSTMModel(nn.Module):
            def __init__(self, input_size, hidden_size, num_layers, output_size, dropout):
                super(LSTMModel, self).__init__()
                self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
                self.linear = nn.Linear(hidden_size, output_size)

            def forward(self, x):
                out, _ = self.lstm(x)
                out = self.linear(out[:, -1, :]) # Use only the last time step's output
                return out

        model = LSTMModel(input_size, hidden_size, num_layers, output_size, dropout)

    elif model_type == 'GRU':
        input_size = X_tensor.shape[2] if X_tensor.ndim == 3 else X_tensor.shape[1]
        hidden_size = cfg.get("gru_units")
        num_layers = cfg.get("gru_layers")
        dropout = cfg.get("gru_dropout")
        output_size = 1  # Regression task
        
        class GRUModel(nn.Module):
            def __init__(self, input_size, hidden_size, num_layers, output_size, dropout):
                super(GRUModel, self).__init__()
                self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
                self.linear = nn.Linear(hidden_size, output_size)

            def forward(self, x):
                out, _ = self.gru(x)
                out = self.linear(out[:, -1, :]) # Use only the last time step's output
                return out

        model = GRUModel(input_size, hidden_size, num_layers, output_size, dropout)

    elif model_type == 'MLP':
        input_size = X_tensor.shape[1] if X_tensor.ndim == 2 else X_tensor.shape[1] * X_tensor.shape[2]
        hidden_size = cfg.get("mlp_units")
        num_layers = cfg.get("mlp_layers")
        dropout = cfg.get("mlp_dropout")
        output_size = 1  # Regression task

        class MLPModel(nn.Module):
            def __init__(self, input_size, hidden_size, num_layers, output_size, dropout):
                super(MLPModel, self).__init__()
                self.flatten = nn.Flatten()
                layers = []
                for i in range(num_layers - 1):
                    layers.append(nn.Linear(input_size if i == 0 else hidden_size, hidden_size))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(dropout))
                layers.append(nn.Linear(input_size if num_layers == 1 else hidden_size, output_size))
                self.layers = nn.Sequential(*layers)

            def forward(self, x):
                x = self.flatten(x)
                return self.layers(x)

        model = MLPModel(input_size, hidden_size, num_layers, output_size, dropout)

    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # Training loop
    model.train()
    epochs = 10
    total_loss = 0.0
    for epoch in range(epochs):
        optimizer.zero_grad()
        outputs = model(X_tensor)
        loss = criterion(outputs, y_tensor)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    avg_loss = total_loss / epochs
    return float(avg_loss)

--------------------------------------------------------------------------------
[2025-06-07 14:01:58] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#1): Expected 3D input (batch, seq_len, features) for LSTM, got torch.Size([289, 1])
--------------------------------------------------------------------------------
[2025-06-07 14:02:06] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
from typing import Any
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from ConfigSpace import Configuration

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dataset dictionary with 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: Average training loss over 10 epochs.
    """
    X = dataset['X']
    y = dataset['y']

    # Convert data to numpy arrays if it is a pandas DataFrame
    if isinstance(X, pd.DataFrame):
        X = X.values
    if isinstance(y, pd.Series):
        y = y.values

    model_type = cfg.get("model_type")
    learning_rate = cfg.get("learning_rate")

    # PyTorch-related setup
    if model_type in ['LSTM', 'GRU', 'MLP']:
        X_tensor = torch.tensor(X, dtype=torch.float32)
        y_tensor = torch.tensor(y, dtype=torch.float32)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")
    
    if X_tensor.ndim == 1:
        X_tensor = X_tensor.unsqueeze(1)
    if y_tensor.ndim == 1:
        y_tensor = y_tensor.unsqueeze(1)
    
    if model_type in ['LSTM', 'GRU']:
        # Reshape X_tensor to 3D if it's not already
        if X_tensor.ndim == 2:
            X_tensor = X_tensor.unsqueeze(0)  # Add a batch dimension
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")

    if model_type == 'LSTM':
        input_size = X_tensor.shape[2] if X_tensor.ndim == 3 else X_tensor.shape[1]
        hidden_size = cfg.get("lstm_units")
        num_layers = cfg.get("lstm_layers")
        dropout = cfg.get("lstm_dropout")
        output_size = 1  # Regression task
        
        class LSTMModel(nn.Module):
            def __init__(self, input_size, hidden_size, num_layers, output_size, dropout):
                super(LSTMModel, self).__init__()
                self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
                self.linear = nn.Linear(hidden_size, output_size)

            def forward(self, x):
                out, _ = self.lstm(x)
                out = self.linear(out[:, -1, :]) # Use only the last time step's output
                return out

        model = LSTMModel(input_size, hidden_size, num_layers, output_size, dropout)

    elif model_type == 'GRU':
        input_size = X_tensor.shape[2] if X_tensor.ndim == 3 else X_tensor.shape[1]
        hidden_size = cfg.get("gru_units")
        num_layers = cfg.get("gru_layers")
        dropout = cfg.get("gru_dropout")
        output_size = 1  # Regression task
        
        class GRUModel(nn.Module):
            def __init__(self, input_size, hidden_size, num_layers, output_size, dropout):
                super(GRUModel, self).__init__()
                self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
                self.linear = nn.Linear(hidden_size, output_size)

            def forward(self, x):
                out, _ = self.gru(x)
                out = self.linear(out[:, -1, :]) # Use only the last time step's output
                return out

        model = GRUModel(input_size, hidden_size, num_layers, output_size, dropout)

    elif model_type == 'MLP':
        input_size = X_tensor.shape[1] if X_tensor.ndim == 2 else X_tensor.shape[1] * X_tensor.shape[2]
        hidden_size = cfg.get("mlp_units")
        num_layers = cfg.get("mlp_layers")
        dropout = cfg.get("mlp_dropout")
        output_size = 1  # Regression task

        class MLPModel(nn.Module):
            def __init__(self, input_size, hidden_size, num_layers, output_size, dropout):
                super(MLPModel, self).__init__()
                self.flatten = nn.Flatten()
                layers = []
                for i in range(num_layers - 1):
                    layers.append(nn.Linear(input_size if i == 0 else hidden_size, hidden_size))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(dropout))
                layers.append(nn.Linear(input_size if num_layers == 1 else hidden_size, output_size))
                self.layers = nn.Sequential(*layers)

            def forward(self, x):
                x = self.flatten(x)
                return self.layers(x)

        model = MLPModel(input_size, hidden_size, num_layers, output_size, dropout)

    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # Training loop
    model.train()
    epochs = 10
    total_loss = 0.0
    for epoch in range(epochs):
        optimizer.zero_grad()
        outputs = model(X_tensor)
        loss = criterion(outputs, y_tensor)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    avg_loss = total_loss / epochs
    return float(avg_loss)
```
--------------------------------------------------------------------------------
[2025-06-07 14:02:06] [Metadata: {'component': 'train_function', 'action': 'run'}] from typing import Any
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.neural_network import MLPRegressor
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from ConfigSpace import Configuration

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dataset dictionary with 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: Average training loss over 10 epochs.
    """
    X = dataset['X']
    y = dataset['y']

    # Convert data to numpy arrays if it is a pandas DataFrame
    if isinstance(X, pd.DataFrame):
        X = X.values
    if isinstance(y, pd.Series):
        y = y.values

    model_type = cfg.get("model_type")
    learning_rate = cfg.get("learning_rate")

    # PyTorch-related setup
    if model_type in ['LSTM', 'GRU', 'MLP']:
        X_tensor = torch.tensor(X, dtype=torch.float32)
        y_tensor = torch.tensor(y, dtype=torch.float32)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")
    
    if X_tensor.ndim == 1:
        X_tensor = X_tensor.unsqueeze(1)
    if y_tensor.ndim == 1:
        y_tensor = y_tensor.unsqueeze(1)
    
    if model_type in ['LSTM', 'GRU']:
        # Reshape X_tensor to 3D if it's not already
        if X_tensor.ndim == 2:
            X_tensor = X_tensor.unsqueeze(0)  # Add a batch dimension
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")

    if model_type == 'LSTM':
        input_size = X_tensor.shape[2] if X_tensor.ndim == 3 else X_tensor.shape[1]
        hidden_size = cfg.get("lstm_units")
        num_layers = cfg.get("lstm_layers")
        dropout = cfg.get("lstm_dropout")
        output_size = 1  # Regression task
        
        class LSTMModel(nn.Module):
            def __init__(self, input_size, hidden_size, num_layers, output_size, dropout):
                super(LSTMModel, self).__init__()
                self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
                self.linear = nn.Linear(hidden_size, output_size)

            def forward(self, x):
                out, _ = self.lstm(x)
                out = self.linear(out[:, -1, :]) # Use only the last time step's output
                return out

        model = LSTMModel(input_size, hidden_size, num_layers, output_size, dropout)

    elif model_type == 'GRU':
        input_size = X_tensor.shape[2] if X_tensor.ndim == 3 else X_tensor.shape[1]
        hidden_size = cfg.get("gru_units")
        num_layers = cfg.get("gru_layers")
        dropout = cfg.get("gru_dropout")
        output_size = 1  # Regression task
        
        class GRUModel(nn.Module):
            def __init__(self, input_size, hidden_size, num_layers, output_size, dropout):
                super(GRUModel, self).__init__()
                self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
                self.linear = nn.Linear(hidden_size, output_size)

            def forward(self, x):
                out, _ = self.gru(x)
                out = self.linear(out[:, -1, :]) # Use only the last time step's output
                return out

        model = GRUModel(input_size, hidden_size, num_layers, output_size, dropout)

    elif model_type == 'MLP':
        input_size = X_tensor.shape[1] if X_tensor.ndim == 2 else X_tensor.shape[1] * X_tensor.shape[2]
        hidden_size = cfg.get("mlp_units")
        num_layers = cfg.get("mlp_layers")
        dropout = cfg.get("mlp_dropout")
        output_size = 1  # Regression task

        class MLPModel(nn.Module):
            def __init__(self, input_size, hidden_size, num_layers, output_size, dropout):
                super(MLPModel, self).__init__()
                self.flatten = nn.Flatten()
                layers = []
                for i in range(num_layers - 1):
                    layers.append(nn.Linear(input_size if i == 0 else hidden_size, hidden_size))
                    layers.append(nn.ReLU())
                    layers.append(nn.Dropout(dropout))
                layers.append(nn.Linear(input_size if num_layers == 1 else hidden_size, output_size))
                self.layers = nn.Sequential(*layers)

            def forward(self, x):
                x = self.flatten(x)
                return self.layers(x)

        model = MLPModel(input_size, hidden_size, num_layers, output_size, dropout)

    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    criterion = nn.MSELoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # Training loop
    model.train()
    epochs = 10
    total_loss = 0.0
    for epoch in range(epochs):
        optimizer.zero_grad()
        outputs = model(X_tensor)
        loss = criterion(outputs, y_tensor)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()

    avg_loss = total_loss / epochs
    return float(avg_loss)

--------------------------------------------------------------------------------
[2025-06-07 14:02:06] [Metadata: {'component': 'train_function', 'status': 'success', 'loss': 3232.3700927734376}] Training executed successfully, loss: 3232.3700927734376
--------------------------------------------------------------------------------
