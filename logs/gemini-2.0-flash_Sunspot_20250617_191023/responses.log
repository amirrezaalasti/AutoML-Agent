[2025-06-17 19:10:39] [Metadata: {'component': 'config'}] ```python
from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, Condition, EqualsCondition
from ConfigSpace.hyperparameters import UnParametrizedHyperparameter

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization of time series models.

    The configuration space includes hyperparameters for model selection (LSTM, GRU, RNN, Linear Regression, or Naive),
    number of layers, number of units per layer, dropout rate, learning rate, batch size,
    and optimization parameters.  Appropriate conditions are added between dependent hyperparameters.
    """
    cs = ConfigurationSpace()

    # Model Type Selection
    model_type = CategoricalHyperparameter(
        "model_type", choices=["LSTM", "GRU", "RNN", "LinearRegression", "Naive"], default_value="LSTM"
    )
    cs.add_hyperparameter(model_type)

    # LSTM/GRU/RNN Specific Hyperparameters
    num_layers = UniformIntegerHyperparameter(
        "num_layers", lower=1, upper=3, default_value=2, q=1
    )
    num_units = UniformIntegerHyperparameter(
        "num_units", lower=32, upper=256, default_value=64, q=32
    )
    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate", lower=0.0, upper=0.5, default_value=0.2
    )
    cs.add_hyperparameters([num_layers, num_units, dropout_rate])

    # Linear Regression / Naive Baseline does not need layers, units, or dropout
    condition_num_layers = Condition(num_layers, "model_type", ["LSTM", "GRU", "RNN"])
    condition_num_units = Condition(num_units, "model_type", ["LSTM", "GRU", "RNN"])
    condition_dropout_rate = Condition(dropout_rate, "model_type", ["LSTM", "GRU", "RNN"])
    cs.add_conditions([condition_num_layers, condition_num_units, condition_dropout_rate])

    # Shared Hyperparameters
    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-4, upper=1e-2, default_value=1e-3, log=True
    )
    batch_size = CategoricalHyperparameter(
        "batch_size", choices=[32, 64, 128, 256], default_value=64
    )
    optimizer = CategoricalHyperparameter(
        "optimizer", choices=["Adam", "SGD"], default_value="Adam"
    )
    cs.add_hyperparameters([learning_rate, batch_size, optimizer])

    # Optimizer-specific hyperparameters
    beta_1 = UniformFloatHyperparameter(
        "beta_1", lower=0.8, upper=0.99, default_value=0.9, log=False
    )
    momentum = UniformFloatHyperparameter(
        "momentum", lower=0.0, upper=0.9, default_value=0.0, log=False
    )
    cs.add_hyperparameters([beta_1, momentum])

    # Condition beta_1 on Adam
    condition_beta_1 = Condition(beta_1, "optimizer", ["Adam"])
    cs.add_condition(condition_beta_1)

    # Condition momentum on SGD
    condition_momentum = Condition(momentum, "optimizer", ["SGD"])
    cs.add_condition(condition_momentum)
        
    # Add preprocessing boolean options for scaling, detrending, seasonality
    scaler = CategoricalHyperparameter(
        "scaler", choices=["MinMaxScaler", "StandardScaler", "None"], default_value="StandardScaler"
    )
    detrend = CategoricalHyperparameter(
        "detrend", choices=["Differencing", "Polynomial", "None"], default_value="None"
    )
    seasonality_decomp = CategoricalHyperparameter(
        "seasonality_decomp", choices=["STL", "ETS", "None"], default_value="None"
    )
    cs.add_hyperparameters([scaler, detrend, seasonality_decomp])


    return cs
```
--------------------------------------------------------------------------------
[2025-06-17 19:10:39] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, Condition, EqualsCondition
from ConfigSpace.hyperparameters import UnParametrizedHyperparameter

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization of time series models.

    The configuration space includes hyperparameters for model selection (LSTM, GRU, RNN, Linear Regression, or Naive),
    number of layers, number of units per layer, dropout rate, learning rate, batch size,
    and optimization parameters.  Appropriate conditions are added between dependent hyperparameters.
    """
    cs = ConfigurationSpace()

    # Model Type Selection
    model_type = CategoricalHyperparameter(
        "model_type", choices=["LSTM", "GRU", "RNN", "LinearRegression", "Naive"], default_value="LSTM"
    )
    cs.add_hyperparameter(model_type)

    # LSTM/GRU/RNN Specific Hyperparameters
    num_layers = UniformIntegerHyperparameter(
        "num_layers", lower=1, upper=3, default_value=2, q=1
    )
    num_units = UniformIntegerHyperparameter(
        "num_units", lower=32, upper=256, default_value=64, q=32
    )
    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate", lower=0.0, upper=0.5, default_value=0.2
    )
    cs.add_hyperparameters([num_layers, num_units, dropout_rate])

    # Linear Regression / Naive Baseline does not need layers, units, or dropout
    condition_num_layers = Condition(num_layers, "model_type", ["LSTM", "GRU", "RNN"])
    condition_num_units = Condition(num_units, "model_type", ["LSTM", "GRU", "RNN"])
    condition_dropout_rate = Condition(dropout_rate, "model_type", ["LSTM", "GRU", "RNN"])
    cs.add_conditions([condition_num_layers, condition_num_units, condition_dropout_rate])

    # Shared Hyperparameters
    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-4, upper=1e-2, default_value=1e-3, log=True
    )
    batch_size = CategoricalHyperparameter(
        "batch_size", choices=[32, 64, 128, 256], default_value=64
    )
    optimizer = CategoricalHyperparameter(
        "optimizer", choices=["Adam", "SGD"], default_value="Adam"
    )
    cs.add_hyperparameters([learning_rate, batch_size, optimizer])

    # Optimizer-specific hyperparameters
    beta_1 = UniformFloatHyperparameter(
        "beta_1", lower=0.8, upper=0.99, default_value=0.9, log=False
    )
    momentum = UniformFloatHyperparameter(
        "momentum", lower=0.0, upper=0.9, default_value=0.0, log=False
    )
    cs.add_hyperparameters([beta_1, momentum])

    # Condition beta_1 on Adam
    condition_beta_1 = Condition(beta_1, "optimizer", ["Adam"])
    cs.add_condition(condition_beta_1)

    # Condition momentum on SGD
    condition_momentum = Condition(momentum, "optimizer", ["SGD"])
    cs.add_condition(condition_momentum)
        
    # Add preprocessing boolean options for scaling, detrending, seasonality
    scaler = CategoricalHyperparameter(
        "scaler", choices=["MinMaxScaler", "StandardScaler", "None"], default_value="StandardScaler"
    )
    detrend = CategoricalHyperparameter(
        "detrend", choices=["Differencing", "Polynomial", "None"], default_value="None"
    )
    seasonality_decomp = CategoricalHyperparameter(
        "seasonality_decomp", choices=["STL", "ETS", "None"], default_value="None"
    )
    cs.add_hyperparameters([scaler, detrend, seasonality_decomp])


    return cs

--------------------------------------------------------------------------------
[2025-06-17 19:10:39] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#1): cannot import name 'Condition' from 'ConfigSpace' (/opt/anaconda3/envs/SMAC/lib/python3.10/site-packages/ConfigSpace/__init__.py)
--------------------------------------------------------------------------------
[2025-06-17 19:10:45] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter
from ConfigSpace.conditions import Condition, EqualsCondition
from ConfigSpace.hyperparameters import UnParametrizedHyperparameter

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization of time series models.

    The configuration space includes hyperparameters for model selection (LSTM, GRU, RNN, Linear Regression, or Naive),
    number of layers, number of units per layer, dropout rate, learning rate, batch size,
    and optimization parameters.  Appropriate conditions are added between dependent hyperparameters.
    """
    cs = ConfigurationSpace()

    # Model Type Selection
    model_type = CategoricalHyperparameter(
        "model_type", choices=["LSTM", "GRU", "RNN", "LinearRegression", "Naive"], default_value="LSTM"
    )
    cs.add_hyperparameter(model_type)

    # LSTM/GRU/RNN Specific Hyperparameters
    num_layers = UniformIntegerHyperparameter(
        "num_layers", lower=1, upper=3, default_value=2, q=1
    )
    num_units = UniformIntegerHyperparameter(
        "num_units", lower=32, upper=256, default_value=64, q=32
    )
    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate", lower=0.0, upper=0.5, default_value=0.2
    )
    cs.add_hyperparameters([num_layers, num_units, dropout_rate])

    # Linear Regression / Naive Baseline does not need layers, units, or dropout
    condition_num_layers = Condition(num_layers, child_values=["LSTM", "GRU", "RNN"], parent=model_type)
    condition_num_units = Condition(num_units, child_values=["LSTM", "GRU", "RNN"], parent=model_type)
    condition_dropout_rate = Condition(dropout_rate, child_values=["LSTM", "GRU", "RNN"], parent=model_type)
    cs.add_conditions([condition_num_layers, condition_num_units, condition_dropout_rate])

    # Shared Hyperparameters
    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-4, upper=1e-2, default_value=1e-3, log=True
    )
    batch_size = CategoricalHyperparameter(
        "batch_size", choices=[32, 64, 128, 256], default_value=64
    )
    optimizer = CategoricalHyperparameter(
        "optimizer", choices=["Adam", "SGD"], default_value="Adam"
    )
    cs.add_hyperparameters([learning_rate, batch_size, optimizer])

    # Optimizer-specific hyperparameters
    beta_1 = UniformFloatHyperparameter(
        "beta_1", lower=0.8, upper=0.99, default_value=0.9, log=False
    )
    momentum = UniformFloatHyperparameter(
        "momentum", lower=0.0, upper=0.9, default_value=0.0, log=False
    )
    cs.add_hyperparameters([beta_1, momentum])

    # Condition beta_1 on Adam
    condition_beta_1 = Condition(beta_1, child_values=["Adam"], parent=optimizer)
    cs.add_condition(condition_beta_1)

    # Condition momentum on SGD
    condition_momentum = Condition(momentum, child_values=["SGD"], parent=optimizer)
    cs.add_condition(condition_momentum)
        
    # Add preprocessing boolean options for scaling, detrending, seasonality
    scaler = CategoricalHyperparameter(
        "scaler", choices=["MinMaxScaler", "StandardScaler", "None"], default_value="StandardScaler"
    )
    detrend = CategoricalHyperparameter(
        "detrend", choices=["Differencing", "Polynomial", "None"], default_value="None"
    )
    seasonality_decomp = CategoricalHyperparameter(
        "seasonality_decomp", choices=["STL", "ETS", "None"], default_value="None"
    )
    cs.add_hyperparameters([scaler, detrend, seasonality_decomp])


    return cs
```
--------------------------------------------------------------------------------
[2025-06-17 19:10:45] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter
from ConfigSpace.conditions import Condition, EqualsCondition
from ConfigSpace.hyperparameters import UnParametrizedHyperparameter

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization of time series models.

    The configuration space includes hyperparameters for model selection (LSTM, GRU, RNN, Linear Regression, or Naive),
    number of layers, number of units per layer, dropout rate, learning rate, batch size,
    and optimization parameters.  Appropriate conditions are added between dependent hyperparameters.
    """
    cs = ConfigurationSpace()

    # Model Type Selection
    model_type = CategoricalHyperparameter(
        "model_type", choices=["LSTM", "GRU", "RNN", "LinearRegression", "Naive"], default_value="LSTM"
    )
    cs.add_hyperparameter(model_type)

    # LSTM/GRU/RNN Specific Hyperparameters
    num_layers = UniformIntegerHyperparameter(
        "num_layers", lower=1, upper=3, default_value=2, q=1
    )
    num_units = UniformIntegerHyperparameter(
        "num_units", lower=32, upper=256, default_value=64, q=32
    )
    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate", lower=0.0, upper=0.5, default_value=0.2
    )
    cs.add_hyperparameters([num_layers, num_units, dropout_rate])

    # Linear Regression / Naive Baseline does not need layers, units, or dropout
    condition_num_layers = Condition(num_layers, child_values=["LSTM", "GRU", "RNN"], parent=model_type)
    condition_num_units = Condition(num_units, child_values=["LSTM", "GRU", "RNN"], parent=model_type)
    condition_dropout_rate = Condition(dropout_rate, child_values=["LSTM", "GRU", "RNN"], parent=model_type)
    cs.add_conditions([condition_num_layers, condition_num_units, condition_dropout_rate])

    # Shared Hyperparameters
    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-4, upper=1e-2, default_value=1e-3, log=True
    )
    batch_size = CategoricalHyperparameter(
        "batch_size", choices=[32, 64, 128, 256], default_value=64
    )
    optimizer = CategoricalHyperparameter(
        "optimizer", choices=["Adam", "SGD"], default_value="Adam"
    )
    cs.add_hyperparameters([learning_rate, batch_size, optimizer])

    # Optimizer-specific hyperparameters
    beta_1 = UniformFloatHyperparameter(
        "beta_1", lower=0.8, upper=0.99, default_value=0.9, log=False
    )
    momentum = UniformFloatHyperparameter(
        "momentum", lower=0.0, upper=0.9, default_value=0.0, log=False
    )
    cs.add_hyperparameters([beta_1, momentum])

    # Condition beta_1 on Adam
    condition_beta_1 = Condition(beta_1, child_values=["Adam"], parent=optimizer)
    cs.add_condition(condition_beta_1)

    # Condition momentum on SGD
    condition_momentum = Condition(momentum, child_values=["SGD"], parent=optimizer)
    cs.add_condition(condition_momentum)
        
    # Add preprocessing boolean options for scaling, detrending, seasonality
    scaler = CategoricalHyperparameter(
        "scaler", choices=["MinMaxScaler", "StandardScaler", "None"], default_value="StandardScaler"
    )
    detrend = CategoricalHyperparameter(
        "detrend", choices=["Differencing", "Polynomial", "None"], default_value="None"
    )
    seasonality_decomp = CategoricalHyperparameter(
        "seasonality_decomp", choices=["STL", "ETS", "None"], default_value="None"
    )
    cs.add_hyperparameters([scaler, detrend, seasonality_decomp])


    return cs

--------------------------------------------------------------------------------
[2025-06-17 19:10:45] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#2): UniformIntegerHyperparameter.__init__() got an unexpected keyword argument 'q'
--------------------------------------------------------------------------------
[2025-06-17 19:10:52] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter
from ConfigSpace.conditions import EqualsCondition
from ConfigSpace.hyperparameters import UnParametrizedHyperparameter

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization of time series models.

    The configuration space includes hyperparameters for model selection (LSTM, GRU, RNN, Linear Regression, or Naive),
    number of layers, number of units per layer, dropout rate, learning rate, batch size,
    and optimization parameters.  Appropriate conditions are added between dependent hyperparameters.
    """
    cs = ConfigurationSpace()

    # Model Type Selection
    model_type = CategoricalHyperparameter(
        "model_type", choices=["LSTM", "GRU", "RNN", "LinearRegression", "Naive"], default_value="LSTM"
    )
    cs.add_hyperparameter(model_type)

    # LSTM/GRU/RNN Specific Hyperparameters
    num_layers = UniformIntegerHyperparameter(
        "num_layers", lower=1, upper=3, default_value=2
    )
    num_units = UniformIntegerHyperparameter(
        "num_units", lower=32, upper=256, default_value=64
    )
    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate", lower=0.0, upper=0.5, default_value=0.2
    )
    cs.add_hyperparameters([num_layers, num_units, dropout_rate])

    # Linear Regression / Naive Baseline does not need layers, units, or dropout
    condition_num_layers = EqualsCondition(num_layers, model_type, "LSTM")
    condition_num_units = EqualsCondition(num_units, model_type, "LSTM")
    condition_dropout_rate = EqualsCondition(dropout_rate, model_type, "LSTM")
    cs.add_conditions([condition_num_layers, condition_num_units, condition_dropout_rate])

    condition_num_layers_gru = EqualsCondition(num_layers, model_type, "GRU")
    condition_num_units_gru = EqualsCondition(num_units, model_type, "GRU")
    condition_dropout_rate_gru = EqualsCondition(dropout_rate, model_type, "GRU")
    cs.add_conditions([condition_num_layers_gru, condition_num_units_gru, condition_dropout_rate_gru])

    condition_num_layers_rnn = EqualsCondition(num_layers, model_type, "RNN")
    condition_num_units_rnn = EqualsCondition(num_units, model_type, "RNN")
    condition_dropout_rate_rnn = EqualsCondition(dropout_rate, model_type, "RNN")
    cs.add_conditions([condition_num_layers_rnn, condition_num_units_rnn, condition_dropout_rate_rnn])

    # Shared Hyperparameters
    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-4, upper=1e-2, default_value=1e-3, log=True
    )
    batch_size = CategoricalHyperparameter(
        "batch_size", choices=[32, 64, 128, 256], default_value=64
    )
    optimizer = CategoricalHyperparameter(
        "optimizer", choices=["Adam", "SGD"], default_value="Adam"
    )
    cs.add_hyperparameters([learning_rate, batch_size, optimizer])

    # Optimizer-specific hyperparameters
    beta_1 = UniformFloatHyperparameter(
        "beta_1", lower=0.8, upper=0.99, default_value=0.9, log=False
    )
    momentum = UniformFloatHyperparameter(
        "momentum", lower=0.0, upper=0.9, default_value=0.0, log=False
    )
    cs.add_hyperparameters([beta_1, momentum])

    # Condition beta_1 on Adam
    condition_beta_1 = EqualsCondition(beta_1, optimizer, "Adam")
    cs.add_condition(condition_beta_1)

    # Condition momentum on SGD
    condition_momentum = EqualsCondition(momentum, optimizer, "SGD")
    cs.add_condition(condition_momentum)
        
    # Add preprocessing boolean options for scaling, detrending, seasonality
    scaler = CategoricalHyperparameter(
        "scaler", choices=["MinMaxScaler", "StandardScaler", "None"], default_value="StandardScaler"
    )
    detrend = CategoricalHyperparameter(
        "detrend", choices=["Differencing", "Polynomial", "None"], default_value="None"
    )
    seasonality_decomp = CategoricalHyperparameter(
        "seasonality_decomp", choices=["STL", "ETS", "None"], default_value="None"
    )
    cs.add_hyperparameters([scaler, detrend, seasonality_decomp])


    return cs
```
--------------------------------------------------------------------------------
[2025-06-17 19:10:52] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter
from ConfigSpace.conditions import EqualsCondition
from ConfigSpace.hyperparameters import UnParametrizedHyperparameter

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization of time series models.

    The configuration space includes hyperparameters for model selection (LSTM, GRU, RNN, Linear Regression, or Naive),
    number of layers, number of units per layer, dropout rate, learning rate, batch size,
    and optimization parameters.  Appropriate conditions are added between dependent hyperparameters.
    """
    cs = ConfigurationSpace()

    # Model Type Selection
    model_type = CategoricalHyperparameter(
        "model_type", choices=["LSTM", "GRU", "RNN", "LinearRegression", "Naive"], default_value="LSTM"
    )
    cs.add_hyperparameter(model_type)

    # LSTM/GRU/RNN Specific Hyperparameters
    num_layers = UniformIntegerHyperparameter(
        "num_layers", lower=1, upper=3, default_value=2
    )
    num_units = UniformIntegerHyperparameter(
        "num_units", lower=32, upper=256, default_value=64
    )
    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate", lower=0.0, upper=0.5, default_value=0.2
    )
    cs.add_hyperparameters([num_layers, num_units, dropout_rate])

    # Linear Regression / Naive Baseline does not need layers, units, or dropout
    condition_num_layers = EqualsCondition(num_layers, model_type, "LSTM")
    condition_num_units = EqualsCondition(num_units, model_type, "LSTM")
    condition_dropout_rate = EqualsCondition(dropout_rate, model_type, "LSTM")
    cs.add_conditions([condition_num_layers, condition_num_units, condition_dropout_rate])

    condition_num_layers_gru = EqualsCondition(num_layers, model_type, "GRU")
    condition_num_units_gru = EqualsCondition(num_units, model_type, "GRU")
    condition_dropout_rate_gru = EqualsCondition(dropout_rate, model_type, "GRU")
    cs.add_conditions([condition_num_layers_gru, condition_num_units_gru, condition_dropout_rate_gru])

    condition_num_layers_rnn = EqualsCondition(num_layers, model_type, "RNN")
    condition_num_units_rnn = EqualsCondition(num_units, model_type, "RNN")
    condition_dropout_rate_rnn = EqualsCondition(dropout_rate, model_type, "RNN")
    cs.add_conditions([condition_num_layers_rnn, condition_num_units_rnn, condition_dropout_rate_rnn])

    # Shared Hyperparameters
    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-4, upper=1e-2, default_value=1e-3, log=True
    )
    batch_size = CategoricalHyperparameter(
        "batch_size", choices=[32, 64, 128, 256], default_value=64
    )
    optimizer = CategoricalHyperparameter(
        "optimizer", choices=["Adam", "SGD"], default_value="Adam"
    )
    cs.add_hyperparameters([learning_rate, batch_size, optimizer])

    # Optimizer-specific hyperparameters
    beta_1 = UniformFloatHyperparameter(
        "beta_1", lower=0.8, upper=0.99, default_value=0.9, log=False
    )
    momentum = UniformFloatHyperparameter(
        "momentum", lower=0.0, upper=0.9, default_value=0.0, log=False
    )
    cs.add_hyperparameters([beta_1, momentum])

    # Condition beta_1 on Adam
    condition_beta_1 = EqualsCondition(beta_1, optimizer, "Adam")
    cs.add_condition(condition_beta_1)

    # Condition momentum on SGD
    condition_momentum = EqualsCondition(momentum, optimizer, "SGD")
    cs.add_condition(condition_momentum)
        
    # Add preprocessing boolean options for scaling, detrending, seasonality
    scaler = CategoricalHyperparameter(
        "scaler", choices=["MinMaxScaler", "StandardScaler", "None"], default_value="StandardScaler"
    )
    detrend = CategoricalHyperparameter(
        "detrend", choices=["Differencing", "Polynomial", "None"], default_value="None"
    )
    seasonality_decomp = CategoricalHyperparameter(
        "seasonality_decomp", choices=["STL", "ETS", "None"], default_value="None"
    )
    cs.add_hyperparameters([scaler, detrend, seasonality_decomp])


    return cs

--------------------------------------------------------------------------------
[2025-06-17 19:10:52] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#3): Adding a second parent condition for a for a hyperparameter is ambiguous and therefore forbidden. Use an `OrConjunction` or `AndConjunction` to combine conditions instead.
Already inserted: num_layers | model_type == 'LSTM'
New one: num_layers | model_type == 'GRU'
--------------------------------------------------------------------------------
[2025-06-17 19:10:58] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter
from ConfigSpace.conditions import EqualsCondition, OrConjunction
from ConfigSpace.hyperparameters import UnParametrizedHyperparameter

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization of time series models.

    The configuration space includes hyperparameters for model selection (LSTM, GRU, RNN, Linear Regression, or Naive),
    number of layers, number of units per layer, dropout rate, learning rate, batch size,
    and optimization parameters.  Appropriate conditions are added between dependent hyperparameters.
    """
    cs = ConfigurationSpace()

    # Model Type Selection
    model_type = CategoricalHyperparameter(
        "model_type", choices=["LSTM", "GRU", "RNN", "LinearRegression", "Naive"], default_value="LSTM"
    )
    cs.add_hyperparameter(model_type)

    # LSTM/GRU/RNN Specific Hyperparameters
    num_layers = UniformIntegerHyperparameter(
        "num_layers", lower=1, upper=3, default_value=2
    )
    num_units = UniformIntegerHyperparameter(
        "num_units", lower=32, upper=256, default_value=64
    )
    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate", lower=0.0, upper=0.5, default_value=0.2
    )
    cs.add_hyperparameters([num_layers, num_units, dropout_rate])

    # Linear Regression / Naive Baseline does not need layers, units, or dropout
    lstm_condition = EqualsCondition(num_layers, model_type, "LSTM")
    gru_condition = EqualsCondition(num_layers, model_type, "GRU")
    rnn_condition = EqualsCondition(num_layers, model_type, "RNN")
    
    condition_num_layers = OrConjunction(lstm_condition, gru_condition, rnn_condition)
    cs.add_condition(condition_num_layers)

    lstm_condition = EqualsCondition(num_units, model_type, "LSTM")
    gru_condition = EqualsCondition(num_units, model_type, "GRU")
    rnn_condition = EqualsCondition(num_units, model_type, "RNN")

    condition_num_units = OrConjunction(lstm_condition, gru_condition, rnn_condition)
    cs.add_condition(condition_num_units)

    lstm_condition = EqualsCondition(dropout_rate, model_type, "LSTM")
    gru_condition = EqualsCondition(dropout_rate, model_type, "GRU")
    rnn_condition = EqualsCondition(dropout_rate, model_type, "RNN")

    condition_dropout_rate = OrConjunction(lstm_condition, gru_condition, rnn_condition)
    cs.add_condition(condition_dropout_rate)

    # Shared Hyperparameters
    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-4, upper=1e-2, default_value=1e-3, log=True
    )
    batch_size = CategoricalHyperparameter(
        "batch_size", choices=[32, 64, 128, 256], default_value=64
    )
    optimizer = CategoricalHyperparameter(
        "optimizer", choices=["Adam", "SGD"], default_value="Adam"
    )
    cs.add_hyperparameters([learning_rate, batch_size, optimizer])

    # Optimizer-specific hyperparameters
    beta_1 = UniformFloatHyperparameter(
        "beta_1", lower=0.8, upper=0.99, default_value=0.9, log=False
    )
    momentum = UniformFloatHyperparameter(
        "momentum", lower=0.0, upper=0.9, default_value=0.0, log=False
    )
    cs.add_hyperparameters([beta_1, momentum])

    # Condition beta_1 on Adam
    condition_beta_1 = EqualsCondition(beta_1, optimizer, "Adam")
    cs.add_condition(condition_beta_1)

    # Condition momentum on SGD
    condition_momentum = EqualsCondition(momentum, optimizer, "SGD")
    cs.add_condition(condition_momentum)
        
    # Add preprocessing boolean options for scaling, detrending, seasonality
    scaler = CategoricalHyperparameter(
        "scaler", choices=["MinMaxScaler", "StandardScaler", "None"], default_value="StandardScaler"
    )
    detrend = CategoricalHyperparameter(
        "detrend", choices=["Differencing", "Polynomial", "None"], default_value="None"
    )
    seasonality_decomp = CategoricalHyperparameter(
        "seasonality_decomp", choices=["STL", "ETS", "None"], default_value="None"
    )
    cs.add_hyperparameters([scaler, detrend, seasonality_decomp])


    return cs
```
--------------------------------------------------------------------------------
[2025-06-17 19:10:58] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter
from ConfigSpace.conditions import EqualsCondition, OrConjunction
from ConfigSpace.hyperparameters import UnParametrizedHyperparameter

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization of time series models.

    The configuration space includes hyperparameters for model selection (LSTM, GRU, RNN, Linear Regression, or Naive),
    number of layers, number of units per layer, dropout rate, learning rate, batch size,
    and optimization parameters.  Appropriate conditions are added between dependent hyperparameters.
    """
    cs = ConfigurationSpace()

    # Model Type Selection
    model_type = CategoricalHyperparameter(
        "model_type", choices=["LSTM", "GRU", "RNN", "LinearRegression", "Naive"], default_value="LSTM"
    )
    cs.add_hyperparameter(model_type)

    # LSTM/GRU/RNN Specific Hyperparameters
    num_layers = UniformIntegerHyperparameter(
        "num_layers", lower=1, upper=3, default_value=2
    )
    num_units = UniformIntegerHyperparameter(
        "num_units", lower=32, upper=256, default_value=64
    )
    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate", lower=0.0, upper=0.5, default_value=0.2
    )
    cs.add_hyperparameters([num_layers, num_units, dropout_rate])

    # Linear Regression / Naive Baseline does not need layers, units, or dropout
    lstm_condition = EqualsCondition(num_layers, model_type, "LSTM")
    gru_condition = EqualsCondition(num_layers, model_type, "GRU")
    rnn_condition = EqualsCondition(num_layers, model_type, "RNN")
    
    condition_num_layers = OrConjunction(lstm_condition, gru_condition, rnn_condition)
    cs.add_condition(condition_num_layers)

    lstm_condition = EqualsCondition(num_units, model_type, "LSTM")
    gru_condition = EqualsCondition(num_units, model_type, "GRU")
    rnn_condition = EqualsCondition(num_units, model_type, "RNN")

    condition_num_units = OrConjunction(lstm_condition, gru_condition, rnn_condition)
    cs.add_condition(condition_num_units)

    lstm_condition = EqualsCondition(dropout_rate, model_type, "LSTM")
    gru_condition = EqualsCondition(dropout_rate, model_type, "GRU")
    rnn_condition = EqualsCondition(dropout_rate, model_type, "RNN")

    condition_dropout_rate = OrConjunction(lstm_condition, gru_condition, rnn_condition)
    cs.add_condition(condition_dropout_rate)

    # Shared Hyperparameters
    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-4, upper=1e-2, default_value=1e-3, log=True
    )
    batch_size = CategoricalHyperparameter(
        "batch_size", choices=[32, 64, 128, 256], default_value=64
    )
    optimizer = CategoricalHyperparameter(
        "optimizer", choices=["Adam", "SGD"], default_value="Adam"
    )
    cs.add_hyperparameters([learning_rate, batch_size, optimizer])

    # Optimizer-specific hyperparameters
    beta_1 = UniformFloatHyperparameter(
        "beta_1", lower=0.8, upper=0.99, default_value=0.9, log=False
    )
    momentum = UniformFloatHyperparameter(
        "momentum", lower=0.0, upper=0.9, default_value=0.0, log=False
    )
    cs.add_hyperparameters([beta_1, momentum])

    # Condition beta_1 on Adam
    condition_beta_1 = EqualsCondition(beta_1, optimizer, "Adam")
    cs.add_condition(condition_beta_1)

    # Condition momentum on SGD
    condition_momentum = EqualsCondition(momentum, optimizer, "SGD")
    cs.add_condition(condition_momentum)
        
    # Add preprocessing boolean options for scaling, detrending, seasonality
    scaler = CategoricalHyperparameter(
        "scaler", choices=["MinMaxScaler", "StandardScaler", "None"], default_value="StandardScaler"
    )
    detrend = CategoricalHyperparameter(
        "detrend", choices=["Differencing", "Polynomial", "None"], default_value="None"
    )
    seasonality_decomp = CategoricalHyperparameter(
        "seasonality_decomp", choices=["STL", "ETS", "None"], default_value="None"
    )
    cs.add_hyperparameters([scaler, detrend, seasonality_decomp])


    return cs

--------------------------------------------------------------------------------
[2025-06-17 19:10:58] [Metadata: {'component': 'config', 'status': 'success'}] Configuration space generated successfully
--------------------------------------------------------------------------------
[2025-06-17 19:11:00] [Metadata: {'component': 'scenario'}] ```python
from smac import Scenario
from ConfigSpace import ConfigurationSpace


def generate_scenario(cs: ConfigurationSpace) -> Scenario:
    scenario = Scenario(
        configspace=cs,
        name="HyperparameterOptimization",
        output_directory="./logs/gemini-2.0-flash_Sunspot_20250617_191023",
        deterministic=False,
        n_trials=10,
        n_workers=1
    )
    return scenario
```
--------------------------------------------------------------------------------
[2025-06-17 19:11:00] [Metadata: {'component': 'scenario', 'action': 'run'}] from smac import Scenario
from ConfigSpace import ConfigurationSpace


def generate_scenario(cs: ConfigurationSpace) -> Scenario:
    scenario = Scenario(
        configspace=cs,
        name="HyperparameterOptimization",
        output_directory="./logs/gemini-2.0-flash_Sunspot_20250617_191023",
        deterministic=False,
        n_trials=10,
        n_workers=1
    )
    return scenario

--------------------------------------------------------------------------------
[2025-06-17 19:11:00] [Metadata: {'component': 'scenario', 'status': 'success'}] Scenario generated successfully
--------------------------------------------------------------------------------
[2025-06-17 19:11:12] [Metadata: {'component': 'train_function'}] ```python
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.linear_model import LinearRegression
import pandas as pd
import numpy as np
from typing import Any
from ConfigSpace import Configuration
from statsmodels.tsa.seasonal import STL
from statsmodels.tsa.api import ExponentialSmoothing

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a time series model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): The configuration object containing hyperparameters.
        dataset (Any): The dataset containing 'X' (features) and 'y' (labels).
        seed (int): The random seed for reproducibility.

    Returns:
        float: The negative validation loss/error.
    """
    torch.manual_seed(seed)
    np.random.seed(seed)

    X = dataset['X']
    y = dataset['y']
    
    # Convert to numpy arrays if they are pandas Series
    if isinstance(X, pd.Series):
        X = X.to_numpy()
    if isinstance(y, pd.Series):
        y = y.to_numpy()

    # Handle different shapes for X
    if X.ndim == 1:
        X = X.reshape(-1, 1)
    
    if y.ndim == 1:
        y = y.reshape(-1, 1)

    # Infer sequence length and feature dimension
    if X.ndim == 3:
        seq_len = X.shape[1]
        num_features = X.shape[2]
    elif X.ndim == 2:
        seq_len = 1
        num_features = X.shape[1]
    else:
        raise ValueError(f"Unexpected input dimension: {X.ndim}.  Expected 2 or 3.")
    
    model_type = cfg.get("model_type")
    
    # Data Preprocessing
    scaler_type = cfg.get("scaler")
    detrend_type = cfg.get("detrend")
    seasonality_decomp_type = cfg.get("seasonality_decomp")

    # Scaling
    if scaler_type == "MinMaxScaler":
        scaler = MinMaxScaler()
        X = scaler.fit_transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)
        y = scaler.fit_transform(y)
    elif scaler_type == "StandardScaler":
        scaler = StandardScaler()
        X = scaler.fit_transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)
        y = scaler.fit_transform(y)

    # Detrending
    if detrend_type == "Differencing":
        X = np.diff(X, axis=0)
        y = np.diff(y, axis=0)
    elif detrend_type == "Polynomial":
        poly = np.polyfit(np.arange(len(y)), y.flatten(), 1)  # Linear detrending
        trend = np.polyval(poly, np.arange(len(y)))
        y = y - trend.reshape(-1, 1)  # Ensure correct shape

    # Seasonality Decomposition
    if seasonality_decomp_type == "STL":
        stl = STL(y.flatten(), seasonal=13)  # Assuming seasonal period of 13
        res = stl.fit()
        y = res.trend
        X = X[13:]
        y = y[13:]
    elif seasonality_decomp_type == "ETS":
        model_ets = ExponentialSmoothing(y, seasonal_periods=12, seasonal='add').fit()
        y = model_ets.resid

    # Split data into training and validation sets
    train_size = int(len(X) * 0.8)
    X_train, X_val = X[:train_size], X[train_size:]
    y_train, y_val = y[:train_size], y[train_size:]

    # Convert to tensors
    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
    y_val_tensor = torch.tensor(y_val, dtype=torch.float32)

    # Model Definition and Training
    if model_type in ["LSTM", "GRU", "RNN"]:
        if X_train_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_train_tensor.shape}")
        
        num_layers = cfg.get("num_layers")
        num_units = cfg.get("num_units")
        dropout_rate = cfg.get("dropout_rate")
        batch_size = cfg.get("batch_size")
        learning_rate = cfg.get("learning_rate")
        optimizer_name = cfg.get("optimizer")
        
        class RNNModel(nn.Module):
            def __init__(self, input_size, hidden_size, num_layers, dropout, rnn_type):
                super(RNNModel, self).__init__()
                self.rnn_type = rnn_type
                if rnn_type == "LSTM":
                    self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
                elif rnn_type == "GRU":
                    self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
                else:  # RNN
                    self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
                self.linear = nn.Linear(hidden_size, 1)
            def forward(self, x):
                out, _ = self.rnn(x)
                out = self.linear(out[:, -1, :])  # Only the last time step
                return out

        model = RNNModel(num_features, num_units, num_layers, dropout_rate, model_type)
        
        if optimizer_name == "Adam":
            beta_1 = cfg.get("beta_1")
            optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta_1, 0.999))
        else:  # SGD
            momentum = cfg.get("momentum")
            optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
        
        criterion = nn.MSELoss()
        
        for epoch in range(10):
            model.train()
            for i in range(0, len(X_train_tensor), batch_size):
                X_batch = X_train_tensor[i:i + batch_size]
                y_batch = y_train_tensor[i:i + batch_size]

                optimizer.zero_grad()
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                loss.backward()
                optimizer.step()

        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val_tensor)
            val_loss = criterion(val_outputs, y_val_tensor)
        return -val_loss.item()
        
    elif model_type == "LinearRegression":
        model = LinearRegression()
        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)
        mse = np.mean((y_val - y_pred)**2)
        return -mse

    elif model_type == "Naive":
        y_pred = np.roll(y_train, 1)
        y_pred[0] = 0 # first pred is set to zero
        mse = np.mean((y_val[:len(y_pred)] - y_pred[-len(y_val):])**2) #compare the two last points from train and val.
        return -mse

    else:
        raise ValueError(f"Unknown model type: {model_type}")
```
--------------------------------------------------------------------------------
[2025-06-17 19:11:12] [Metadata: {'component': 'train_function', 'action': 'run'}] import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.linear_model import LinearRegression
import pandas as pd
import numpy as np
from typing import Any
from ConfigSpace import Configuration
from statsmodels.tsa.seasonal import STL
from statsmodels.tsa.api import ExponentialSmoothing

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a time series model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): The configuration object containing hyperparameters.
        dataset (Any): The dataset containing 'X' (features) and 'y' (labels).
        seed (int): The random seed for reproducibility.

    Returns:
        float: The negative validation loss/error.
    """
    torch.manual_seed(seed)
    np.random.seed(seed)

    X = dataset['X']
    y = dataset['y']
    
    # Convert to numpy arrays if they are pandas Series
    if isinstance(X, pd.Series):
        X = X.to_numpy()
    if isinstance(y, pd.Series):
        y = y.to_numpy()

    # Handle different shapes for X
    if X.ndim == 1:
        X = X.reshape(-1, 1)
    
    if y.ndim == 1:
        y = y.reshape(-1, 1)

    # Infer sequence length and feature dimension
    if X.ndim == 3:
        seq_len = X.shape[1]
        num_features = X.shape[2]
    elif X.ndim == 2:
        seq_len = 1
        num_features = X.shape[1]
    else:
        raise ValueError(f"Unexpected input dimension: {X.ndim}.  Expected 2 or 3.")
    
    model_type = cfg.get("model_type")
    
    # Data Preprocessing
    scaler_type = cfg.get("scaler")
    detrend_type = cfg.get("detrend")
    seasonality_decomp_type = cfg.get("seasonality_decomp")

    # Scaling
    if scaler_type == "MinMaxScaler":
        scaler = MinMaxScaler()
        X = scaler.fit_transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)
        y = scaler.fit_transform(y)
    elif scaler_type == "StandardScaler":
        scaler = StandardScaler()
        X = scaler.fit_transform(X.reshape(-1, X.shape[-1])).reshape(X.shape)
        y = scaler.fit_transform(y)

    # Detrending
    if detrend_type == "Differencing":
        X = np.diff(X, axis=0)
        y = np.diff(y, axis=0)
    elif detrend_type == "Polynomial":
        poly = np.polyfit(np.arange(len(y)), y.flatten(), 1)  # Linear detrending
        trend = np.polyval(poly, np.arange(len(y)))
        y = y - trend.reshape(-1, 1)  # Ensure correct shape

    # Seasonality Decomposition
    if seasonality_decomp_type == "STL":
        stl = STL(y.flatten(), seasonal=13)  # Assuming seasonal period of 13
        res = stl.fit()
        y = res.trend
        X = X[13:]
        y = y[13:]
    elif seasonality_decomp_type == "ETS":
        model_ets = ExponentialSmoothing(y, seasonal_periods=12, seasonal='add').fit()
        y = model_ets.resid

    # Split data into training and validation sets
    train_size = int(len(X) * 0.8)
    X_train, X_val = X[:train_size], X[train_size:]
    y_train, y_val = y[:train_size], y[train_size:]

    # Convert to tensors
    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
    y_val_tensor = torch.tensor(y_val, dtype=torch.float32)

    # Model Definition and Training
    if model_type in ["LSTM", "GRU", "RNN"]:
        if X_train_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_train_tensor.shape}")
        
        num_layers = cfg.get("num_layers")
        num_units = cfg.get("num_units")
        dropout_rate = cfg.get("dropout_rate")
        batch_size = cfg.get("batch_size")
        learning_rate = cfg.get("learning_rate")
        optimizer_name = cfg.get("optimizer")
        
        class RNNModel(nn.Module):
            def __init__(self, input_size, hidden_size, num_layers, dropout, rnn_type):
                super(RNNModel, self).__init__()
                self.rnn_type = rnn_type
                if rnn_type == "LSTM":
                    self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
                elif rnn_type == "GRU":
                    self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
                else:  # RNN
                    self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
                self.linear = nn.Linear(hidden_size, 1)
            def forward(self, x):
                out, _ = self.rnn(x)
                out = self.linear(out[:, -1, :])  # Only the last time step
                return out

        model = RNNModel(num_features, num_units, num_layers, dropout_rate, model_type)
        
        if optimizer_name == "Adam":
            beta_1 = cfg.get("beta_1")
            optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta_1, 0.999))
        else:  # SGD
            momentum = cfg.get("momentum")
            optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
        
        criterion = nn.MSELoss()
        
        for epoch in range(10):
            model.train()
            for i in range(0, len(X_train_tensor), batch_size):
                X_batch = X_train_tensor[i:i + batch_size]
                y_batch = y_train_tensor[i:i + batch_size]

                optimizer.zero_grad()
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                loss.backward()
                optimizer.step()

        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val_tensor)
            val_loss = criterion(val_outputs, y_val_tensor)
        return -val_loss.item()
        
    elif model_type == "LinearRegression":
        model = LinearRegression()
        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)
        mse = np.mean((y_val - y_pred)**2)
        return -mse

    elif model_type == "Naive":
        y_pred = np.roll(y_train, 1)
        y_pred[0] = 0 # first pred is set to zero
        mse = np.mean((y_val[:len(y_pred)] - y_pred[-len(y_val):])**2) #compare the two last points from train and val.
        return -mse

    else:
        raise ValueError(f"Unknown model type: {model_type}")

--------------------------------------------------------------------------------
[2025-06-17 19:11:12] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#1): 'DataFrame' object has no attribute 'reshape'
--------------------------------------------------------------------------------
[2025-06-17 19:11:23] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.linear_model import LinearRegression
import pandas as pd
import numpy as np
from typing import Any
from ConfigSpace import Configuration
from statsmodels.tsa.seasonal import STL
from statsmodels.tsa.api import ExponentialSmoothing

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a time series model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): The configuration object containing hyperparameters.
        dataset (Any): The dataset containing 'X' (features) and 'y' (labels).
        seed (int): The random seed for reproducibility.

    Returns:
        float: The negative validation loss/error.
    """
    torch.manual_seed(seed)
    np.random.seed(seed)

    X = dataset['X']
    y = dataset['y']
    
    # Convert to numpy arrays if they are pandas Series
    if isinstance(X, pd.Series):
        X = X.to_numpy()
    if isinstance(y, pd.Series):
        y = y.to_numpy()

    # Handle different shapes for X
    if X.ndim == 1:
        X = X.reshape(-1, 1)
    
    if y.ndim == 1:
        y = y.reshape(-1, 1)

    # Infer sequence length and feature dimension
    if X.ndim == 3:
        seq_len = X.shape[1]
        num_features = X.shape[2]
    elif X.ndim == 2:
        seq_len = 1
        num_features = X.shape[1]
    else:
        raise ValueError(f"Unexpected input dimension: {X.ndim}.  Expected 2 or 3.")
    
    model_type = cfg.get("model_type")
    
    # Data Preprocessing
    scaler_type = cfg.get("scaler")
    detrend_type = cfg.get("detrend")
    seasonality_decomp_type = cfg.get("seasonality_decomp")

    # Scaling
    if scaler_type == "MinMaxScaler":
        scaler = MinMaxScaler()
        X = scaler.fit_transform(X)
        y = scaler.fit_transform(y)
    elif scaler_type == "StandardScaler":
        scaler = StandardScaler()
        X = scaler.fit_transform(X)
        y = scaler.fit_transform(y)

    # Detrending
    if detrend_type == "Differencing":
        X = np.diff(X, axis=0)
        y = np.diff(y, axis=0)
    elif detrend_type == "Polynomial":
        poly = np.polyfit(np.arange(len(y)), y.flatten(), 1)  # Linear detrending
        trend = np.polyval(poly, np.arange(len(y)))
        y = y - trend.reshape(-1, 1)  # Ensure correct shape

    # Seasonality Decomposition
    if seasonality_decomp_type == "STL":
        stl = STL(y.flatten(), seasonal=13)  # Assuming seasonal period of 13
        res = stl.fit()
        y = res.trend
        X = X[13:]
        y = y[13:]
    elif seasonality_decomp_type == "ETS":
        model_ets = ExponentialSmoothing(y, seasonal_periods=12, seasonal='add').fit()
        y = model_ets.resid

    # Split data into training and validation sets
    train_size = int(len(X) * 0.8)
    X_train, X_val = X[:train_size], X[train_size:]
    y_train, y_val = y[:train_size], y[train_size:]

    # Convert to tensors
    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
    y_val_tensor = torch.tensor(y_val, dtype=torch.float32)

    # Model Definition and Training
    if model_type in ["LSTM", "GRU", "RNN"]:
        if X_train_tensor.ndim != 3 and seq_len > 1:
            X_train_tensor = X_train_tensor.reshape(X_train_tensor.shape[0], seq_len, num_features)
            X_val_tensor = X_val_tensor.reshape(X_val_tensor.shape[0], seq_len, num_features)
        elif X_train_tensor.ndim != 3 and seq_len == 1:
            X_train_tensor = X_train_tensor.reshape(X_train_tensor.shape[0], seq_len, num_features)
            X_val_tensor = X_val_tensor.reshape(X_val_tensor.shape[0], seq_len, num_features)

        num_layers = cfg.get("num_layers")
        num_units = cfg.get("num_units")
        dropout_rate = cfg.get("dropout_rate")
        batch_size = cfg.get("batch_size")
        learning_rate = cfg.get("learning_rate")
        optimizer_name = cfg.get("optimizer")
        
        class RNNModel(nn.Module):
            def __init__(self, input_size, hidden_size, num_layers, dropout, rnn_type):
                super(RNNModel, self).__init__()
                self.rnn_type = rnn_type
                if rnn_type == "LSTM":
                    self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
                elif rnn_type == "GRU":
                    self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
                else:  # RNN
                    self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
                self.linear = nn.Linear(hidden_size, 1)
            def forward(self, x):
                out, _ = self.rnn(x)
                out = self.linear(out[:, -1, :])  # Only the last time step
                return out

        model = RNNModel(num_features, num_units, num_layers, dropout_rate, model_type)
        
        if optimizer_name == "Adam":
            beta_1 = cfg.get("beta_1")
            optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta_1, 0.999))
        else:  # SGD
            momentum = cfg.get("momentum")
            optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
        
        criterion = nn.MSELoss()
        
        for epoch in range(10):
            model.train()
            for i in range(0, len(X_train_tensor), batch_size):
                X_batch = X_train_tensor[i:i + batch_size]
                y_batch = y_train_tensor[i:i + batch_size]

                optimizer.zero_grad()
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                loss.backward()
                optimizer.step()

        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val_tensor)
            val_loss = criterion(val_outputs, y_val_tensor)
        return -val_loss.item()
        
    elif model_type == "LinearRegression":
        model = LinearRegression()
        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)
        mse = np.mean((y_val - y_pred)**2)
        return -mse

    elif model_type == "Naive":
        y_pred = np.roll(y_train, 1)
        y_pred[0] = 0 # first pred is set to zero
        mse = np.mean((y_val[:len(y_pred)] - y_pred[-len(y_val):])**2) #compare the two last points from train and val.
        return -mse

    else:
        raise ValueError(f"Unknown model type: {model_type}")
```
--------------------------------------------------------------------------------
[2025-06-17 19:11:23] [Metadata: {'component': 'train_function', 'action': 'run'}] import torch
import torch.nn as nn
import torch.optim as optim
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.linear_model import LinearRegression
import pandas as pd
import numpy as np
from typing import Any
from ConfigSpace import Configuration
from statsmodels.tsa.seasonal import STL
from statsmodels.tsa.api import ExponentialSmoothing

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a time series model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): The configuration object containing hyperparameters.
        dataset (Any): The dataset containing 'X' (features) and 'y' (labels).
        seed (int): The random seed for reproducibility.

    Returns:
        float: The negative validation loss/error.
    """
    torch.manual_seed(seed)
    np.random.seed(seed)

    X = dataset['X']
    y = dataset['y']
    
    # Convert to numpy arrays if they are pandas Series
    if isinstance(X, pd.Series):
        X = X.to_numpy()
    if isinstance(y, pd.Series):
        y = y.to_numpy()

    # Handle different shapes for X
    if X.ndim == 1:
        X = X.reshape(-1, 1)
    
    if y.ndim == 1:
        y = y.reshape(-1, 1)

    # Infer sequence length and feature dimension
    if X.ndim == 3:
        seq_len = X.shape[1]
        num_features = X.shape[2]
    elif X.ndim == 2:
        seq_len = 1
        num_features = X.shape[1]
    else:
        raise ValueError(f"Unexpected input dimension: {X.ndim}.  Expected 2 or 3.")
    
    model_type = cfg.get("model_type")
    
    # Data Preprocessing
    scaler_type = cfg.get("scaler")
    detrend_type = cfg.get("detrend")
    seasonality_decomp_type = cfg.get("seasonality_decomp")

    # Scaling
    if scaler_type == "MinMaxScaler":
        scaler = MinMaxScaler()
        X = scaler.fit_transform(X)
        y = scaler.fit_transform(y)
    elif scaler_type == "StandardScaler":
        scaler = StandardScaler()
        X = scaler.fit_transform(X)
        y = scaler.fit_transform(y)

    # Detrending
    if detrend_type == "Differencing":
        X = np.diff(X, axis=0)
        y = np.diff(y, axis=0)
    elif detrend_type == "Polynomial":
        poly = np.polyfit(np.arange(len(y)), y.flatten(), 1)  # Linear detrending
        trend = np.polyval(poly, np.arange(len(y)))
        y = y - trend.reshape(-1, 1)  # Ensure correct shape

    # Seasonality Decomposition
    if seasonality_decomp_type == "STL":
        stl = STL(y.flatten(), seasonal=13)  # Assuming seasonal period of 13
        res = stl.fit()
        y = res.trend
        X = X[13:]
        y = y[13:]
    elif seasonality_decomp_type == "ETS":
        model_ets = ExponentialSmoothing(y, seasonal_periods=12, seasonal='add').fit()
        y = model_ets.resid

    # Split data into training and validation sets
    train_size = int(len(X) * 0.8)
    X_train, X_val = X[:train_size], X[train_size:]
    y_train, y_val = y[:train_size], y[train_size:]

    # Convert to tensors
    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)
    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)
    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)
    y_val_tensor = torch.tensor(y_val, dtype=torch.float32)

    # Model Definition and Training
    if model_type in ["LSTM", "GRU", "RNN"]:
        if X_train_tensor.ndim != 3 and seq_len > 1:
            X_train_tensor = X_train_tensor.reshape(X_train_tensor.shape[0], seq_len, num_features)
            X_val_tensor = X_val_tensor.reshape(X_val_tensor.shape[0], seq_len, num_features)
        elif X_train_tensor.ndim != 3 and seq_len == 1:
            X_train_tensor = X_train_tensor.reshape(X_train_tensor.shape[0], seq_len, num_features)
            X_val_tensor = X_val_tensor.reshape(X_val_tensor.shape[0], seq_len, num_features)

        num_layers = cfg.get("num_layers")
        num_units = cfg.get("num_units")
        dropout_rate = cfg.get("dropout_rate")
        batch_size = cfg.get("batch_size")
        learning_rate = cfg.get("learning_rate")
        optimizer_name = cfg.get("optimizer")
        
        class RNNModel(nn.Module):
            def __init__(self, input_size, hidden_size, num_layers, dropout, rnn_type):
                super(RNNModel, self).__init__()
                self.rnn_type = rnn_type
                if rnn_type == "LSTM":
                    self.rnn = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
                elif rnn_type == "GRU":
                    self.rnn = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
                else:  # RNN
                    self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
                self.linear = nn.Linear(hidden_size, 1)
            def forward(self, x):
                out, _ = self.rnn(x)
                out = self.linear(out[:, -1, :])  # Only the last time step
                return out

        model = RNNModel(num_features, num_units, num_layers, dropout_rate, model_type)
        
        if optimizer_name == "Adam":
            beta_1 = cfg.get("beta_1")
            optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta_1, 0.999))
        else:  # SGD
            momentum = cfg.get("momentum")
            optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
        
        criterion = nn.MSELoss()
        
        for epoch in range(10):
            model.train()
            for i in range(0, len(X_train_tensor), batch_size):
                X_batch = X_train_tensor[i:i + batch_size]
                y_batch = y_train_tensor[i:i + batch_size]

                optimizer.zero_grad()
                outputs = model(X_batch)
                loss = criterion(outputs, y_batch)
                loss.backward()
                optimizer.step()

        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val_tensor)
            val_loss = criterion(val_outputs, y_val_tensor)
        return -val_loss.item()
        
    elif model_type == "LinearRegression":
        model = LinearRegression()
        model.fit(X_train, y_train)
        y_pred = model.predict(X_val)
        mse = np.mean((y_val - y_pred)**2)
        return -mse

    elif model_type == "Naive":
        y_pred = np.roll(y_train, 1)
        y_pred[0] = 0 # first pred is set to zero
        mse = np.mean((y_val[:len(y_pred)] - y_pred[-len(y_val):])**2) #compare the two last points from train and val.
        return -mse

    else:
        raise ValueError(f"Unknown model type: {model_type}")

--------------------------------------------------------------------------------
[2025-06-17 19:11:23] [Metadata: {'component': 'train_function', 'status': 'success', 'loss': -0.0883231737506197}] Training executed successfully, loss: -0.0883231737506197
--------------------------------------------------------------------------------
