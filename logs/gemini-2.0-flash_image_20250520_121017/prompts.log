[2025-05-20 12:10:19] [Metadata: {'component': 'config'}] **TASK**

Goal: Write a Python function called `get_configspace()` that returns a valid `ConfigurationSpace` for a classification task.

---

**STRICT OUTPUT RULES**

* Output only the `get_configspace()` function and necessary imports.
* Do not include any extra text, explanations, or comments.
* Code must be syntactically correct, executable, and compatible with SMAC.

---

**ALLOWED CLASSES**

**Core Classes**

* `ConfigurationSpace`
* `Categorical`
* `Float`
* `Integer`
* `Constant`

**Conditions**

* `EqualsCondition`
* `InCondition`
* `OrConjunction`

**Forbidden Clauses**

* `ForbiddenEqualsClause`
* `ForbiddenAndConjunction`

**Distributions (only if needed)**

* `Beta`
* `Normal`

**Serialization (only if needed)**

* `to_yaml()`
* `from_yaml()`

---

**ALLOWED OPTIONS**

* `default`
* `log`
* `distribution`
* `seed`

---

**CONSTRAINTS**

* Must include **at least one** `ForbiddenAndConjunction` to block invalid combinations.

---

**CONFIGURATION SPACE REQUIREMENTS**

* Initialize `ConfigurationSpace` with `seed=1234`.

---

**DATASET DESCRIPTION**

* The configuration space must be based on the following information
This is an image dataset.
Number of images: 60000
Labels available: 60000
Raw feature shape: (60000, 784)
.
* Hyperparameters and model choices must reflect what is appropriate for that dataset type.

---

**IMPORTANT RULE**

* Do **not** use any classes, functions, methods, or modules outside of the **ALLOWED CLASSES**.

[EXAMPLES]

# Example 1: Basic ConfigurationSpace
```python
from ConfigSpace import ConfigurationSpace

cs = ConfigurationSpace(
    space={
        "C": (-1.0, 1.0),
        "max_iter": (10, 100),
    },
    seed=1234,
)
```
# Example 2: Adding Hyperparameters
```python
from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer

kernel_type = Categorical('kernel_type', ['linear', 'poly', 'rbf', 'sigmoid'])
degree = Integer('degree', bounds=(2, 4), default=2)
coef0 = Float('coef0', bounds=(0, 1), default=0.0)
gamma = Float('gamma', bounds=(1e-5, 1e2), default=1, log=True)

cs = ConfigurationSpace()
cs.add([kernel_type, degree, coef0, gamma])
```
# Example 3: Adding Conditions
```python
from ConfigSpace import EqualsCondition, InCondition, OrConjunction

cond_1 = EqualsCondition(degree, kernel_type, 'poly')
cond_2 = OrConjunction(
    EqualsCondition(coef0, kernel_type, 'poly'),
    EqualsCondition(coef0, kernel_type, 'sigmoid')
)
cond_3 = InCondition(gamma, kernel_type, ['rbf', 'poly', 'sigmoid'])
```
# Example 4: Adding Forbidden Clauses
```pyhon
from ConfigSpace import ForbiddenEqualsClause, ForbiddenAndConjunction

penalty_and_loss = ForbiddenAndConjunction(
    ForbiddenEqualsClause(penalty, "l1"),
    ForbiddenEqualsClause(loss, "hinge")
)
constant_penalty_and_loss = ForbiddenAndConjunction(
    ForbiddenEqualsClause(dual, "False"),
    ForbiddenEqualsClause(penalty, "l2"),
    ForbiddenEqualsClause(loss, "hinge")
)
penalty_and_dual = ForbiddenAndConjunction(
    ForbiddenEqualsClause(dual, "False"),
    ForbiddenEqualsClause(penalty, "l1")
)
```
Example 5: Serialization
```python
from pathlib import Path
from ConfigSpace import ConfigurationSpace

path = Path("configspace.yaml")
cs = ConfigurationSpace(
    space={
        "C": (-1.0, 1.0),
        "max_iter": (10, 100),
    },
    seed=1234,
)
cs.to_yaml(path)
loaded_cs = ConfigurationSpace.from_yaml(path)
```
# Example 6: Priors
```python
import numpy as np
from ConfigSpace import ConfigurationSpace, Float, Categorical, Beta, Normal

cs = ConfigurationSpace(
    space={
        "lr": Float(
            'lr',
            bounds=(1e-5, 1e-1),
            default=1e-3,
            log=True,
            distribution=Normal(1e-3, 1e-1)
        ),
        "dropout": Float(
            'dropout',
            bounds=(0, 0.99),
            default=0.25,
            distribution=Beta(alpha=2, beta=4)
        ),
        "activation": Categorical(
            'activation',
            items=['tanh', 'relu'],
            weights=[0.2, 0.8]
        ),
    },
    seed=1234,
)
```

--------------------------------------------------------------------------------
[2025-05-20 12:10:19] [Metadata: {'component': 'config', 'action': 'run'}] Running config code:
--------------------------------------------------------------------------------
[2025-05-20 12:10:19] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#1): Given vector violates forbidden clause: (Forbidden: momentum == 0.9 && Forbidden: optimizer == 'adam')
--------------------------------------------------------------------------------
[2025-05-20 12:10:19] [Metadata: {'component': 'config', 'action': 'fix'}] The following python code is generated by an LLM which now has run into the following errors:
Given vector violates forbidden clause: (Forbidden: momentum == 0.9 && Forbidden: optimizer == 'adam')

Your task is to fix the issues in the code. The requirements below MUST be respected:

1. Do not change the function name or its purpose.
2. Fix all syntax and runtime errors.
3. Use only valid parameters.
4. Do not change the logic unless necessary for fixing errors.
5. Return ONLY the corrected function code — no explanations, comments, or extra output.
6. The function must be syntactically and semantically correct, and executable with `exec`.

Buggy code:
```python
from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause

def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    optimizer = Categorical("optimizer", ["sgd", "adam", "rmsprop"], default="adam")
    learning_rate = Float("learning_rate", (1e-5, 1e-2), log=True, default=1e-3)
    batch_size = Integer("batch_size", (32, 256), default=64, log=True)
    momentum = Float("momentum", (0.0, 0.99), default=0.9)
    beta1 = Float("beta1", (0.0, 0.999), default=0.9)

    # Add hyperparameters to the configuration space
    cs.add([optimizer, learning_rate, batch_size, momentum, beta1])

    # Add forbidden clauses
    forbidden_momentum = ForbiddenAndConjunction(
        ForbiddenEqualsClause(optimizer, "adam"),
        ForbiddenEqualsClause(momentum, 0.9)
    )

    cs.add_forbidden_clause(forbidden_momentum)

    return cs
```

--------------------------------------------------------------------------------
[2025-05-20 12:10:21] [Metadata: {'component': 'config', 'action': 'run'}] Running config code:
--------------------------------------------------------------------------------
[2025-05-20 12:10:21] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#2): Given vector violates forbidden clause: (Forbidden: momentum == 0.9 && Forbidden: optimizer == 'adam')
--------------------------------------------------------------------------------
[2025-05-20 12:10:21] [Metadata: {'component': 'config', 'action': 'fix'}] The following python code is generated by an LLM which now has run into the following errors:
Given vector violates forbidden clause: (Forbidden: momentum == 0.9 && Forbidden: optimizer == 'adam')
Given vector violates forbidden clause: (Forbidden: momentum == 0.9 && Forbidden: optimizer == 'adam')

Your task is to fix the issues in the code. The requirements below MUST be respected:

1. Do not change the function name or its purpose.
2. Fix all syntax and runtime errors.
3. Use only valid parameters.
4. Do not change the logic unless necessary for fixing errors.
5. Return ONLY the corrected function code — no explanations, comments, or extra output.
6. The function must be syntactically and semantically correct, and executable with `exec`.

Buggy code:
```python
from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause

def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    optimizer = Categorical("optimizer", ["sgd", "adam", "rmsprop"], default="adam")
    learning_rate = Float("learning_rate", (1e-5, 1e-2), log=True, default=1e-3)
    batch_size = Integer("batch_size", (32, 256), default=64, log=True)
    momentum = Float("momentum", (0.0, 0.9), default=0.9)
    beta1 = Float("beta1", (0.0, 0.999), default=0.9)

    # Add hyperparameters to the configuration space
    cs.add([optimizer, learning_rate, batch_size, momentum, beta1])

    # Add forbidden clauses
    forbidden_momentum = ForbiddenAndConjunction(
        ForbiddenEqualsClause(optimizer, "adam"),
        ForbiddenEqualsClause(momentum, 0.9)
    )

    cs.add_forbidden_clause(forbidden_momentum)

    return cs
```

--------------------------------------------------------------------------------
[2025-05-20 12:10:23] [Metadata: {'component': 'config', 'action': 'run'}] Running config code:
--------------------------------------------------------------------------------
[2025-05-20 12:10:23] [Metadata: {'component': 'scenario'}] ---

**Objective:**
Generate a **Python function** named `generate_scenario(cs)` that returns a valid `Scenario` object configured for SMAC (v2.0+), strictly following the rules below.

---

**Output Format Rules (Strict):**

* Output **only** the function `generate_scenario(cs)` and the **necessary import statements**.
* Use **Python 3.10 syntax** but **do not** include type annotations for the function or parameters.
* The code must be **fully executable** with the latest **SMAC v2.0+** version.
* Output **only valid Python code** – **no comments**, **no explanations**, **no extra text**, and **no example usage**.
* The function must be **self-contained**.

---

**Functional Requirements:**

* The input `cs` is a `ConfigurationSpace` object.
* Return a `Scenario` configured with the following:
  * `output_directory`: `"./automl_results"`
  * `deterministic`: `False` (enable variability)
  * `n_workers`: greater than 1 (to enable parallel optimization)
  * `min_budget` and `max_budget`: set appropriately for multi-fidelity tuning (e.g., training epochs)
---

**Reminder:** The output must be limited to:

* Valid `import` statements
* A single `generate_scenario(cs)` function that returns a properly configured `Scenario` object
* Do not use any parameters other than the ones explicitly listed in this prompt.

---

**Example (Correct Output Format):**

```python
from smac import Scenario
from ConfigSpace import Configuration

def generate_scenario(cs: Configuration):
    scenario = Scenario(
        configspace=cs,
        objectives="validation_loss",
        output_directory="./automl_results",
        deterministic=False,
        min_budget=1,
        max_budget=100,
        n_workers=4
    )
    return scenario
```

--------------------------------------------------------------------------------
[2025-05-20 12:10:23] [Metadata: {'component': 'scenario', 'action': 'run'}] Running scenario code:
--------------------------------------------------------------------------------
[2025-05-20 12:10:32] [Metadata: {'component': 'train_function'}] **Generate production-grade Python code for a machine learning training function with the following STRICT requirements:**

---

### **Function signature** must be:

```python
from ConfigSpace import Configuration
def train(cfg: Configuration, dataset: Any) -> float:
```

---

### **Function Behavior Requirements:**

* The function **must accept** a `dataset` dictionary with:

  * `dataset['X']`: feature matrix or input tensor
  * `dataset['y']`: label vector or label tensor

* Assume `cfg` is a sampled configuration object:

  * Access primitive values using `cfg.get('key')` (only `int`, `float`, `str`, etc.).
  * **Do not access or manipulate non-primitive hyperparameter objects**.

* The function must return the **average training loss** over 10 epochs.

* You must check whether dataset['X'] is already image-shaped (e.g., len(X.shape) == 4). If not, and CNN is used, reshape carefully and raise a ValueError if the input size is not a perfect square.

* Do not assume dataset['X'] has a specific shape. Always verify input dimensions before reshaping.

* If using a CNN model, you must validate that reshaping is safe and explain your assumption.

```python
return loss  # float
```

* Lower `loss` means a better model.

---

### **Frameworks**

You may choose **PyTorch**, **TensorFlow**, or **scikit-learn**, depending on the dataset and supporting code provided.

---

### **Model Requirements**

* Infer input and output dimensions dynamically from the dataset:

  ```python
  input_size = dataset['X'].shape[1]
  num_classes = len(np.unique(dataset['y']))
  ```

---

### **Supporting Code Provided:**

* ConfigSpace definition: `from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause

def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    optimizer = Categorical("optimizer", ["sgd", "adam", "rmsprop"], default="adam")
    learning_rate = Float("learning_rate", (1e-5, 1e-2), log=True, default=1e-3)
    batch_size = Integer("batch_size", (32, 256), default=64, log=True)
    momentum = Float("momentum", (0.0, 0.9), default=0.0)
    beta1 = Float("beta1", (0.0, 0.999), default=0.9)

    # Add hyperparameters to the configuration space
    cs.add([optimizer, learning_rate, batch_size, momentum, beta1])

    # Add forbidden clauses
    forbidden_momentum = ForbiddenAndConjunction(
        ForbiddenEqualsClause(optimizer, "adam"),
        ForbiddenEqualsClause(momentum, 0.9)
    )

    cs.add_forbidden_clause(forbidden_momentum)

    return cs
`
* SMAC scenario: `from smac import Scenario
from ConfigSpace import ConfigurationSpace

def generate_scenario(cs):
    scenario = Scenario(
        configspace=cs,
        output_directory="./automl_results",
        deterministic=False,
        n_workers=2,
        min_budget=1,
        max_budget=9
    )
    return scenario
`
* Dataset description: `This is an image dataset.
Number of images: 60000
Labels available: 60000
Raw feature shape: (60000, 784)
`

---

### **Additional Instructions**

* The code must not hardcode dataset dimensions like `784` or class count `10`.
* The function must be runnable and not assume unavailable classes or modules.
* You must only output the `def train(...)` function and nothing else.

--------------------------------------------------------------------------------
[2025-05-20 12:10:32] [Metadata: {'component': 'train_function', 'action': 'run'}] Running train_function code:
--------------------------------------------------------------------------------
[2025-05-20 12:10:32] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#1): expected sequence of length 60000 at dim 0 (got 784)
--------------------------------------------------------------------------------
[2025-05-20 12:10:32] [Metadata: {'component': 'train_function', 'action': 'fix'}] The following python code is generated by an LLM which now has run into the following errors:
expected sequence of length 60000 at dim 0 (got 784)

Your task is to fix the issues in the code. The requirements below MUST be respected:

1. Do not change the function name or its purpose.
2. Fix all syntax and runtime errors.
3. Use only valid parameters.
4. Do not change the logic unless necessary for fixing errors.
5. Return ONLY the corrected function code — no explanations, comments, or extra output.
6. The function must be syntactically and semantically correct, and executable with `exec`.

Buggy code:
```python
from typing import Any
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from ConfigSpace import Configuration


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a neural network model on the provided dataset.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing the training data with keys 'X' (features) and 'y' (labels).

    Returns:
        float: The average training loss over 10 epochs.
    """

    X = dataset['X']
    y = dataset['y']

    # Infer input and output dimensions dynamically from the dataset
    if len(X.shape) == 1:
       raise ValueError("Invalid feature data with dimensions 1")
    elif len(X.shape) == 2:
        input_size = X.shape[1]
    elif len(X.shape) == 4:  # Assuming image-shaped data (batch_size, channels, height, width)
        input_size = X.shape[1] * X.shape[2] * X.shape[3]  # Flattening for fully connected network
    else:
        raise ValueError("Unsupported input data shape.  Expected 2D or 4D array.")

    num_classes = len(np.unique(y))

    # Hyperparameter loading
    optimizer_name = cfg.get("optimizer")
    learning_rate = cfg.get("learning_rate")
    batch_size = cfg.get("batch_size")
    momentum = cfg.get("momentum")
    beta1 = cfg.get("beta1")

    # Convert data to PyTorch tensors
    X = torch.tensor(X, dtype=torch.float32)
    y = torch.tensor(y, dtype=torch.long)

    # Reshape if needed and use a CNN
    use_cnn = True  # Assume we will use a CNN by default.
    if len(X.shape) != 4:  # data is not image shaped.
        if use_cnn: # User is requesting a CNN model. We must reshape to use it.
            side = int(np.sqrt(X.shape[1]))  # e.g., sqrt(784) = 28
            if side * side != X.shape[1]:
                raise ValueError(f"Input size {X.shape[1]} is not a perfect square. Cannot reshape to image for CNN.")
            X = X.reshape(-1, 1, side, side)  # Reshape to (batch_size, channels, height, width)
        input_channels = 1  # Assuming grayscale images if reshaping.
        image_size = side
    else:
        input_channels = X.shape[1]
        image_size = X.shape[2]

    # Create TensorDataset and DataLoader
    dataset = TensorDataset(X, y)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Define a CNN model
    class CNN(nn.Module):
        def __init__(self, num_classes):
            super(CNN, self).__init__()
            self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1)
            self.relu1 = nn.ReLU()
            self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
            self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
            self.relu2 = nn.ReLU()
            self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
            self.fc1 = nn.Linear(64 * (image_size // 4) * (image_size // 4), 128)
            self.relu3 = nn.ReLU()
            self.fc2 = nn.Linear(128, num_classes)

        def forward(self, x):
            x = self.pool1(self.relu1(self.conv1(x)))
            x = self.pool2(self.relu2(self.conv2(x)))
            x = x.view(x.size(0), -1)  # Flatten
            x = self.relu3(self.fc1(x))
            x = self.fc2(x)
            return x

    model = CNN(num_classes=num_classes)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # Use GPU if available
    model.to(device)
    # Define optimizer
    if optimizer_name == "sgd":
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    elif optimizer_name == "adam":
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, 0.999))
    elif optimizer_name == "rmsprop":
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Unknown optimizer: {optimizer_name}")

    # Define loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    epochs = 10
    total_loss = 0.0
    for epoch in range(epochs):
        epoch_loss = 0.0
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

        avg_epoch_loss = epoch_loss / len(dataloader)
        total_loss += avg_epoch_loss
        #print(f"Epoch {epoch+1}, Loss: {avg_epoch_loss}") # Consider commenting to reduce log output

    avg_loss = total_loss / epochs
    return float(avg_loss)
```

--------------------------------------------------------------------------------
[2025-05-20 12:10:40] [Metadata: {'component': 'train_function', 'action': 'run'}] Running train_function code:
--------------------------------------------------------------------------------
[2025-05-20 12:10:40] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#2): expected sequence of length 60000 at dim 0 (got 784)
--------------------------------------------------------------------------------
[2025-05-20 12:10:40] [Metadata: {'component': 'train_function', 'action': 'fix'}] The following python code is generated by an LLM which now has run into the following errors:
expected sequence of length 60000 at dim 0 (got 784)
expected sequence of length 60000 at dim 0 (got 784)

Your task is to fix the issues in the code. The requirements below MUST be respected:

1. Do not change the function name or its purpose.
2. Fix all syntax and runtime errors.
3. Use only valid parameters.
4. Do not change the logic unless necessary for fixing errors.
5. Return ONLY the corrected function code — no explanations, comments, or extra output.
6. The function must be syntactically and semantically correct, and executable with `exec`.

Buggy code:
```python
from typing import Any
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from ConfigSpace import Configuration


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a neural network model on the provided dataset.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing the training data with keys 'X' (features) and 'y' (labels).

    Returns:
        float: The average training loss over 10 epochs.
    """

    X = dataset['X']
    y = dataset['y']

    # Infer input and output dimensions dynamically from the dataset
    if len(X.shape) == 1:
       raise ValueError("Invalid feature data with dimensions 1")
    elif len(X.shape) == 2:
        input_size = X.shape[1]
    elif len(X.shape) == 4:  # Assuming image-shaped data (batch_size, channels, height, width)
        input_size = X.shape[1] * X.shape[2] * X.shape[3]  # Flattening for fully connected network
    else:
        raise ValueError("Unsupported input data shape.  Expected 2D or 4D array.")

    num_classes = len(np.unique(y))

    # Hyperparameter loading
    optimizer_name = cfg.get("optimizer")
    learning_rate = cfg.get("learning_rate")
    batch_size = cfg.get("batch_size")
    momentum = cfg.get("momentum")
    beta1 = cfg.get("beta1")

    # Convert data to PyTorch tensors
    X = torch.tensor(X, dtype=torch.float32)
    y = torch.tensor(y, dtype=torch.long)

    # Reshape if needed and use a CNN
    use_cnn = True  # Assume we will use a CNN by default.
    if len(X.shape) != 4:  # data is not image shaped.
        if use_cnn: # User is requesting a CNN model. We must reshape to use it.
            side = int(np.sqrt(X.shape[1]))  # e.g., sqrt(784) = 28
            if side * side != X.shape[1]:
                raise ValueError(f"Input size {X.shape[1]} is not a perfect square. Cannot reshape to image for CNN.")
            X = X.reshape(-1, 1, side, side)  # Reshape to (batch_size, channels, height, width)
        input_channels = 1  # Assuming grayscale images if reshaping.
        image_size = side
    else:
        input_channels = X.shape[1]
        image_size = X.shape[2]

    # Create TensorDataset and DataLoader
    dataset = TensorDataset(X, y)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Define a CNN model
    class CNN(nn.Module):
        def __init__(self, num_classes):
            super(CNN, self).__init__()
            self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1)
            self.relu1 = nn.ReLU()
            self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
            self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
            self.relu2 = nn.ReLU()
            self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
            self.fc1 = nn.Linear(64 * (image_size // 4) * (image_size // 4), 128)
            self.relu3 = nn.ReLU()
            self.fc2 = nn.Linear(128, num_classes)

        def forward(self, x):
            x = self.pool1(self.relu1(self.conv1(x)))
            x = self.pool2(self.relu2(self.conv2(x)))
            x = x.view(x.size(0), -1)  # Flatten
            x = self.relu3(self.fc1(x))
            x = self.fc2(x)
            return x

    model = CNN(num_classes=num_classes)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # Use GPU if available
    model.to(device)
    # Define optimizer
    if optimizer_name == "sgd":
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum if momentum is not None else 0)
    elif optimizer_name == "adam":
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1 if beta1 is not None else 0.9, 0.999))
    elif optimizer_name == "rmsprop":
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Unknown optimizer: {optimizer_name}")

    # Define loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    epochs = 10
    total_loss = 0.0
    for epoch in range(epochs):
        epoch_loss = 0.0
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

        avg_epoch_loss = epoch_loss / len(dataloader)
        total_loss += avg_epoch_loss
        #print(f"Epoch {{epoch+1}}, Loss: {{avg_epoch_loss}}") # Consider commenting to reduce log output

    avg_loss = total_loss / epochs
    return float(avg_loss)
```

--------------------------------------------------------------------------------
[2025-05-20 12:10:47] [Metadata: {'component': 'train_function', 'action': 'run'}] Running train_function code:
--------------------------------------------------------------------------------
[2025-05-20 12:10:47] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#3): expected sequence of length 60000 at dim 0 (got 784)
--------------------------------------------------------------------------------
[2025-05-20 12:10:47] [Metadata: {'component': 'train_function', 'action': 'fix'}] The following python code is generated by an LLM which now has run into the following errors:
expected sequence of length 60000 at dim 0 (got 784)
expected sequence of length 60000 at dim 0 (got 784)
expected sequence of length 60000 at dim 0 (got 784)

Your task is to fix the issues in the code. The requirements below MUST be respected:

1. Do not change the function name or its purpose.
2. Fix all syntax and runtime errors.
3. Use only valid parameters.
4. Do not change the logic unless necessary for fixing errors.
5. Return ONLY the corrected function code — no explanations, comments, or extra output.
6. The function must be syntactically and semantically correct, and executable with `exec`.

Buggy code:
```python
from typing import Any
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from ConfigSpace import Configuration


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a neural network model on the provided dataset.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing the training data with keys 'X' (features) and 'y' (labels).

    Returns:
        float: The average training loss over 10 epochs.
    """

    X = dataset['X']
    y = dataset['y']

    # Infer input and output dimensions dynamically from the dataset
    if len(X.shape) == 1:
       raise ValueError("Invalid feature data with dimensions 1")
    elif len(X.shape) == 2:
        input_size = X.shape[1]
    elif len(X.shape) == 4:  # Assuming image-shaped data (batch_size, channels, height, width)
        input_size = X.shape[1] * X.shape[2] * X.shape[3]  # Flattening for fully connected network
    else:
        raise ValueError("Unsupported input data shape.  Expected 2D or 4D array.")

    num_classes = len(np.unique(y))

    # Hyperparameter loading
    optimizer_name = cfg.get("optimizer")
    learning_rate = cfg.get("learning_rate")
    batch_size = cfg.get("batch_size")
    momentum = cfg.get("momentum")
    beta1 = cfg.get("beta1")

    # Convert data to PyTorch tensors
    X = torch.tensor(X, dtype=torch.float32)
    y = torch.tensor(y, dtype=torch.long)

    # Reshape if needed and use a CNN
    use_cnn = True  # Assume we will use a CNN by default.
    if len(X.shape) != 4:  # data is not image shaped.
        if use_cnn: # User is requesting a CNN model. We must reshape to use it.
            side = int(np.sqrt(X.shape[1]))  # e.g., sqrt(784) = 28
            if side * side != X.shape[1]:
                raise ValueError(f"Input size {X.shape[1]} is not a perfect square. Cannot reshape to image for CNN.")
            X = X.reshape(-1, 1, side, side)  # Reshape to (batch_size, channels, height, width)
        input_channels = 1  # Assuming grayscale images if reshaping.
        image_size = side
    else:
        input_channels = X.shape[1]
        image_size = X.shape[2]

    # Create TensorDataset and DataLoader
    dataset = TensorDataset(X, y)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Define a CNN model
    class CNN(nn.Module):
        def __init__(self, num_classes):
            super(CNN, self).__init__()
            self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1)
            self.relu1 = nn.ReLU()
            self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
            self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
            self.relu2 = nn.ReLU()
            self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
            self.fc1 = nn.Linear(64 * (image_size // 4) * (image_size // 4), 128)
            self.relu3 = nn.ReLU()
            self.fc2 = nn.Linear(128, num_classes)

        def forward(self, x):
            x = self.pool1(self.relu1(self.conv1(x)))
            x = self.pool2(self.relu2(self.conv2(x)))
            x = x.view(x.size(0), -1)  # Flatten
            x = self.relu3(self.fc1(x))
            x = self.fc2(x)
            return x

    model = CNN(num_classes=num_classes)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # Use GPU if available
    model.to(device)
    # Define optimizer
    if optimizer_name == "sgd":
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum if momentum is not None else 0)
    elif optimizer_name == "adam":
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1 if beta1 is not None else 0.9, 0.999))
    elif optimizer_name == "rmsprop":
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Unknown optimizer: {optimizer_name}")

    # Define loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    epochs = 10
    total_loss = 0.0
    for epoch in range(epochs):
        epoch_loss = 0.0
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

        avg_epoch_loss = epoch_loss / len(dataloader)
        total_loss += avg_epoch_loss
        #print(f"Epoch {{{{epoch+1}}}}, Loss: {{{{avg_epoch_loss}}}}") # Consider commenting to reduce log output

    avg_loss = total_loss / epochs
    return float(avg_loss)
```

--------------------------------------------------------------------------------
[2025-05-20 12:10:55] [Metadata: {'component': 'train_function', 'action': 'run'}] Running train_function code:
--------------------------------------------------------------------------------
[2025-05-20 12:10:55] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#4): expected sequence of length 60000 at dim 0 (got 784)
--------------------------------------------------------------------------------
[2025-05-20 12:10:55] [Metadata: {'component': 'train_function', 'action': 'fix'}] The following python code is generated by an LLM which now has run into the following errors:
expected sequence of length 60000 at dim 0 (got 784)
expected sequence of length 60000 at dim 0 (got 784)
expected sequence of length 60000 at dim 0 (got 784)
expected sequence of length 60000 at dim 0 (got 784)

Your task is to fix the issues in the code. The requirements below MUST be respected:

1. Do not change the function name or its purpose.
2. Fix all syntax and runtime errors.
3. Use only valid parameters.
4. Do not change the logic unless necessary for fixing errors.
5. Return ONLY the corrected function code — no explanations, comments, or extra output.
6. The function must be syntactically and semantically correct, and executable with `exec`.

Buggy code:
```python
from typing import Any
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from ConfigSpace import Configuration


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a neural network model on the provided dataset.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing the training data with keys 'X' (features) and 'y' (labels).

    Returns:
        float: The average training loss over 10 epochs.
    """

    X = dataset['X']
    y = dataset['y']

    # Infer input and output dimensions dynamically from the dataset
    if len(X.shape) == 1:
       raise ValueError("Invalid feature data with dimensions 1")
    elif len(X.shape) == 2:
        input_size = X.shape[1]
    elif len(X.shape) == 4:  # Assuming image-shaped data (batch_size, channels, height, width)
        input_size = X.shape[1] * X.shape[2] * X.shape[3]  # Flattening for fully connected network
    else:
        raise ValueError("Unsupported input data shape.  Expected 2D or 4D array.")

    num_classes = len(np.unique(y))

    # Hyperparameter loading
    optimizer_name = cfg.get("optimizer")
    learning_rate = cfg.get("learning_rate")
    batch_size = cfg.get("batch_size")
    momentum = cfg.get("momentum")
    beta1 = cfg.get("beta1")

    # Convert data to PyTorch tensors
    X = torch.tensor(X, dtype=torch.float32)
    y = torch.tensor(y, dtype=torch.long)

    # Reshape if needed and use a CNN
    use_cnn = True  # Assume we will use a CNN by default.
    if len(X.shape) != 4:  # data is not image shaped.
        if use_cnn: # User is requesting a CNN model. We must reshape to use it.
            side = int(np.sqrt(X.shape[1]))  # e.g., sqrt(784) = 28
            if side * side != X.shape[1]:
                raise ValueError(f"Input size {X.shape[1]} is not a perfect square. Cannot reshape to image for CNN.")
            X = X.reshape(-1, 1, side, side)  # Reshape to (batch_size, channels, height, width)
        input_channels = 1  # Assuming grayscale images if reshaping.
        image_size = side
    else:
        input_channels = X.shape[1]
        image_size = X.shape[2]

    # Create TensorDataset and DataLoader
    dataset = TensorDataset(X, y)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Define a CNN model
    class CNN(nn.Module):
        def __init__(self, num_classes):
            super(CNN, self).__init__()
            self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1)
            self.relu1 = nn.ReLU()
            self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
            self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
            self.relu2 = nn.ReLU()
            self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
            self.fc1 = nn.Linear(64 * (image_size // 4) * (image_size // 4), 128)
            self.relu3 = nn.ReLU()
            self.fc2 = nn.Linear(128, num_classes)

        def forward(self, x):
            x = self.pool1(self.relu1(self.conv1(x)))
            x = self.pool2(self.relu2(self.conv2(x)))
            x = x.view(x.size(0), -1)  # Flatten
            x = self.relu3(self.fc1(x))
            x = self.fc2(x)
            return x

    model = CNN(num_classes=num_classes)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu") # Use GPU if available
    model.to(device)
    # Define optimizer
    if optimizer_name == "sgd":
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum if momentum is not None else 0)
    elif optimizer_name == "adam":
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1 if beta1 is not None else 0.9, 0.999))
    elif optimizer_name == "rmsprop":
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Unknown optimizer: {optimizer_name}")

    # Define loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    epochs = 10
    total_loss = 0.0
    for epoch in range(epochs):
        epoch_loss = 0.0
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()

        avg_epoch_loss = epoch_loss / len(dataloader)
        total_loss += avg_epoch_loss
        #print(f"Epoch {epoch+1}, Loss: {avg_epoch_loss}") # Consider commenting to reduce log output

    avg_loss = total_loss / epochs
    return float(avg_loss)
```

--------------------------------------------------------------------------------
[2025-05-20 12:11:03] [Metadata: {'component': 'train_function', 'action': 'run'}] Running train_function code:
--------------------------------------------------------------------------------
[2025-05-20 12:11:03] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#5): expected sequence of length 60000 at dim 0 (got 784)
--------------------------------------------------------------------------------
[2025-05-20 12:11:03] [Metadata: {'component': 'train_function', 'action': 'retry'}] Retry limit reached for train_function. Fetching fresh code from LLM.
--------------------------------------------------------------------------------
[2025-05-20 12:11:10] [Metadata: {'component': 'train_function', 'action': 'run'}] Running train_function code:
--------------------------------------------------------------------------------
[2025-05-20 12:11:10] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#1): expected sequence of length 60000 at dim 0 (got 784)
--------------------------------------------------------------------------------
[2025-05-20 12:11:10] [Metadata: {'component': 'train_function', 'action': 'fix'}] The following python code is generated by an LLM which now has run into the following errors:
expected sequence of length 60000 at dim 0 (got 784)
expected sequence of length 60000 at dim 0 (got 784)
expected sequence of length 60000 at dim 0 (got 784)
expected sequence of length 60000 at dim 0 (got 784)
expected sequence of length 60000 at dim 0 (got 784)

Your task is to fix the issues in the code. The requirements below MUST be respected:

1. Do not change the function name or its purpose.
2. Fix all syntax and runtime errors.
3. Use only valid parameters.
4. Do not change the logic unless necessary for fixing errors.
5. Return ONLY the corrected function code — no explanations, comments, or extra output.
6. The function must be syntactically and semantically correct, and executable with `exec`.

Buggy code:
```python
from typing import Any
import numpy as np
from ConfigSpace import Configuration
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

class SimpleCNN(nn.Module):
    def __init__(self, input_channels, num_classes):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(input_channels, 32, kernel_size=3, stride=1, padding=1)
        self.relu1 = nn.ReLU()
        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.relu2 = nn.ReLU()
        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.flatten = nn.Flatten()
        self.fc1 = nn.Linear(64 * 7 * 7, 128)  # Assuming 28x28 images after reshaping
        self.relu3 = nn.ReLU()
        self.fc2 = nn.Linear(128, num_classes)

    def forward(self, x):
        x = self.conv1(x)
        x = self.relu1(x)
        x = self.maxpool1(x)
        x = self.conv2(x)
        x = self.relu2(x)
        x = self.maxpool2(x)
        x = self.flatten(x)
        x = self.fc1(x)
        x = self.relu3(x)
        x = self.fc2(x)
        return x

def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a simple CNN on the given dataset for 10 epochs and returns the average training loss.

    Args:
        cfg (Configuration): A ConfigSpace Configuration object containing hyperparameters.
        dataset (Any): A dictionary containing the training data with keys 'X' (features) and 'y' (labels).

    Returns:
        float: The average training loss over 10 epochs.
    """

    X = dataset['X']
    y = dataset['y']

    # Infer input and output dimensions dynamically
    num_classes = len(np.unique(y))

    # Hyperparameter extraction from Configuration object
    optimizer_name = cfg.get("optimizer")
    learning_rate = cfg.get("learning_rate")
    batch_size = cfg.get("batch_size")
    momentum = cfg.get("momentum")
    beta1 = cfg.get("beta1")

    # Device configuration
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Data Preprocessing
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.long)

    # Reshape the input if it's not already an image
    if len(X_tensor.shape) == 2:
        input_size = X_tensor.shape[1]
        side = int(np.sqrt(input_size))
        if side * side != input_size:
            raise ValueError("Input size is not a perfect square. Cannot reshape to image.")

        X_tensor = X_tensor.reshape(-1, 1, side, side)  # Reshape to (N, 1, side, side)
        input_channels = 1  # Grayscale images

        # Explanation:
        # The original data is a 2D tensor (N, 784) where 784 represents a flattened 28x28 image.
        # This reshape operation transforms the data into a 4D tensor (N, 1, 28, 28), which is suitable
        # for a CNN. The '1' in the second dimension represents a single channel (grayscale).  If we had
        # color images, this would be 3.
        # We are assuming each feature array can be perfectly formed to a NxNx1 channel image
    elif len(X_tensor.shape) == 4:
        # Assuming it is an image shaped tensor N x C x H x W
        input_channels = X_tensor.shape[1]

    else:
        raise ValueError("Unsupported input shape. Expecting 2D (N, features) or 4D (N, C, H, W) tensor.")

    # Create TensorDataset and DataLoader
    dataset = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Model instantiation
    model = SimpleCNN(input_channels, num_classes).to(device)

    # Optimizer selection
    if optimizer_name == "adam":
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, 0.999))
    elif optimizer_name == "sgd":
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    elif optimizer_name == "rmsprop":
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Unknown optimizer: {optimizer_name}")

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    total_loss = 0.0
    for epoch in range(10):
        epoch_loss = 0.0
        for inputs, labels in dataloader:
            inputs, labels = inputs.to(device), labels.to(device)

            # Zero the parameter gradients
            optimizer.zero_grad()

            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_epoch_loss = epoch_loss / len(dataloader)
        total_loss += avg_epoch_loss

    avg_loss = total_loss / 10
    return float(avg_loss)
```

--------------------------------------------------------------------------------
[2025-05-20 12:11:17] [Metadata: {'component': 'train_function', 'action': 'run'}] Running train_function code:
--------------------------------------------------------------------------------
