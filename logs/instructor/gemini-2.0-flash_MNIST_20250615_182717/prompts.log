[2025-06-15 18:27:28] [Metadata: {'dataset_name': 'MNIST'}] You are a data science expert with deep knowledge of image datasets. Your task is to generate structured, and comprehensive instructions for effectively working with the dataset 'MNIST'. Dataset description: This is an image dataset.

Number of classes: 10
Class distribution:
1    6742
7    6265
3    6131
2    5958
9    5949
0    5923
6    5918
8    5851
4    5842
5    5421
Name: count, dtype: int64

Image Data Handling Requirements:
1. Input Format Requirements:
   - For CNN models: Input must be in (batch, channels, height, width) format
   - For dense/linear layers: Input should be flattened

2. Data Processing Steps:
   a) For flattened input (2D):
      - Calculate dimensions: height = width = int(sqrt(n_features))
      - Verify square dimensions: height * height == n_features
      - Reshape to (N, 1, H, W) for CNNs
   b) For 3D input (N, H, W):
      - Add channel dimension: reshape to (N, 1, H, W)
   c) For 4D input:
      - Verify channel order matches framework requirements

3. Framework-Specific Format:
   - PyTorch: (N, C, H, W)
   - TensorFlow: (N, H, W, C)
   - Convert between formats if necessary

4. Normalization:
   - Scale pixel values to [0, 1] by dividing by 255.0
   - Or standardize to mean=0, std=1


Please perform the following tasks:
1. Describe the recommended data preprocessing steps specific to this dataset.
2. Outline useful feature engineering strategies relevant to the dataset's type and domain.
3. Mention any common challenges or important considerations when working with this dataset type.
4. If this dataset exists on OpenML:
   - Provide the exact dataset name as listed on OpenML.
   - Include the dataset's tag(s) on OpenML.
   - Provide the direct OpenML URL to the dataset.

Return all the above in a structured JSON format matching the following fields:
- dataset_name (str)
- dataset_tag (str)
- dataset_instructions (list of str)

            Below, I am providing a set of parameters that were used on the top-performing models evaluated on the dataset.
            Please generate an instruction for creating a configuration for the dataset based on the following parameters:
            [{'batch_size': '128', 'build_fn': '{"oml-python:serialized_object": "function", "value": "__main__.vggnet_fmnist_fmnist_5_128_False"}', 'epochs': '5', 'layer0': '{"class_name": "Reshape", "config": {"batch_input_shape": [null, 784], "dtype": "float32", "name": "reshape_99", "target_shape": [28, 28, 1], "trainable": true}}', 'layer1': '{"class_name": "Conv2D", "config": {"activation": "relu", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "data_format": "channels_last", "dilation_rate": [1, 1], "filters": 64, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "kernel_size": [3, 3], "name": "conv2d_687", "padding": "same", "strides": [1, 1], "trainable": false, "use_bias": true}}', 'layer10': '{"class_name": "Conv2D", "config": {"activation": "relu", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "data_format": "channels_last", "dilation_rate": [1, 1], "filters": 256, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "kernel_size": [3, 3], "name": "conv2d_692", "padding": "same", "strides": [1, 1], "trainable": true, "use_bias": true}}', 'layer11': '{"class_name": "Conv2D", "config": {"activation": "relu", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "data_format": "channels_last", "dilation_rate": [1, 1], "filters": 256, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "kernel_size": [3, 3], "name": "conv2d_693", "padding": "same", "strides": [1, 1], "trainable": true, "use_bias": true}}', 'layer12': '{"class_name": "MaxPooling2D", "config": {"data_format": "channels_last", "name": "max_pooling2d_297", "padding": "valid", "pool_size": [2, 2], "strides": [2, 2], "trainable": true}}', 'layer13': '{"class_name": "BatchNormalization", "config": {"axis": -1, "beta_constraint": null, "beta_initializer": {"class_name": "Zeros", "config": {}}, "beta_regularizer": null, "center": true, "epsilon": 0.001, "gamma_constraint": null, "gamma_initializer": {"class_name": "Ones", "config": {}}, "gamma_regularizer": null, "momentum": 0.99, "moving_mean_initializer": {"class_name": "Zeros", "config": {}}, "moving_variance_initializer": {"class_name": "Ones", "config": {}}, "name": "batch_normalization_297", "scale": true, "trainable": true}}', 'layer14': '{"class_name": "Flatten", "config": {"data_format": "channels_last", "name": "flatten_99", "trainable": true}}', 'layer15': '{"class_name": "Dense", "config": {"activation": "relu", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "name": "dense_295", "trainable": true, "units": 4096, "use_bias": true}}', 'layer16': '{"class_name": "Dropout", "config": {"name": "dropout_197", "noise_shape": null, "rate": 0.5, "seed": null, "trainable": true}}', 'layer17': '{"class_name": "Dense", "config": {"activation": "relu", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "name": "dense_296", "trainable": true, "units": 4096, "use_bias": true}}', 'layer18': '{"class_name": "Dropout", "config": {"name": "dropout_198", "noise_shape": null, "rate": 0.5, "seed": null, "trainable": true}}', 'layer19': '{"class_name": "Dense", "config": {"activation": "softmax", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "name": "dense_297", "trainable": true, "units": 10, "use_bias": true}}', 'layer2': '{"class_name": "Conv2D", "config": {"activation": "relu", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "data_format": "channels_last", "dilation_rate": [1, 1], "filters": 64, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "kernel_size": [3, 3], "name": "conv2d_688", "padding": "same", "strides": [1, 1], "trainable": false, "use_bias": true}}', 'layer3': '{"class_name": "MaxPooling2D", "config": {"data_format": "channels_last", "name": "max_pooling2d_295", "padding": "valid", "pool_size": [2, 2], "strides": [2, 2], "trainable": true}}', 'layer4': '{"class_name": "BatchNormalization", "config": {"axis": -1, "beta_constraint": null, "beta_initializer": {"class_name": "Zeros", "config": {}}, "beta_regularizer": null, "center": true, "epsilon": 0.001, "gamma_constraint": null, "gamma_initializer": {"class_name": "Ones", "config": {}}, "gamma_regularizer": null, "momentum": 0.99, "moving_mean_initializer": {"class_name": "Zeros", "config": {}}, "moving_variance_initializer": {"class_name": "Ones", "config": {}}, "name": "batch_normalization_295", "scale": true, "trainable": true}}', 'layer5': '{"class_name": "Conv2D", "config": {"activation": "relu", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "data_format": "channels_last", "dilation_rate": [1, 1], "filters": 128, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "kernel_size": [3, 3], "name": "conv2d_689", "padding": "same", "strides": [1, 1], "trainable": true, "use_bias": true}}', 'layer6': '{"class_name": "Conv2D", "config": {"activation": "relu", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "data_format": "channels_last", "dilation_rate": [1, 1], "filters": 128, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "kernel_size": [3, 3], "name": "conv2d_690", "padding": "same", "strides": [1, 1], "trainable": true, "use_bias": true}}', 'layer7': '{"class_name": "MaxPooling2D", "config": {"data_format": "channels_last", "name": "max_pooling2d_296", "padding": "valid", "pool_size": [2, 2], "strides": [2, 2], "trainable": true}}', 'layer8': '{"class_name": "BatchNormalization", "config": {"axis": -1, "beta_constraint": null, "beta_initializer": {"class_name": "Zeros", "config": {}}, "beta_regularizer": null, "center": true, "epsilon": 0.001, "gamma_constraint": null, "gamma_initializer": {"class_name": "Ones", "config": {}}, "gamma_regularizer": null, "momentum": 0.99, "moving_mean_initializer": {"class_name": "Zeros", "config": {}}, "moving_variance_initializer": {"class_name": "Ones", "config": {}}, "name": "batch_normalization_296", "scale": true, "trainable": true}}', 'layer9': '{"class_name": "Conv2D", "config": {"activation": "relu", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "data_format": "channels_last", "dilation_rate": [1, 1], "filters": 256, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "kernel_size": [3, 3], "name": "conv2d_691", "padding": "same", "strides": [1, 1], "trainable": true, "use_bias": true}}', 'verbose': '2'}, {'batch_size': '32', 'build_fn': '{"oml-python:serialized_object": "function", "value": "__main__.vggnet_fmnist_fmnist_5_32_False"}', 'epochs': '5', 'layer0': '{"class_name": "Reshape", "config": {"batch_input_shape": [null, 784], "dtype": "float32", "name": "reshape_99", "target_shape": [28, 28, 1], "trainable": true}}', 'layer1': '{"class_name": "Conv2D", "config": {"activation": "relu", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "data_format": "channels_last", "dilation_rate": [1, 1], "filters": 64, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "kernel_size": [3, 3], "name": "conv2d_687", "padding": "same", "strides": [1, 1], "trainable": false, "use_bias": true}}', 'layer10': '{"class_name": "Conv2D", "config": {"activation": "relu", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "data_format": "channels_last", "dilation_rate": [1, 1], "filters": 256, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "kernel_size": [3, 3], "name": "conv2d_692", "padding": "same", "strides": [1, 1], "trainable": true, "use_bias": true}}', 'layer11': '{"class_name": "Conv2D", "config": {"activation": "relu", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "data_format": "channels_last", "dilation_rate": [1, 1], "filters": 256, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "kernel_size": [3, 3], "name": "conv2d_693", "padding": "same", "strides": [1, 1], "trainable": true, "use_bias": true}}', 'layer12': '{"class_name": "MaxPooling2D", "config": {"data_format": "channels_last", "name": "max_pooling2d_297", "padding": "valid", "pool_size": [2, 2], "strides": [2, 2], "trainable": true}}', 'layer13': '{"class_name": "BatchNormalization", "config": {"axis": -1, "beta_constraint": null, "beta_initializer": {"class_name": "Zeros", "config": {}}, "beta_regularizer": null, "center": true, "epsilon": 0.001, "gamma_constraint": null, "gamma_initializer": {"class_name": "Ones", "config": {}}, "gamma_regularizer": null, "momentum": 0.99, "moving_mean_initializer": {"class_name": "Zeros", "config": {}}, "moving_variance_initializer": {"class_name": "Ones", "config": {}}, "name": "batch_normalization_297", "scale": true, "trainable": true}}', 'layer14': '{"class_name": "Flatten", "config": {"data_format": "channels_last", "name": "flatten_99", "trainable": true}}', 'layer15': '{"class_name": "Dense", "config": {"activation": "relu", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "name": "dense_295", "trainable": true, "units": 4096, "use_bias": true}}', 'layer16': '{"class_name": "Dropout", "config": {"name": "dropout_197", "noise_shape": null, "rate": 0.5, "seed": null, "trainable": true}}', 'layer17': '{"class_name": "Dense", "config": {"activation": "relu", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "name": "dense_296", "trainable": true, "units": 4096, "use_bias": true}}', 'layer18': '{"class_name": "Dropout", "config": {"name": "dropout_198", "noise_shape": null, "rate": 0.5, "seed": null, "trainable": true}}', 'layer19': '{"class_name": "Dense", "config": {"activation": "softmax", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "name": "dense_297", "trainable": true, "units": 10, "use_bias": true}}', 'layer2': '{"class_name": "Conv2D", "config": {"activation": "relu", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "data_format": "channels_last", "dilation_rate": [1, 1], "filters": 64, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "kernel_size": [3, 3], "name": "conv2d_688", "padding": "same", "strides": [1, 1], "trainable": false, "use_bias": true}}', 'layer3': '{"class_name": "MaxPooling2D", "config": {"data_format": "channels_last", "name": "max_pooling2d_295", "padding": "valid", "pool_size": [2, 2], "strides": [2, 2], "trainable": true}}', 'layer4': '{"class_name": "BatchNormalization", "config": {"axis": -1, "beta_constraint": null, "beta_initializer": {"class_name": "Zeros", "config": {}}, "beta_regularizer": null, "center": true, "epsilon": 0.001, "gamma_constraint": null, "gamma_initializer": {"class_name": "Ones", "config": {}}, "gamma_regularizer": null, "momentum": 0.99, "moving_mean_initializer": {"class_name": "Zeros", "config": {}}, "moving_variance_initializer": {"class_name": "Ones", "config": {}}, "name": "batch_normalization_295", "scale": true, "trainable": true}}', 'layer5': '{"class_name": "Conv2D", "config": {"activation": "relu", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "data_format": "channels_last", "dilation_rate": [1, 1], "filters": 128, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "kernel_size": [3, 3], "name": "conv2d_689", "padding": "same", "strides": [1, 1], "trainable": true, "use_bias": true}}', 'layer6': '{"class_name": "Conv2D", "config": {"activation": "relu", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "data_format": "channels_last", "dilation_rate": [1, 1], "filters": 128, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "kernel_size": [3, 3], "name": "conv2d_690", "padding": "same", "strides": [1, 1], "trainable": true, "use_bias": true}}', 'layer7': '{"class_name": "MaxPooling2D", "config": {"data_format": "channels_last", "name": "max_pooling2d_296", "padding": "valid", "pool_size": [2, 2], "strides": [2, 2], "trainable": true}}', 'layer8': '{"class_name": "BatchNormalization", "config": {"axis": -1, "beta_constraint": null, "beta_initializer": {"class_name": "Zeros", "config": {}}, "beta_regularizer": null, "center": true, "epsilon": 0.001, "gamma_constraint": null, "gamma_initializer": {"class_name": "Ones", "config": {}}, "gamma_regularizer": null, "momentum": 0.99, "moving_mean_initializer": {"class_name": "Zeros", "config": {}}, "moving_variance_initializer": {"class_name": "Ones", "config": {}}, "name": "batch_normalization_296", "scale": true, "trainable": true}}', 'layer9': '{"class_name": "Conv2D", "config": {"activation": "relu", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "data_format": "channels_last", "dilation_rate": [1, 1], "filters": 256, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "kernel_size": [3, 3], "name": "conv2d_691", "padding": "same", "strides": [1, 1], "trainable": true, "use_bias": true}}', 'verbose': '2'}, {'batch_size': '32', 'build_fn': '{"oml-python:serialized_object": "function", "value": "__main__.vggnet_fmnist_fmnist_5_32_True"}', 'epochs': '5', 'layer0': '{"class_name": "Reshape", "config": {"batch_input_shape": [null, 784], "dtype": "float32", "name": "reshape_99", "target_shape": [28, 28, 1], "trainable": true}}', 'layer1': '{"class_name": "Conv2D", "config": {"activation": "relu", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "data_format": "channels_last", "dilation_rate": [1, 1], "filters": 64, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "kernel_size": [3, 3], "name": "conv2d_687", "padding": "same", "strides": [1, 1], "trainable": true, "use_bias": true}}', 'layer10': '{"class_name": "Conv2D", "config": {"activation": "relu", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "data_format": "channels_last", "dilation_rate": [1, 1], "filters": 256, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "kernel_size": [3, 3], "name": "conv2d_692", "padding": "same", "strides": [1, 1], "trainable": true, "use_bias": true}}', 'layer11': '{"class_name": "Conv2D", "config": {"activation": "relu", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "data_format": "channels_last", "dilation_rate": [1, 1], "filters": 256, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "kernel_size": [3, 3], "name": "conv2d_693", "padding": "same", "strides": [1, 1], "trainable": true, "use_bias": true}}', 'layer12': '{"class_name": "MaxPooling2D", "config": {"data_format": "channels_last", "name": "max_pooling2d_297", "padding": "valid", "pool_size": [2, 2], "strides": [2, 2], "trainable": true}}', 'layer13': '{"class_name": "BatchNormalization", "config": {"axis": -1, "beta_constraint": null, "beta_initializer": {"class_name": "Zeros", "config": {}}, "beta_regularizer": null, "center": true, "epsilon": 0.001, "gamma_constraint": null, "gamma_initializer": {"class_name": "Ones", "config": {}}, "gamma_regularizer": null, "momentum": 0.99, "moving_mean_initializer": {"class_name": "Zeros", "config": {}}, "moving_variance_initializer": {"class_name": "Ones", "config": {}}, "name": "batch_normalization_297", "scale": true, "trainable": true}}', 'layer14': '{"class_name": "Flatten", "config": {"data_format": "channels_last", "name": "flatten_99", "trainable": true}}', 'layer15': '{"class_name": "Dense", "config": {"activation": "relu", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "name": "dense_295", "trainable": true, "units": 4096, "use_bias": true}}', 'layer16': '{"class_name": "Dropout", "config": {"name": "dropout_197", "noise_shape": null, "rate": 0.5, "seed": null, "trainable": true}}', 'layer17': '{"class_name": "Dense", "config": {"activation": "relu", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "name": "dense_296", "trainable": true, "units": 4096, "use_bias": true}}', 'layer18': '{"class_name": "Dropout", "config": {"name": "dropout_198", "noise_shape": null, "rate": 0.5, "seed": null, "trainable": true}}', 'layer19': '{"class_name": "Dense", "config": {"activation": "softmax", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "name": "dense_297", "trainable": true, "units": 10, "use_bias": true}}', 'layer2': '{"class_name": "Conv2D", "config": {"activation": "relu", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "data_format": "channels_last", "dilation_rate": [1, 1], "filters": 64, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "kernel_size": [3, 3], "name": "conv2d_688", "padding": "same", "strides": [1, 1], "trainable": true, "use_bias": true}}', 'layer3': '{"class_name": "MaxPooling2D", "config": {"data_format": "channels_last", "name": "max_pooling2d_295", "padding": "valid", "pool_size": [2, 2], "strides": [2, 2], "trainable": true}}', 'layer4': '{"class_name": "BatchNormalization", "config": {"axis": -1, "beta_constraint": null, "beta_initializer": {"class_name": "Zeros", "config": {}}, "beta_regularizer": null, "center": true, "epsilon": 0.001, "gamma_constraint": null, "gamma_initializer": {"class_name": "Ones", "config": {}}, "gamma_regularizer": null, "momentum": 0.99, "moving_mean_initializer": {"class_name": "Zeros", "config": {}}, "moving_variance_initializer": {"class_name": "Ones", "config": {}}, "name": "batch_normalization_295", "scale": true, "trainable": true}}', 'layer5': '{"class_name": "Conv2D", "config": {"activation": "relu", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "data_format": "channels_last", "dilation_rate": [1, 1], "filters": 128, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "kernel_size": [3, 3], "name": "conv2d_689", "padding": "same", "strides": [1, 1], "trainable": true, "use_bias": true}}', 'layer6': '{"class_name": "Conv2D", "config": {"activation": "relu", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "data_format": "channels_last", "dilation_rate": [1, 1], "filters": 128, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "kernel_size": [3, 3], "name": "conv2d_690", "padding": "same", "strides": [1, 1], "trainable": true, "use_bias": true}}', 'layer7': '{"class_name": "MaxPooling2D", "config": {"data_format": "channels_last", "name": "max_pooling2d_296", "padding": "valid", "pool_size": [2, 2], "strides": [2, 2], "trainable": true}}', 'layer8': '{"class_name": "BatchNormalization", "config": {"axis": -1, "beta_constraint": null, "beta_initializer": {"class_name": "Zeros", "config": {}}, "beta_regularizer": null, "center": true, "epsilon": 0.001, "gamma_constraint": null, "gamma_initializer": {"class_name": "Ones", "config": {}}, "gamma_regularizer": null, "momentum": 0.99, "moving_mean_initializer": {"class_name": "Zeros", "config": {}}, "moving_variance_initializer": {"class_name": "Ones", "config": {}}, "name": "batch_normalization_296", "scale": true, "trainable": true}}', 'layer9': '{"class_name": "Conv2D", "config": {"activation": "relu", "activity_regularizer": null, "bias_constraint": null, "bias_initializer": {"class_name": "Zeros", "config": {}}, "bias_regularizer": null, "data_format": "channels_last", "dilation_rate": [1, 1], "filters": 256, "kernel_constraint": null, "kernel_initializer": {"class_name": "VarianceScaling", "config": {"distribution": "uniform", "mode": "fan_avg", "scale": 1.0, "seed": null}}, "kernel_regularizer": null, "kernel_size": [3, 3], "name": "conv2d_691", "padding": "same", "strides": [1, 1], "trainable": true, "use_bias": true}}', 'verbose': '2'}]
            
            Based on the following SMAC documentation, analyze the dataset characteristics and choose appropriate:
            1. Facade type (e.g., MultiFidelityFacade for multi-fidelity optimization)
            2. Budget settings (min_budget and max_budget)
            3. Number of workers (n_workers)
            4. Other relevant scenario parameters

            SMAC Documentation:
            Getting Started
#
SMAC needs four core components (configuration space, target function, scenario and a facade) to run an
optimization process, all of which are explained on this page.
They interact in the following way:
Interaction of SMAC's components
Configuration Space
#
The configuration space defines the search space of the hyperparameters and, therefore, the tunable parameters' legal
ranges and default values.
from
ConfigSpace
import
ConfigSpace
cs
=
ConfigurationSpace
({
"myfloat"
:
(
0.1
,
1.5
),
# Uniform Float
"myint"
:
(
2
,
10
),
# Uniform Integer
"species"
:
[
"mouse"
,
"cat"
,
"dog"
],
# Categorical
})
Please see the documentation of
ConfigurationSpace
for more details.
Target Function
#
The target function takes a configuration from the configuration space and returns a performance value.
For example, you could use a Neural Network to predict on your data and get some validation performance.
If, for instance, you would tune the learning rate of the Network's optimizer, every learning rate will
change the final validation performance of the network. This is the target function.
SMAC tries to find the best performing learning rate by trying different values and evaluating the target function -
in an efficient way.
def
train
(
self
,
config
:
Configuration
,
seed
:
int
)
->
float
:
model
=
MultiLayerPerceptron
(
learning_rate
=
config
[
"learning_rate"
])
model
.
fit
(
...
)
accuracy
=
model
.
validate
(
...
)
return
1
-
accuracy
# SMAC always minimizes (the smaller the better)
Note
In general, the arguments of the target function depend on the intensifier. However,
in all cases, the first argument must be the configuration (arbitrary argument name is possible here) and a seed.
If you specified instances in the scenario, SMAC requires
instance
as argument additionally. If you use
SuccessiveHalving
or
Hyperband
as intensifier but you did not specify instances, SMAC passes
budget
as
argument to the target function. But don't worry: SMAC will tell you if something is missing or if something is not
used.
Warning
SMAC
always
minimizes the value returned from the target function.
Warning
SMAC passes either
instance
or
budget
to the target function but never both.
Scenario
#
The
Scenario
is used to provide environment variables. For example, 
if you want to limit the optimization process by a time limit or want to specify where to save the results.
from
smac
import
Scenario
scenario
=
Scenario
(
configspace
=
cs
,
name
=
"experiment_name"
,
output_directory
=
Path
(
"your_output_directory"
)
walltime_limit
=
120
,
# Limit to two minutes
n_trials
=
500
,
# Evaluated max 500 trials
n_workers
=
8
,
# Use eight workers
...
)
Note
If no
name
is given, a hash of the experiment is used. Running the same experiment again at a later time will result in exactly the same hash. This is important, because the optimization will warmstart on the preexisting evaluations, if not otherwise specified in the
Facade
.
Facade
#
Warn
By default Facades will try to warmstart on preexisting logs. This behavior can be specified using the
overwrite
parameter.
A
facade
is the entry point to SMAC, which constructs a default optimization 
pipeline for you. SMAC offers various facades, which satisfy many common use cases and are crucial to
achieving peak performance. The idea behind the facades is to provide a simple interface to all of SMAC's components,
which is easy to use and understand and without the need of deep diving into the material. However, experts are
invited to change the components to their specific hyperparameter optimization needs. The following
table (horizontally scrollable) shows you what is supported and reveals the default
components
:
Black-Box
Hyperparameter Optimization
Multi-Fidelity
Algorithm Configuration
Random
Hyperband
#Parameters
low
low/medium/high
low/medium/high
low/medium/high
low/medium/high
low/medium/high
Supports Instances
❌
✅
✅
✅
✅
✅
Supports Multi-Fidelity
❌
❌
✅
✅
❌
✅
Initial Design
Sobol
Sobol
Random
Default
Default
Default
Surrogate Model
Gaussian Process
Random Forest
Random Forest
Random Forest
Not used
Not used
Acquisition Function
Expected Improvement
Log Expected Improvement
Log Expected Improvement
Expected Improvement
Not used
Not used
Acquisition Maximizer
Local and Sorted Random Search
Local and Sorted Random Search
Local and Sorted Random Search
Local and Sorted Random Search
Not Used
Not Used
Intensifier
Default
Default
Hyperband
Default
Default
Hyperband
Runhistory Encoder
Default
Log
Log
Default
Default
Default
Random Design Probability
8.5%
20%
20%
50%
Not used
Not used
Info
The multi-fidelity facade is the closest implementation to
BOHB
.
Note
We want to emphasize that SMAC is a highly modular optimization framework.
The facade accepts many arguments to specify components of the pipeline. Please also note, that in contrast
to previous versions, instantiated objects are passed instead of
kwargs
.
The facades can be imported directly from the
smac
module.
from
smac
import
BlackBoxFacade
as
BBFacade
from
smac
import
HyperparameterOptimizationFacade
as
HPOFacade
from
smac
import
MultiFidelityFacade
as
MFFacade
from
smac
import
AlgorithmConfigurationFacade
as
ACFacade
from
smac
import
RandomFacade
as
RFacade
from
smac
import
HyperbandFacade
as
HBFacade
smac
=
HPOFacade
(
scenario
=
scenario
,
target_function
=
train
)
smac
=
MFFacade
(
scenario
=
scenario
,
target_function
=
train
)
smac
=
ACFacade
(
scenario
=
scenario
,
target_function
=
train
)
smac
=
RFacade
(
scenario
=
scenario
,
target_function
=
train
)
smac
=
HBFacade
(
scenario
=
scenario
,
target_function
=
train
)

Multi-Fidelity Optimization
#
Multi-fidelity refers to running an algorithm on multiple budgets (such as number of epochs or
subsets of data) and thereby evaluating the performance prematurely. You can run a multi-fidelity optimization
when using
Successive Halving
or
Hyperband
.
Hyperband
is the default intensifier in the
multi-fidelity facade
and requires the arguments
min_budget
and
max_budget
in the scenario if no instances are used.
In general, multi-fidelity works for both real-valued and instance budgets. In the real-valued case,
the budget is directly passed to the target function. In the instance case, the budget is not passed to the 
target function but
min_budget
and
max_budget
are used internally to determine the number of instances of 
each stage. That's also the reason why
min_budget
and
max_budget
are
not required
when using instances: 
The
max_budget
is simply the max number of instances, whereas the
min_budget
is simply 1.
Warning
smac.main.config_selector.ConfigSelector
contains the
min_trials
parameter. This parameter determines
how many samples are required to train the surrogate model. If budgets are involved, the highest budgets 
are checked first. For example, if min_trials is three, but we find only two trials in the runhistory for
the highest budget, we will use trials of a lower budget instead.
Please have a look into our
multi-fidelity examples
to see how to use
multi-fidelity optimization in real-world applications.

Components
#
In addition to the basic components mentioned in
Getting Started
, all other components are
explained in the following paragraphs to give a better picture of SMAC. These components are all used to guide
the optimization process and simple changes can influence the results drastically.
Before diving into the components, we shortly want to explain the main Bayesian optimization loop in SMAC.
The
SMBO
receives all instantiated components from the facade and the logic happens here.
In general, a while loop is used to ask for the next trial, submit it to the runner, and wait for the runner to 
finish the evaluation. Since the runner and the
SMBO
object are decoupled, the while loop continues and asks for even 
more trials (e.g., in case of multi-threading), which also can be submitted to the runner. If all workers are
occupied, SMAC will wait until a new worker is available again. Moreover, limitations like wallclock time and remaining 
trials are checked in every iteration.
Surrogate Model
#
The surrogate model is used to approximate the objective function of configurations. In previous versions, the model was 
referred to as the Empirical Performance Model (EPM). Mostly, Bayesian optimization is used/associated with Gaussian
processes. However, SMAC also incorporates random forests as surrogate models, which makes it possible to optimize for 
higher dimensional and complex spaces.
The data used to train the surrogate model is collected by the runhistory encoder (receives data from the runhistory 
and transforms it). If budgets are
involved, the highest budget which satisfies
min_trials
(defaults to 1) in
smac.main.config_selector
is
used. If no budgets are used, all observations are used.
If you are using instances, it is recommended to use instance features. The model is trained on each instance 
associated with its features. Imagine you have two hyperparameters, two instances and no instance features, the model 
would be trained on:
HP 1
HP 2
Objective Value
0.1
0.8
0.5
0.1
0.8
0.75
505
7
2.4
505
7
1.3
You can see that the same inputs lead to different objective values because of two instances. If you associate
each instance with a feature, you would end-up with the following data points:
HP 1
HP 2
Instance Feature
Objective Value
0.1
0.8
0
0.5
0.1
0.8
1
0.75
505
7
0
2.4
505
7
1
1.3
The steps to receiving data are as follows:
The intensifier requests new configurations via
next(self.config_generator)
.
The config selector collects the data via the runhistory encoder which iterates over the runhistory trials.
The runhistory encoder only collects trials which are in
considered_states
and timeout trials. Also, only the
   highest budget is considered if budgets are used. In this step, multi-objective values are scalarized using the
normalize_costs
function (uses
objective_bounds
from the runhistory) and the multi-objective algorithm.
   For example, when ParEGO is used, the scalarization would be different in each training.
The selected trial objectives are transformed (e.g., log-transformed, depending on the selected
   encoder).
The hyperparameters might still have inactive values. The model takes care of that after the collected data
   are passed to the model.
Acquisition Function
#
Acquisition functions are mathematical techniques that guide how the parameter space should be explored during Bayesian 
optimization. They use the predicted mean and predicted variance generated by the surrogate model.
The acquisition function is used by the acquisition maximizer (see next section). Otherwise, SMAC provides
a bunch of different acquisition functions (Lower Confidence Bound, Expected Improvement, Probability Improvement, 
Thompson, integrated acquisition functions and prior acquisition functions). We refer to literature 
for more information about acquisition functions.
Note
The acquisition function calculates the acquisition value for each configuration. However, the configurations
are provided by the acquisition maximizer. Therefore, the acquisition maximizer is responsible for receiving
the next configurations.
Acquisition Maximize
#
The acquisition maximizer is a wrapper for the acquisition function. It returns the next configurations. SMAC
supports local search, (sorted) random search, local and (sorted) random search, and differential evolution.
While local search checks neighbours of the best configurations, random search makes sure to explore the configuration
space. When using sorted random search, random configurations are sorted by the value of the acquisition function.
Warning
Pay attention to the number of challengers: If you experience RAM issues or long computational times in the
acquisition function, you might lower the number of challengers.
The acquisition maximizer also incorporates the
Random Design
. Please see the
ChallengerList
for more information.
Initial Design
#
The surrogate model needs data to be trained. Therefore, the initial design is used to generate the initial data points.
We provide random, latin hypercube, sobol, factorial and default initial designs. The default initial design uses
the default configuration from the configuration space and with the factorial initial design, we generate corner
points of the configuration space. The sobol sequences are an example of quasi-random low-discrepancy sequences and
the latin hypercube design is a statistical method for generating a near-random sample of parameter values from
a multidimensional distribution.
The initial design configurations are yielded by the config selector first. Moreover, the config selector keeps
track of which configurations already have been returned to make sure a configuration is not returned twice.
Random Design
#
The random design is used in the acquisition maximizer to tell whether the next configuration should be
random or sampled from the acquisition function. For example, if we use a random design with a probability of 
50%, we have a 50% chance to sample a random configuration and a 50% chance to sample a configuration from the
acquisition function (although the acquisition function includes exploration and exploitation trade-off already). 
This design makes sure that the optimization process is not stuck in a local optimum and we 
are
guaranteed
to find the best configuration over time.
In addition to simple probability random design, we also provide annealing and modulus random design.
Intensifier
#
The intensifier compares different configurations based on evaluated :term:
trial<Trial>
so far. It decides
which configuration should be
intensified
or, in other words, if a configuration is worth to spend more time on (e.g.,
evaluate another seed pair, evaluate on another instance, or evaluate on a higher budget).
Warning
Always pay attention to
max_config_calls
or
n_seeds
: If this argument is set high, the intensifier might 
spend a lot of time on a single configuration.
Depending on the components and arguments, the intensifier tells you which seeds, budgets, and/or instances
are used throughout the optimization process. You can use the methods
uses_seeds
,
uses_budgets
, and
uses_instances
(directly callable via the facade) to (sanity-)check whether the intensifier uses these arguments.
Another important fact is that the intensifier keeps track of the current incumbent (a.k.a. the best configuration 
found so far). In case of multi-objective, multiple incumbents could be found.
All intensifiers support multi-objective, multi-fidelity, and multi-threading:
Multi-Objective: Keeping track of multiple incumbents at once.
Multi-Fidelity: Incorporating instances or budgets.
Multi-Threading: Intensifier are implemented as generators so that calling
next
on the intensifier can be
  repeated as often as needed. Intensifier are not required to receive results as the results are directly taken from
  the runhistory.
Note
All intensifiers are working on the runhistory and recognize previous logged trials (e.g., if the user already
evaluated something beforehand). Previous configurations (in the best case, also complete trials) are added to the 
queue/tracker again so that they are integrated into the intensification process.
That means continuing a run as well as incorporating user inputs are natively supported.
Configuration Selector
#
The configuration selector uses the initial design, surrogate model, acquisition maximizer/function, runhistory,
runhistory encoder, and random design to select the next configuration. The configuration selector is directly
used by the intensifier and is called everytime a new configuration is requested.
The idea behind the configuration selector is straight forward:
Yield the initial design configurations.
Train the surrogate model with the data from the runhistory encoder.
Get the next
retrain_after
configurations from the acquisition function/maximizer and yield them.
After all
retrain_after
configurations were yield, go back to step 2.
Note
The configuration selector is a generator and yields configurations. Therefore, the current state of the 
selector is saved and when the intensifier calls
next
, the selector continues there where it stopped.
Note
Everytime the surrogate model is trained, the multi-objective algorithm is updated via
update_on_iteration_start
.
Multi-Objective Algorithm
#
The multi-objective algorithm is used to scalarize multi-objective values. The multi-objective algorithm 
gets normalized objective values passed and returns a single value. The resulting value (called by the 
runhistory encoder) is then used to train the surrogate model.
Warning
Depending on the multi-objective algorithm, the values for the runhistory encoder might differ each time 
the surrogate model is trained. Let's take ParEGO for example:
Everytime a new configuration is sampled (see ConfigSelector), the objective weights are updated. Therefore,
the scalarized values are different and the acquisition maximizer might return completely different configurations.
RunHistory
#
The runhistory holds all (un-)evaluated trials of the optimization run. You can use the runhistory to 
get (running) configs, (running) trials, trials of a specific config, and more.
The runhistory encoder iterates over the runhistory to receive data for the surrogate model. The following 
code shows how to iterate over the runhistory:
smac
=
HPOFacade
(
...
)
# Iterate over all trials
for
trial_info
,
trial_value
in
smac
.
runhistory
.
items
():
# Trial info
config
=
trial_info
.
config
instance
=
trial_info
.
instance
budget
=
trial_info
.
budget
seed
=
trial_info
.
seed
# Trial value
cost
=
trial_value
.
cost
time
=
trial_value
.
time
status
=
trial_value
.
status
starttime
=
trial_value
.
starttime
endtime
=
trial_value
.
endtime
additional_info
=
trial_value
.
additional_info
# Iterate over all configs
for
config
in
smac
.
runhistory
.
get_configs
():
# Get the cost of all trials of this config
average_cost
=
smac
.
runhistory
.
average_cost
(
config
)
Warning
The intensifier uses a callback to update the incumbent everytime a new trial is added to the runhistory.
RunHistory Encoder
#
The runhistory encoder is used to encode the runhistory data into a format that can be used by the surrogate model.
Only trials with the status
considered_states
and timeout trials are considered. Multi-objective values are 
scalarized using the
normalize_costs
function (uses
objective_bounds
from the runhistory). Afterwards, the 
normalized value is processed by the multi-objective algorithm.
Callback
#
Callbacks provide the ability to easily execute code before, inside, and after the Bayesian optimization loop.
To add a callback, you have to inherit from
smac.Callback
and overwrite the methods (if needed).
Afterwards, you can pass the callbacks to any facade.
from
smac
import
MultiFidelityFacade
,
Callback
class
CustomCallback
(
Callback
):
def
on_start
(
self
,
smbo
:
SMBO
)
->
None
:
pass
def
on_end
(
self
,
smbo
:
SMBO
)
->
None
:
pass
def
on_iteration_start
(
self
,
smbo
:
SMBO
)
->
None
:
pass
def
on_iteration_end
(
self
,
smbo
:
SMBO
,
info
:
RunInfo
,
value
:
RunValue
)
->
bool
|
None
:
# We just do a simple printing here
print
(
info
,
value
)
smac
=
MultiFidelityFacade
(
...
callbacks
=
[
CustomCallback
()]
)
smac
.
optimize
()
            Please analyze the dataset and documentation to determine:
            1. Should multi-fidelity optimization be used? (Consider dataset size and training time)
            2. What budget range is appropriate? (Consider training epochs or data subsets)
            3. How many workers should be used? (Consider available resources)
            4. Are there any special considerations for this dataset type?

            Then generate a scenario configuration that best suits this dataset.
            
--------------------------------------------------------------------------------
