[2025-06-15 18:53:20] [Metadata: {'component': 'config'}] ```python
from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, Constant
from ConfigSpace.conditions import InCondition
from ConfigSpace.hyperparameters import UnParametrizedHyperparameter

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization of a CNN
    for image classification.  The dataset is assumed to be a grayscale image
    dataset with 10 classes and images of size 28x28, similar to Fashion MNIST.
    """

    cs = ConfigurationSpace()

    # Model type: CNN or MLP
    model_type = CategoricalHyperparameter(
        "model_type", choices=["cnn", "mlp"], default_value="cnn"
    )
    cs.add_hyperparameter(model_type)

    # Batch size
    batch_size = CategoricalHyperparameter(
        "batch_size", choices=[32, 64, 128, 256], default_value=128
    )
    cs.add_hyperparameter(batch_size)

    # Optimizer
    optimizer = CategoricalHyperparameter(
        "optimizer", choices=["adam", "sgd"], default_value="adam"
    )
    cs.add_hyperparameter(optimizer)

    # Learning rate
    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-5, upper=1e-2, default_value=1e-3, log=True
    )
    cs.add_hyperparameter(learning_rate)

    # SGD-specific hyperparameters
    sgd_momentum = UniformFloatHyperparameter(
        "sgd_momentum", lower=0.0, upper=0.9, default_value=0.0
    )
    cs.add_hyperparameter(sgd_momentum)

    # Condition for SGD momentum
    momentum_condition = InCondition(sgd_momentum, optimizer, ["sgd"])
    cs.add_condition(momentum_condition)

    # CNN specific hyperparameters
    num_conv_layers = UniformIntegerHyperparameter(
        "num_conv_layers", lower=1, upper=3, default_value=2
    )
    cs.add_hyperparameter(num_conv_layers)

    # Number of filters for conv layers
    conv_filters_1 = CategoricalHyperparameter(
        "conv_filters_1", choices=[32, 64, 128], default_value=64
    )
    cs.add_hyperparameter(conv_filters_1)

    conv_filters_2 = CategoricalHyperparameter(
        "conv_filters_2", choices=[64, 128, 256], default_value=128
    )
    cs.add_hyperparameter(conv_filters_2)

    conv_filters_3 = CategoricalHyperparameter(
        "conv_filters_3", choices=[128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(conv_filters_3)

    # Kernel size for conv layers
    kernel_size = CategoricalHyperparameter(
        "kernel_size", choices=[3, 5], default_value=3
    )
    cs.add_hyperparameter(kernel_size)

    # Pooling type
    pooling_type = CategoricalHyperparameter(
        "pooling_type", choices=["max", "avg"], default_value="max"
    )
    cs.add_hyperparameter(pooling_type)

    # Number of dense layers
    num_dense_layers = UniformIntegerHyperparameter(
        "num_dense_layers", lower=1, upper=2, default_value=1
    )
    cs.add_hyperparameter(num_dense_layers)

    # Size of dense layers
    dense_size_1 = CategoricalHyperparameter(
        "dense_size_1", choices=[128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(dense_size_1)

    dense_size_2 = CategoricalHyperparameter(
        "dense_size_2", choices=[256, 512, 1024], default_value=512
    )
    cs.add_hyperparameter(dense_size_2)

    # Dropout rate
    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate", lower=0.0, upper=0.5, default_value=0.2
    )
    cs.add_hyperparameter(dropout_rate)

    # Normalization Strategy
    normalization_strategy = CategoricalHyperparameter(
        "normalization_strategy", choices=["none", "batchnorm", "layernorm"], default_value="batchnorm"
    )
    cs.add_hyperparameter(normalization_strategy)

    # CNN Condition
    cnn_condition = InCondition(child=num_conv_layers, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition)

    cnn_condition_filters1 = InCondition(child=conv_filters_1, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters1)

    cnn_condition_filters2 = InCondition(child=conv_filters_2, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters2)

    cnn_condition_filters3 = InCondition(child=conv_filters_3, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters3)

    cnn_condition_kernel = InCondition(child=kernel_size, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_kernel)

    cnn_condition_pool = InCondition(child=pooling_type, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_pool)

    dense_condition_1 = InCondition(child=dense_size_1, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_1)

    dense_condition_2 = InCondition(child=dense_size_2, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_2)

    dense_condition_layers = InCondition(child=num_dense_layers, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_layers)

    drop_condition = InCondition(child=dropout_rate, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(drop_condition)

    norm_condition = InCondition(child=normalization_strategy, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(norm_condition)


    # Conditions for number of conv layers
    conv_filters_2_condition = InCondition(conv_filters_2, num_conv_layers, [2, 3])
    cs.add_condition(conv_filters_2_condition)

    conv_filters_3_condition = InCondition(conv_filters_3, num_conv_layers, [3])
    cs.add_condition(conv_filters_3_condition)

    return cs
```
--------------------------------------------------------------------------------
[2025-06-15 18:53:20] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, Constant
from ConfigSpace.conditions import InCondition
from ConfigSpace.hyperparameters import UnParametrizedHyperparameter

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization of a CNN
    for image classification.  The dataset is assumed to be a grayscale image
    dataset with 10 classes and images of size 28x28, similar to Fashion MNIST.
    """

    cs = ConfigurationSpace()

    # Model type: CNN or MLP
    model_type = CategoricalHyperparameter(
        "model_type", choices=["cnn", "mlp"], default_value="cnn"
    )
    cs.add_hyperparameter(model_type)

    # Batch size
    batch_size = CategoricalHyperparameter(
        "batch_size", choices=[32, 64, 128, 256], default_value=128
    )
    cs.add_hyperparameter(batch_size)

    # Optimizer
    optimizer = CategoricalHyperparameter(
        "optimizer", choices=["adam", "sgd"], default_value="adam"
    )
    cs.add_hyperparameter(optimizer)

    # Learning rate
    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-5, upper=1e-2, default_value=1e-3, log=True
    )
    cs.add_hyperparameter(learning_rate)

    # SGD-specific hyperparameters
    sgd_momentum = UniformFloatHyperparameter(
        "sgd_momentum", lower=0.0, upper=0.9, default_value=0.0
    )
    cs.add_hyperparameter(sgd_momentum)

    # Condition for SGD momentum
    momentum_condition = InCondition(sgd_momentum, optimizer, ["sgd"])
    cs.add_condition(momentum_condition)

    # CNN specific hyperparameters
    num_conv_layers = UniformIntegerHyperparameter(
        "num_conv_layers", lower=1, upper=3, default_value=2
    )
    cs.add_hyperparameter(num_conv_layers)

    # Number of filters for conv layers
    conv_filters_1 = CategoricalHyperparameter(
        "conv_filters_1", choices=[32, 64, 128], default_value=64
    )
    cs.add_hyperparameter(conv_filters_1)

    conv_filters_2 = CategoricalHyperparameter(
        "conv_filters_2", choices=[64, 128, 256], default_value=128
    )
    cs.add_hyperparameter(conv_filters_2)

    conv_filters_3 = CategoricalHyperparameter(
        "conv_filters_3", choices=[128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(conv_filters_3)

    # Kernel size for conv layers
    kernel_size = CategoricalHyperparameter(
        "kernel_size", choices=[3, 5], default_value=3
    )
    cs.add_hyperparameter(kernel_size)

    # Pooling type
    pooling_type = CategoricalHyperparameter(
        "pooling_type", choices=["max", "avg"], default_value="max"
    )
    cs.add_hyperparameter(pooling_type)

    # Number of dense layers
    num_dense_layers = UniformIntegerHyperparameter(
        "num_dense_layers", lower=1, upper=2, default_value=1
    )
    cs.add_hyperparameter(num_dense_layers)

    # Size of dense layers
    dense_size_1 = CategoricalHyperparameter(
        "dense_size_1", choices=[128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(dense_size_1)

    dense_size_2 = CategoricalHyperparameter(
        "dense_size_2", choices=[256, 512, 1024], default_value=512
    )
    cs.add_hyperparameter(dense_size_2)

    # Dropout rate
    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate", lower=0.0, upper=0.5, default_value=0.2
    )
    cs.add_hyperparameter(dropout_rate)

    # Normalization Strategy
    normalization_strategy = CategoricalHyperparameter(
        "normalization_strategy", choices=["none", "batchnorm", "layernorm"], default_value="batchnorm"
    )
    cs.add_hyperparameter(normalization_strategy)

    # CNN Condition
    cnn_condition = InCondition(child=num_conv_layers, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition)

    cnn_condition_filters1 = InCondition(child=conv_filters_1, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters1)

    cnn_condition_filters2 = InCondition(child=conv_filters_2, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters2)

    cnn_condition_filters3 = InCondition(child=conv_filters_3, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters3)

    cnn_condition_kernel = InCondition(child=kernel_size, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_kernel)

    cnn_condition_pool = InCondition(child=pooling_type, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_pool)

    dense_condition_1 = InCondition(child=dense_size_1, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_1)

    dense_condition_2 = InCondition(child=dense_size_2, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_2)

    dense_condition_layers = InCondition(child=num_dense_layers, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_layers)

    drop_condition = InCondition(child=dropout_rate, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(drop_condition)

    norm_condition = InCondition(child=normalization_strategy, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(norm_condition)


    # Conditions for number of conv layers
    conv_filters_2_condition = InCondition(conv_filters_2, num_conv_layers, [2, 3])
    cs.add_condition(conv_filters_2_condition)

    conv_filters_3_condition = InCondition(conv_filters_3, num_conv_layers, [3])
    cs.add_condition(conv_filters_3_condition)

    return cs

--------------------------------------------------------------------------------
[2025-06-15 18:53:20] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#1): Adding a second parent condition for a for a hyperparameter is ambiguous and therefore forbidden. Use an `OrConjunction` or `AndConjunction` to combine conditions instead.
Already inserted: conv_filters_2 | model_type in {'cnn'}
New one: conv_filters_2 | num_conv_layers in {2, 3}
--------------------------------------------------------------------------------
[2025-06-15 18:53:28] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, Constant
from ConfigSpace.conditions import InCondition, AndConjunction
from ConfigSpace.hyperparameters import UnParametrizedHyperparameter

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization of a CNN
    for image classification.  The dataset is assumed to be a grayscale image
    dataset with 10 classes and images of size 28x28, similar to Fashion MNIST.
    """

    cs = ConfigurationSpace()

    # Model type: CNN or MLP
    model_type = CategoricalHyperparameter(
        "model_type", choices=["cnn", "mlp"], default_value="cnn"
    )
    cs.add_hyperparameter(model_type)

    # Batch size
    batch_size = CategoricalHyperparameter(
        "batch_size", choices=[32, 64, 128, 256], default_value=128
    )
    cs.add_hyperparameter(batch_size)

    # Optimizer
    optimizer = CategoricalHyperparameter(
        "optimizer", choices=["adam", "sgd"], default_value="adam"
    )
    cs.add_hyperparameter(optimizer)

    # Learning rate
    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-5, upper=1e-2, default_value=1e-3, log=True
    )
    cs.add_hyperparameter(learning_rate)

    # SGD-specific hyperparameters
    sgd_momentum = UniformFloatHyperparameter(
        "sgd_momentum", lower=0.0, upper=0.9, default_value=0.0
    )
    cs.add_hyperparameter(sgd_momentum)

    # Condition for SGD momentum
    momentum_condition = InCondition(sgd_momentum, optimizer, ["sgd"])
    cs.add_condition(momentum_condition)

    # CNN specific hyperparameters
    num_conv_layers = UniformIntegerHyperparameter(
        "num_conv_layers", lower=1, upper=3, default_value=2
    )
    cs.add_hyperparameter(num_conv_layers)

    # Number of filters for conv layers
    conv_filters_1 = CategoricalHyperparameter(
        "conv_filters_1", choices=[32, 64, 128], default_value=64
    )
    cs.add_hyperparameter(conv_filters_1)

    conv_filters_2 = CategoricalHyperparameter(
        "conv_filters_2", choices=[64, 128, 256], default_value=128
    )
    cs.add_hyperparameter(conv_filters_2)

    conv_filters_3 = CategoricalHyperparameter(
        "conv_filters_3", choices=[128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(conv_filters_3)

    # Kernel size for conv layers
    kernel_size = CategoricalHyperparameter(
        "kernel_size", choices=[3, 5], default_value=3
    )
    cs.add_hyperparameter(kernel_size)

    # Pooling type
    pooling_type = CategoricalHyperparameter(
        "pooling_type", choices=["max", "avg"], default_value="max"
    )
    cs.add_hyperparameter(pooling_type)

    # Number of dense layers
    num_dense_layers = UniformIntegerHyperparameter(
        "num_dense_layers", lower=1, upper=2, default_value=1
    )
    cs.add_hyperparameter(num_dense_layers)

    # Size of dense layers
    dense_size_1 = CategoricalHyperparameter(
        "dense_size_1", choices=[128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(dense_size_1)

    dense_size_2 = CategoricalHyperparameter(
        "dense_size_2", choices=[256, 512, 1024], default_value=512
    )
    cs.add_hyperparameter(dense_size_2)

    # Dropout rate
    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate", lower=0.0, upper=0.5, default_value=0.2
    )
    cs.add_hyperparameter(dropout_rate)

    # Normalization Strategy
    normalization_strategy = CategoricalHyperparameter(
        "normalization_strategy", choices=["none", "batchnorm", "layernorm"], default_value="batchnorm"
    )
    cs.add_hyperparameter(normalization_strategy)

    # CNN Condition
    cnn_condition = InCondition(child=num_conv_layers, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition)

    cnn_condition_filters1 = InCondition(child=conv_filters_1, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters1)

    cnn_condition_filters2 = InCondition(child=conv_filters_2, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters2)

    cnn_condition_filters3 = InCondition(child=conv_filters_3, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters3)

    cnn_condition_kernel = InCondition(child=kernel_size, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_kernel)

    cnn_condition_pool = InCondition(child=pooling_type, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_pool)

    dense_condition_1 = InCondition(child=dense_size_1, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_1)

    dense_condition_2 = InCondition(child=dense_size_2, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_2)

    dense_condition_layers = InCondition(child=num_dense_layers, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_layers)

    drop_condition = InCondition(child=dropout_rate, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(drop_condition)

    norm_condition = InCondition(child=normalization_strategy, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(norm_condition)


    # Conditions for number of conv layers
    conv_filters_2_condition = InCondition(conv_filters_2, num_conv_layers, [2, 3])
    conv_filters_2_and_cnn = AndConjunction(cnn_condition_filters2, conv_filters_2_condition)
    cs.add_condition(conv_filters_2_and_cnn)

    conv_filters_3_condition = InCondition(conv_filters_3, num_conv_layers, [3])
    conv_filters_3_and_cnn = AndConjunction(cnn_condition_filters3, conv_filters_3_condition)
    cs.add_condition(conv_filters_3_and_cnn)

    return cs
```
--------------------------------------------------------------------------------
[2025-06-15 18:53:28] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, Constant
from ConfigSpace.conditions import InCondition, AndConjunction
from ConfigSpace.hyperparameters import UnParametrizedHyperparameter

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization of a CNN
    for image classification.  The dataset is assumed to be a grayscale image
    dataset with 10 classes and images of size 28x28, similar to Fashion MNIST.
    """

    cs = ConfigurationSpace()

    # Model type: CNN or MLP
    model_type = CategoricalHyperparameter(
        "model_type", choices=["cnn", "mlp"], default_value="cnn"
    )
    cs.add_hyperparameter(model_type)

    # Batch size
    batch_size = CategoricalHyperparameter(
        "batch_size", choices=[32, 64, 128, 256], default_value=128
    )
    cs.add_hyperparameter(batch_size)

    # Optimizer
    optimizer = CategoricalHyperparameter(
        "optimizer", choices=["adam", "sgd"], default_value="adam"
    )
    cs.add_hyperparameter(optimizer)

    # Learning rate
    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-5, upper=1e-2, default_value=1e-3, log=True
    )
    cs.add_hyperparameter(learning_rate)

    # SGD-specific hyperparameters
    sgd_momentum = UniformFloatHyperparameter(
        "sgd_momentum", lower=0.0, upper=0.9, default_value=0.0
    )
    cs.add_hyperparameter(sgd_momentum)

    # Condition for SGD momentum
    momentum_condition = InCondition(sgd_momentum, optimizer, ["sgd"])
    cs.add_condition(momentum_condition)

    # CNN specific hyperparameters
    num_conv_layers = UniformIntegerHyperparameter(
        "num_conv_layers", lower=1, upper=3, default_value=2
    )
    cs.add_hyperparameter(num_conv_layers)

    # Number of filters for conv layers
    conv_filters_1 = CategoricalHyperparameter(
        "conv_filters_1", choices=[32, 64, 128], default_value=64
    )
    cs.add_hyperparameter(conv_filters_1)

    conv_filters_2 = CategoricalHyperparameter(
        "conv_filters_2", choices=[64, 128, 256], default_value=128
    )
    cs.add_hyperparameter(conv_filters_2)

    conv_filters_3 = CategoricalHyperparameter(
        "conv_filters_3", choices=[128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(conv_filters_3)

    # Kernel size for conv layers
    kernel_size = CategoricalHyperparameter(
        "kernel_size", choices=[3, 5], default_value=3
    )
    cs.add_hyperparameter(kernel_size)

    # Pooling type
    pooling_type = CategoricalHyperparameter(
        "pooling_type", choices=["max", "avg"], default_value="max"
    )
    cs.add_hyperparameter(pooling_type)

    # Number of dense layers
    num_dense_layers = UniformIntegerHyperparameter(
        "num_dense_layers", lower=1, upper=2, default_value=1
    )
    cs.add_hyperparameter(num_dense_layers)

    # Size of dense layers
    dense_size_1 = CategoricalHyperparameter(
        "dense_size_1", choices=[128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(dense_size_1)

    dense_size_2 = CategoricalHyperparameter(
        "dense_size_2", choices=[256, 512, 1024], default_value=512
    )
    cs.add_hyperparameter(dense_size_2)

    # Dropout rate
    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate", lower=0.0, upper=0.5, default_value=0.2
    )
    cs.add_hyperparameter(dropout_rate)

    # Normalization Strategy
    normalization_strategy = CategoricalHyperparameter(
        "normalization_strategy", choices=["none", "batchnorm", "layernorm"], default_value="batchnorm"
    )
    cs.add_hyperparameter(normalization_strategy)

    # CNN Condition
    cnn_condition = InCondition(child=num_conv_layers, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition)

    cnn_condition_filters1 = InCondition(child=conv_filters_1, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters1)

    cnn_condition_filters2 = InCondition(child=conv_filters_2, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters2)

    cnn_condition_filters3 = InCondition(child=conv_filters_3, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters3)

    cnn_condition_kernel = InCondition(child=kernel_size, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_kernel)

    cnn_condition_pool = InCondition(child=pooling_type, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_pool)

    dense_condition_1 = InCondition(child=dense_size_1, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_1)

    dense_condition_2 = InCondition(child=dense_size_2, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_2)

    dense_condition_layers = InCondition(child=num_dense_layers, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_layers)

    drop_condition = InCondition(child=dropout_rate, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(drop_condition)

    norm_condition = InCondition(child=normalization_strategy, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(norm_condition)


    # Conditions for number of conv layers
    conv_filters_2_condition = InCondition(conv_filters_2, num_conv_layers, [2, 3])
    conv_filters_2_and_cnn = AndConjunction(cnn_condition_filters2, conv_filters_2_condition)
    cs.add_condition(conv_filters_2_and_cnn)

    conv_filters_3_condition = InCondition(conv_filters_3, num_conv_layers, [3])
    conv_filters_3_and_cnn = AndConjunction(cnn_condition_filters3, conv_filters_3_condition)
    cs.add_condition(conv_filters_3_and_cnn)

    return cs

--------------------------------------------------------------------------------
[2025-06-15 18:53:28] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#2): Adding a second parent condition for a for a hyperparameter is ambiguous and therefore forbidden. Use an `OrConjunction` or `AndConjunction` to combine conditions instead.
Already inserted: conv_filters_2 | model_type in {'cnn'}
New one: (conv_filters_2 | model_type in {'cnn'} && conv_filters_2 | num_conv_layers in {2, 3})
--------------------------------------------------------------------------------
[2025-06-15 18:53:36] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, Constant
from ConfigSpace.conditions import InCondition, AndConjunction
from ConfigSpace.hyperparameters import UnParametrizedHyperparameter

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization of a CNN
    for image classification.  The dataset is assumed to be a grayscale image
    dataset with 10 classes and images of size 28x28, similar to Fashion MNIST.
    """

    cs = ConfigurationSpace()

    # Model type: CNN or MLP
    model_type = CategoricalHyperparameter(
        "model_type", choices=["cnn", "mlp"], default_value="cnn"
    )
    cs.add_hyperparameter(model_type)

    # Batch size
    batch_size = CategoricalHyperparameter(
        "batch_size", choices=[32, 64, 128, 256], default_value=128
    )
    cs.add_hyperparameter(batch_size)

    # Optimizer
    optimizer = CategoricalHyperparameter(
        "optimizer", choices=["adam", "sgd"], default_value="adam"
    )
    cs.add_hyperparameter(optimizer)

    # Learning rate
    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-5, upper=1e-2, default_value=1e-3, log=True
    )
    cs.add_hyperparameter(learning_rate)

    # SGD-specific hyperparameters
    sgd_momentum = UniformFloatHyperparameter(
        "sgd_momentum", lower=0.0, upper=0.9, default_value=0.0
    )
    cs.add_hyperparameter(sgd_momentum)

    # Condition for SGD momentum
    momentum_condition = InCondition(sgd_momentum, optimizer, ["sgd"])
    cs.add_condition(momentum_condition)

    # CNN specific hyperparameters
    num_conv_layers = UniformIntegerHyperparameter(
        "num_conv_layers", lower=1, upper=3, default_value=2
    )
    cs.add_hyperparameter(num_conv_layers)

    # Number of filters for conv layers
    conv_filters_1 = CategoricalHyperparameter(
        "conv_filters_1", choices=[32, 64, 128], default_value=64
    )
    cs.add_hyperparameter(conv_filters_1)

    conv_filters_2 = CategoricalHyperparameter(
        "conv_filters_2", choices=[64, 128, 256], default_value=128
    )
    cs.add_hyperparameter(conv_filters_2)

    conv_filters_3 = CategoricalHyperparameter(
        "conv_filters_3", choices=[128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(conv_filters_3)

    # Kernel size for conv layers
    kernel_size = CategoricalHyperparameter(
        "kernel_size", choices=[3, 5], default_value=3
    )
    cs.add_hyperparameter(kernel_size)

    # Pooling type
    pooling_type = CategoricalHyperparameter(
        "pooling_type", choices=["max", "avg"], default_value="max"
    )
    cs.add_hyperparameter(pooling_type)

    # Number of dense layers
    num_dense_layers = UniformIntegerHyperparameter(
        "num_dense_layers", lower=1, upper=2, default_value=1
    )
    cs.add_hyperparameter(num_dense_layers)

    # Size of dense layers
    dense_size_1 = CategoricalHyperparameter(
        "dense_size_1", choices=[128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(dense_size_1)

    dense_size_2 = CategoricalHyperparameter(
        "dense_size_2", choices=[256, 512, 1024], default_value=512
    )
    cs.add_hyperparameter(dense_size_2)

    # Dropout rate
    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate", lower=0.0, upper=0.5, default_value=0.2
    )
    cs.add_hyperparameter(dropout_rate)

    # Normalization Strategy
    normalization_strategy = CategoricalHyperparameter(
        "normalization_strategy", choices=["none", "batchnorm", "layernorm"], default_value="batchnorm"
    )
    cs.add_hyperparameter(normalization_strategy)

    # CNN Condition
    cnn_condition = InCondition(child=num_conv_layers, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition)

    cnn_condition_filters1 = InCondition(child=conv_filters_1, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters1)

    cnn_condition_filters2 = InCondition(child=conv_filters_2, parent=model_type, values=['cnn'])
    #cs.add_condition(cnn_condition_filters2)

    cnn_condition_filters3 = InCondition(child=conv_filters_3, parent=model_type, values=['cnn'])
    #cs.add_condition(cnn_condition_filters3)

    cnn_condition_kernel = InCondition(child=kernel_size, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_kernel)

    cnn_condition_pool = InCondition(child=pooling_type, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_pool)

    dense_condition_1 = InCondition(child=dense_size_1, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_1)

    dense_condition_2 = InCondition(child=dense_size_2, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_2)

    dense_condition_layers = InCondition(child=num_dense_layers, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_layers)

    drop_condition = InCondition(child=dropout_rate, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(drop_condition)

    norm_condition = InCondition(child=normalization_strategy, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(norm_condition)


    # Conditions for number of conv layers
    conv_filters_2_condition = InCondition(conv_filters_2, num_conv_layers, [2, 3])
    conv_filters_2_and_cnn = AndConjunction(cnn_condition, conv_filters_2_condition)
    cs.add_condition(conv_filters_2_and_cnn)

    conv_filters_3_condition = InCondition(conv_filters_3, num_conv_layers, [3])
    conv_filters_3_and_cnn = AndConjunction(cnn_condition, conv_filters_3_condition)
    cs.add_condition(conv_filters_3_and_cnn)

    return cs
```
--------------------------------------------------------------------------------
[2025-06-15 18:53:36] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, Constant
from ConfigSpace.conditions import InCondition, AndConjunction
from ConfigSpace.hyperparameters import UnParametrizedHyperparameter

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization of a CNN
    for image classification.  The dataset is assumed to be a grayscale image
    dataset with 10 classes and images of size 28x28, similar to Fashion MNIST.
    """

    cs = ConfigurationSpace()

    # Model type: CNN or MLP
    model_type = CategoricalHyperparameter(
        "model_type", choices=["cnn", "mlp"], default_value="cnn"
    )
    cs.add_hyperparameter(model_type)

    # Batch size
    batch_size = CategoricalHyperparameter(
        "batch_size", choices=[32, 64, 128, 256], default_value=128
    )
    cs.add_hyperparameter(batch_size)

    # Optimizer
    optimizer = CategoricalHyperparameter(
        "optimizer", choices=["adam", "sgd"], default_value="adam"
    )
    cs.add_hyperparameter(optimizer)

    # Learning rate
    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-5, upper=1e-2, default_value=1e-3, log=True
    )
    cs.add_hyperparameter(learning_rate)

    # SGD-specific hyperparameters
    sgd_momentum = UniformFloatHyperparameter(
        "sgd_momentum", lower=0.0, upper=0.9, default_value=0.0
    )
    cs.add_hyperparameter(sgd_momentum)

    # Condition for SGD momentum
    momentum_condition = InCondition(sgd_momentum, optimizer, ["sgd"])
    cs.add_condition(momentum_condition)

    # CNN specific hyperparameters
    num_conv_layers = UniformIntegerHyperparameter(
        "num_conv_layers", lower=1, upper=3, default_value=2
    )
    cs.add_hyperparameter(num_conv_layers)

    # Number of filters for conv layers
    conv_filters_1 = CategoricalHyperparameter(
        "conv_filters_1", choices=[32, 64, 128], default_value=64
    )
    cs.add_hyperparameter(conv_filters_1)

    conv_filters_2 = CategoricalHyperparameter(
        "conv_filters_2", choices=[64, 128, 256], default_value=128
    )
    cs.add_hyperparameter(conv_filters_2)

    conv_filters_3 = CategoricalHyperparameter(
        "conv_filters_3", choices=[128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(conv_filters_3)

    # Kernel size for conv layers
    kernel_size = CategoricalHyperparameter(
        "kernel_size", choices=[3, 5], default_value=3
    )
    cs.add_hyperparameter(kernel_size)

    # Pooling type
    pooling_type = CategoricalHyperparameter(
        "pooling_type", choices=["max", "avg"], default_value="max"
    )
    cs.add_hyperparameter(pooling_type)

    # Number of dense layers
    num_dense_layers = UniformIntegerHyperparameter(
        "num_dense_layers", lower=1, upper=2, default_value=1
    )
    cs.add_hyperparameter(num_dense_layers)

    # Size of dense layers
    dense_size_1 = CategoricalHyperparameter(
        "dense_size_1", choices=[128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(dense_size_1)

    dense_size_2 = CategoricalHyperparameter(
        "dense_size_2", choices=[256, 512, 1024], default_value=512
    )
    cs.add_hyperparameter(dense_size_2)

    # Dropout rate
    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate", lower=0.0, upper=0.5, default_value=0.2
    )
    cs.add_hyperparameter(dropout_rate)

    # Normalization Strategy
    normalization_strategy = CategoricalHyperparameter(
        "normalization_strategy", choices=["none", "batchnorm", "layernorm"], default_value="batchnorm"
    )
    cs.add_hyperparameter(normalization_strategy)

    # CNN Condition
    cnn_condition = InCondition(child=num_conv_layers, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition)

    cnn_condition_filters1 = InCondition(child=conv_filters_1, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters1)

    cnn_condition_filters2 = InCondition(child=conv_filters_2, parent=model_type, values=['cnn'])
    #cs.add_condition(cnn_condition_filters2)

    cnn_condition_filters3 = InCondition(child=conv_filters_3, parent=model_type, values=['cnn'])
    #cs.add_condition(cnn_condition_filters3)

    cnn_condition_kernel = InCondition(child=kernel_size, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_kernel)

    cnn_condition_pool = InCondition(child=pooling_type, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_pool)

    dense_condition_1 = InCondition(child=dense_size_1, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_1)

    dense_condition_2 = InCondition(child=dense_size_2, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_2)

    dense_condition_layers = InCondition(child=num_dense_layers, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_layers)

    drop_condition = InCondition(child=dropout_rate, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(drop_condition)

    norm_condition = InCondition(child=normalization_strategy, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(norm_condition)


    # Conditions for number of conv layers
    conv_filters_2_condition = InCondition(conv_filters_2, num_conv_layers, [2, 3])
    conv_filters_2_and_cnn = AndConjunction(cnn_condition, conv_filters_2_condition)
    cs.add_condition(conv_filters_2_and_cnn)

    conv_filters_3_condition = InCondition(conv_filters_3, num_conv_layers, [3])
    conv_filters_3_and_cnn = AndConjunction(cnn_condition, conv_filters_3_condition)
    cs.add_condition(conv_filters_3_and_cnn)

    return cs

--------------------------------------------------------------------------------
[2025-06-15 18:53:36] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#3): All Conjunctions, Conditions must have the same child.
--------------------------------------------------------------------------------
[2025-06-15 18:53:44] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, Constant
from ConfigSpace.conditions import InCondition, AndConjunction
from ConfigSpace.hyperparameters import UnParametrizedHyperparameter

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization of a CNN
    for image classification.  The dataset is assumed to be a grayscale image
    dataset with 10 classes and images of size 28x28, similar to Fashion MNIST.
    """

    cs = ConfigurationSpace()

    # Model type: CNN or MLP
    model_type = CategoricalHyperparameter(
        "model_type", choices=["cnn", "mlp"], default_value="cnn"
    )
    cs.add_hyperparameter(model_type)

    # Batch size
    batch_size = CategoricalHyperparameter(
        "batch_size", choices=[32, 64, 128, 256], default_value=128
    )
    cs.add_hyperparameter(batch_size)

    # Optimizer
    optimizer = CategoricalHyperparameter(
        "optimizer", choices=["adam", "sgd"], default_value="adam"
    )
    cs.add_hyperparameter(optimizer)

    # Learning rate
    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-5, upper=1e-2, default_value=1e-3, log=True
    )
    cs.add_hyperparameter(learning_rate)

    # SGD-specific hyperparameters
    sgd_momentum = UniformFloatHyperparameter(
        "sgd_momentum", lower=0.0, upper=0.9, default_value=0.0
    )
    cs.add_hyperparameter(sgd_momentum)

    # Condition for SGD momentum
    momentum_condition = InCondition(sgd_momentum, optimizer, ["sgd"])
    cs.add_condition(momentum_condition)

    # CNN specific hyperparameters
    num_conv_layers = UniformIntegerHyperparameter(
        "num_conv_layers", lower=1, upper=3, default_value=2
    )
    cs.add_hyperparameter(num_conv_layers)

    # Number of filters for conv layers
    conv_filters_1 = CategoricalHyperparameter(
        "conv_filters_1", choices=[32, 64, 128], default_value=64
    )
    cs.add_hyperparameter(conv_filters_1)

    conv_filters_2 = CategoricalHyperparameter(
        "conv_filters_2", choices=[64, 128, 256], default_value=128
    )
    cs.add_hyperparameter(conv_filters_2)

    conv_filters_3 = CategoricalHyperparameter(
        "conv_filters_3", choices=[128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(conv_filters_3)

    # Kernel size for conv layers
    kernel_size = CategoricalHyperparameter(
        "kernel_size", choices=[3, 5], default_value=3
    )
    cs.add_hyperparameter(kernel_size)

    # Pooling type
    pooling_type = CategoricalHyperparameter(
        "pooling_type", choices=["max", "avg"], default_value="max"
    )
    cs.add_hyperparameter(pooling_type)

    # Number of dense layers
    num_dense_layers = UniformIntegerHyperparameter(
        "num_dense_layers", lower=1, upper=2, default_value=1
    )
    cs.add_hyperparameter(num_dense_layers)

    # Size of dense layers
    dense_size_1 = CategoricalHyperparameter(
        "dense_size_1", choices=[128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(dense_size_1)

    dense_size_2 = CategoricalHyperparameter(
        "dense_size_2", choices=[256, 512, 1024], default_value=512
    )
    cs.add_hyperparameter(dense_size_2)

    # Dropout rate
    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate", lower=0.0, upper=0.5, default_value=0.2
    )
    cs.add_hyperparameter(dropout_rate)

    # Normalization Strategy
    normalization_strategy = CategoricalHyperparameter(
        "normalization_strategy", choices=["none", "batchnorm", "layernorm"], default_value="batchnorm"
    )
    cs.add_hyperparameter(normalization_strategy)

    # CNN Condition
    cnn_condition = InCondition(child=num_conv_layers, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition)

    cnn_condition_filters1 = InCondition(child=conv_filters_1, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters1)

    cnn_condition_filters2 = InCondition(child=conv_filters_2, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters2)

    cnn_condition_filters3 = InCondition(child=conv_filters_3, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters3)

    cnn_condition_kernel = InCondition(child=kernel_size, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_kernel)

    cnn_condition_pool = InCondition(child=pooling_type, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_pool)

    dense_condition_1 = InCondition(child=dense_size_1, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_1)

    dense_condition_2 = InCondition(child=dense_size_2, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_2)

    dense_condition_layers = InCondition(child=num_dense_layers, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_layers)

    drop_condition = InCondition(child=dropout_rate, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(drop_condition)

    norm_condition = InCondition(child=normalization_strategy, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(norm_condition)


    # Conditions for number of conv layers
    conv_filters_2_condition = InCondition(conv_filters_2, num_conv_layers, [2, 3])
    #conv_filters_2_and_cnn = AndConjunction(cnn_condition, conv_filters_2_condition)
    #cs.add_condition(conv_filters_2_and_cnn)
    cs.add_condition(conv_filters_2_condition)


    conv_filters_3_condition = InCondition(conv_filters_3, num_conv_layers, [3])
    #conv_filters_3_and_cnn = AndConjunction(cnn_condition, conv_filters_3_condition)
    #cs.add_condition(conv_filters_3_and_cnn)
    cs.add_condition(conv_filters_3_condition)

    return cs
```
--------------------------------------------------------------------------------
[2025-06-15 18:53:44] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, Constant
from ConfigSpace.conditions import InCondition, AndConjunction
from ConfigSpace.hyperparameters import UnParametrizedHyperparameter

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization of a CNN
    for image classification.  The dataset is assumed to be a grayscale image
    dataset with 10 classes and images of size 28x28, similar to Fashion MNIST.
    """

    cs = ConfigurationSpace()

    # Model type: CNN or MLP
    model_type = CategoricalHyperparameter(
        "model_type", choices=["cnn", "mlp"], default_value="cnn"
    )
    cs.add_hyperparameter(model_type)

    # Batch size
    batch_size = CategoricalHyperparameter(
        "batch_size", choices=[32, 64, 128, 256], default_value=128
    )
    cs.add_hyperparameter(batch_size)

    # Optimizer
    optimizer = CategoricalHyperparameter(
        "optimizer", choices=["adam", "sgd"], default_value="adam"
    )
    cs.add_hyperparameter(optimizer)

    # Learning rate
    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-5, upper=1e-2, default_value=1e-3, log=True
    )
    cs.add_hyperparameter(learning_rate)

    # SGD-specific hyperparameters
    sgd_momentum = UniformFloatHyperparameter(
        "sgd_momentum", lower=0.0, upper=0.9, default_value=0.0
    )
    cs.add_hyperparameter(sgd_momentum)

    # Condition for SGD momentum
    momentum_condition = InCondition(sgd_momentum, optimizer, ["sgd"])
    cs.add_condition(momentum_condition)

    # CNN specific hyperparameters
    num_conv_layers = UniformIntegerHyperparameter(
        "num_conv_layers", lower=1, upper=3, default_value=2
    )
    cs.add_hyperparameter(num_conv_layers)

    # Number of filters for conv layers
    conv_filters_1 = CategoricalHyperparameter(
        "conv_filters_1", choices=[32, 64, 128], default_value=64
    )
    cs.add_hyperparameter(conv_filters_1)

    conv_filters_2 = CategoricalHyperparameter(
        "conv_filters_2", choices=[64, 128, 256], default_value=128
    )
    cs.add_hyperparameter(conv_filters_2)

    conv_filters_3 = CategoricalHyperparameter(
        "conv_filters_3", choices=[128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(conv_filters_3)

    # Kernel size for conv layers
    kernel_size = CategoricalHyperparameter(
        "kernel_size", choices=[3, 5], default_value=3
    )
    cs.add_hyperparameter(kernel_size)

    # Pooling type
    pooling_type = CategoricalHyperparameter(
        "pooling_type", choices=["max", "avg"], default_value="max"
    )
    cs.add_hyperparameter(pooling_type)

    # Number of dense layers
    num_dense_layers = UniformIntegerHyperparameter(
        "num_dense_layers", lower=1, upper=2, default_value=1
    )
    cs.add_hyperparameter(num_dense_layers)

    # Size of dense layers
    dense_size_1 = CategoricalHyperparameter(
        "dense_size_1", choices=[128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(dense_size_1)

    dense_size_2 = CategoricalHyperparameter(
        "dense_size_2", choices=[256, 512, 1024], default_value=512
    )
    cs.add_hyperparameter(dense_size_2)

    # Dropout rate
    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate", lower=0.0, upper=0.5, default_value=0.2
    )
    cs.add_hyperparameter(dropout_rate)

    # Normalization Strategy
    normalization_strategy = CategoricalHyperparameter(
        "normalization_strategy", choices=["none", "batchnorm", "layernorm"], default_value="batchnorm"
    )
    cs.add_hyperparameter(normalization_strategy)

    # CNN Condition
    cnn_condition = InCondition(child=num_conv_layers, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition)

    cnn_condition_filters1 = InCondition(child=conv_filters_1, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters1)

    cnn_condition_filters2 = InCondition(child=conv_filters_2, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters2)

    cnn_condition_filters3 = InCondition(child=conv_filters_3, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters3)

    cnn_condition_kernel = InCondition(child=kernel_size, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_kernel)

    cnn_condition_pool = InCondition(child=pooling_type, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_pool)

    dense_condition_1 = InCondition(child=dense_size_1, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_1)

    dense_condition_2 = InCondition(child=dense_size_2, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_2)

    dense_condition_layers = InCondition(child=num_dense_layers, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_layers)

    drop_condition = InCondition(child=dropout_rate, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(drop_condition)

    norm_condition = InCondition(child=normalization_strategy, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(norm_condition)


    # Conditions for number of conv layers
    conv_filters_2_condition = InCondition(conv_filters_2, num_conv_layers, [2, 3])
    #conv_filters_2_and_cnn = AndConjunction(cnn_condition, conv_filters_2_condition)
    #cs.add_condition(conv_filters_2_and_cnn)
    cs.add_condition(conv_filters_2_condition)


    conv_filters_3_condition = InCondition(conv_filters_3, num_conv_layers, [3])
    #conv_filters_3_and_cnn = AndConjunction(cnn_condition, conv_filters_3_condition)
    #cs.add_condition(conv_filters_3_and_cnn)
    cs.add_condition(conv_filters_3_condition)

    return cs

--------------------------------------------------------------------------------
[2025-06-15 18:53:44] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#4): Adding a second parent condition for a for a hyperparameter is ambiguous and therefore forbidden. Use an `OrConjunction` or `AndConjunction` to combine conditions instead.
Already inserted: conv_filters_2 | model_type in {'cnn'}
New one: conv_filters_2 | num_conv_layers in {2, 3}
--------------------------------------------------------------------------------
[2025-06-15 18:53:52] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, Constant
from ConfigSpace.conditions import InCondition, AndConjunction
from ConfigSpace.hyperparameters import UnParametrizedHyperparameter

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization of a CNN
    for image classification.  The dataset is assumed to be a grayscale image
    dataset with 10 classes and images of size 28x28, similar to Fashion MNIST.
    """

    cs = ConfigurationSpace()

    # Model type: CNN or MLP
    model_type = CategoricalHyperparameter(
        "model_type", choices=["cnn", "mlp"], default_value="cnn"
    )
    cs.add_hyperparameter(model_type)

    # Batch size
    batch_size = CategoricalHyperparameter(
        "batch_size", choices=[32, 64, 128, 256], default_value=128
    )
    cs.add_hyperparameter(batch_size)

    # Optimizer
    optimizer = CategoricalHyperparameter(
        "optimizer", choices=["adam", "sgd"], default_value="adam"
    )
    cs.add_hyperparameter(optimizer)

    # Learning rate
    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-5, upper=1e-2, default_value=1e-3, log=True
    )
    cs.add_hyperparameter(learning_rate)

    # SGD-specific hyperparameters
    sgd_momentum = UniformFloatHyperparameter(
        "sgd_momentum", lower=0.0, upper=0.9, default_value=0.0
    )
    cs.add_hyperparameter(sgd_momentum)

    # Condition for SGD momentum
    momentum_condition = InCondition(sgd_momentum, optimizer, ["sgd"])
    cs.add_condition(momentum_condition)

    # CNN specific hyperparameters
    num_conv_layers = UniformIntegerHyperparameter(
        "num_conv_layers", lower=1, upper=3, default_value=2
    )
    cs.add_hyperparameter(num_conv_layers)

    # Number of filters for conv layers
    conv_filters_1 = CategoricalHyperparameter(
        "conv_filters_1", choices=[32, 64, 128], default_value=64
    )
    cs.add_hyperparameter(conv_filters_1)

    conv_filters_2 = CategoricalHyperparameter(
        "conv_filters_2", choices=[64, 128, 256], default_value=128
    )
    cs.add_hyperparameter(conv_filters_2)

    conv_filters_3 = CategoricalHyperparameter(
        "conv_filters_3", choices=[128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(conv_filters_3)

    # Kernel size for conv layers
    kernel_size = CategoricalHyperparameter(
        "kernel_size", choices=[3, 5], default_value=3
    )
    cs.add_hyperparameter(kernel_size)

    # Pooling type
    pooling_type = CategoricalHyperparameter(
        "pooling_type", choices=["max", "avg"], default_value="max"
    )
    cs.add_hyperparameter(pooling_type)

    # Number of dense layers
    num_dense_layers = UniformIntegerHyperparameter(
        "num_dense_layers", lower=1, upper=2, default_value=1
    )
    cs.add_hyperparameter(num_dense_layers)

    # Size of dense layers
    dense_size_1 = CategoricalHyperparameter(
        "dense_size_1", choices=[128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(dense_size_1)

    dense_size_2 = CategoricalHyperparameter(
        "dense_size_2", choices=[256, 512, 1024], default_value=512
    )
    cs.add_hyperparameter(dense_size_2)

    # Dropout rate
    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate", lower=0.0, upper=0.5, default_value=0.2
    )
    cs.add_hyperparameter(dropout_rate)

    # Normalization Strategy
    normalization_strategy = CategoricalHyperparameter(
        "normalization_strategy", choices=["none", "batchnorm", "layernorm"], default_value="batchnorm"
    )
    cs.add_hyperparameter(normalization_strategy)

    # CNN Condition
    cnn_condition = InCondition(child=num_conv_layers, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition)

    cnn_condition_filters1 = InCondition(child=conv_filters_1, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters1)

    cnn_condition_filters2 = InCondition(child=conv_filters_2, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters2)

    cnn_condition_filters3 = InCondition(child=conv_filters_3, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters3)

    cnn_condition_kernel = InCondition(child=kernel_size, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_kernel)

    cnn_condition_pool = InCondition(child=pooling_type, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_pool)

    dense_condition_1 = InCondition(child=dense_size_1, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_1)

    dense_condition_2 = InCondition(child=dense_size_2, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_2)

    dense_condition_layers = InCondition(child=num_dense_layers, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_layers)

    drop_condition = InCondition(child=dropout_rate, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(drop_condition)

    norm_condition = InCondition(child=normalization_strategy, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(norm_condition)


    # Conditions for number of conv layers
    conv_filters_2_condition = InCondition(child=conv_filters_2, parent=num_conv_layers, values=[2, 3])
    conv_filters_2_and_cnn = AndConjunction(cnn_condition_filters2, conv_filters_2_condition)
    cs.add_condition(conv_filters_2_and_cnn)


    conv_filters_3_condition = InCondition(child=conv_filters_3, parent=num_conv_layers, values=[3])
    conv_filters_3_and_cnn = AndConjunction(cnn_condition_filters3, conv_filters_3_condition)
    cs.add_condition(conv_filters_3_and_cnn)

    return cs
```
--------------------------------------------------------------------------------
[2025-06-15 18:53:52] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, Constant
from ConfigSpace.conditions import InCondition, AndConjunction
from ConfigSpace.hyperparameters import UnParametrizedHyperparameter

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization of a CNN
    for image classification.  The dataset is assumed to be a grayscale image
    dataset with 10 classes and images of size 28x28, similar to Fashion MNIST.
    """

    cs = ConfigurationSpace()

    # Model type: CNN or MLP
    model_type = CategoricalHyperparameter(
        "model_type", choices=["cnn", "mlp"], default_value="cnn"
    )
    cs.add_hyperparameter(model_type)

    # Batch size
    batch_size = CategoricalHyperparameter(
        "batch_size", choices=[32, 64, 128, 256], default_value=128
    )
    cs.add_hyperparameter(batch_size)

    # Optimizer
    optimizer = CategoricalHyperparameter(
        "optimizer", choices=["adam", "sgd"], default_value="adam"
    )
    cs.add_hyperparameter(optimizer)

    # Learning rate
    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-5, upper=1e-2, default_value=1e-3, log=True
    )
    cs.add_hyperparameter(learning_rate)

    # SGD-specific hyperparameters
    sgd_momentum = UniformFloatHyperparameter(
        "sgd_momentum", lower=0.0, upper=0.9, default_value=0.0
    )
    cs.add_hyperparameter(sgd_momentum)

    # Condition for SGD momentum
    momentum_condition = InCondition(sgd_momentum, optimizer, ["sgd"])
    cs.add_condition(momentum_condition)

    # CNN specific hyperparameters
    num_conv_layers = UniformIntegerHyperparameter(
        "num_conv_layers", lower=1, upper=3, default_value=2
    )
    cs.add_hyperparameter(num_conv_layers)

    # Number of filters for conv layers
    conv_filters_1 = CategoricalHyperparameter(
        "conv_filters_1", choices=[32, 64, 128], default_value=64
    )
    cs.add_hyperparameter(conv_filters_1)

    conv_filters_2 = CategoricalHyperparameter(
        "conv_filters_2", choices=[64, 128, 256], default_value=128
    )
    cs.add_hyperparameter(conv_filters_2)

    conv_filters_3 = CategoricalHyperparameter(
        "conv_filters_3", choices=[128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(conv_filters_3)

    # Kernel size for conv layers
    kernel_size = CategoricalHyperparameter(
        "kernel_size", choices=[3, 5], default_value=3
    )
    cs.add_hyperparameter(kernel_size)

    # Pooling type
    pooling_type = CategoricalHyperparameter(
        "pooling_type", choices=["max", "avg"], default_value="max"
    )
    cs.add_hyperparameter(pooling_type)

    # Number of dense layers
    num_dense_layers = UniformIntegerHyperparameter(
        "num_dense_layers", lower=1, upper=2, default_value=1
    )
    cs.add_hyperparameter(num_dense_layers)

    # Size of dense layers
    dense_size_1 = CategoricalHyperparameter(
        "dense_size_1", choices=[128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(dense_size_1)

    dense_size_2 = CategoricalHyperparameter(
        "dense_size_2", choices=[256, 512, 1024], default_value=512
    )
    cs.add_hyperparameter(dense_size_2)

    # Dropout rate
    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate", lower=0.0, upper=0.5, default_value=0.2
    )
    cs.add_hyperparameter(dropout_rate)

    # Normalization Strategy
    normalization_strategy = CategoricalHyperparameter(
        "normalization_strategy", choices=["none", "batchnorm", "layernorm"], default_value="batchnorm"
    )
    cs.add_hyperparameter(normalization_strategy)

    # CNN Condition
    cnn_condition = InCondition(child=num_conv_layers, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition)

    cnn_condition_filters1 = InCondition(child=conv_filters_1, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters1)

    cnn_condition_filters2 = InCondition(child=conv_filters_2, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters2)

    cnn_condition_filters3 = InCondition(child=conv_filters_3, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters3)

    cnn_condition_kernel = InCondition(child=kernel_size, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_kernel)

    cnn_condition_pool = InCondition(child=pooling_type, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_pool)

    dense_condition_1 = InCondition(child=dense_size_1, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_1)

    dense_condition_2 = InCondition(child=dense_size_2, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_2)

    dense_condition_layers = InCondition(child=num_dense_layers, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_layers)

    drop_condition = InCondition(child=dropout_rate, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(drop_condition)

    norm_condition = InCondition(child=normalization_strategy, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(norm_condition)


    # Conditions for number of conv layers
    conv_filters_2_condition = InCondition(child=conv_filters_2, parent=num_conv_layers, values=[2, 3])
    conv_filters_2_and_cnn = AndConjunction(cnn_condition_filters2, conv_filters_2_condition)
    cs.add_condition(conv_filters_2_and_cnn)


    conv_filters_3_condition = InCondition(child=conv_filters_3, parent=num_conv_layers, values=[3])
    conv_filters_3_and_cnn = AndConjunction(cnn_condition_filters3, conv_filters_3_condition)
    cs.add_condition(conv_filters_3_and_cnn)

    return cs

--------------------------------------------------------------------------------
[2025-06-15 18:53:52] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#5): Adding a second parent condition for a for a hyperparameter is ambiguous and therefore forbidden. Use an `OrConjunction` or `AndConjunction` to combine conditions instead.
Already inserted: conv_filters_2 | model_type in {'cnn'}
New one: (conv_filters_2 | model_type in {'cnn'} && conv_filters_2 | num_conv_layers in {2, 3})
--------------------------------------------------------------------------------
[2025-06-15 18:53:59] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, Constant
from ConfigSpace.conditions import InCondition, AndConjunction
from ConfigSpace.hyperparameters import UnParametrizedHyperparameter

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization of a CNN
    for image classification.  The dataset is assumed to be a grayscale image
    dataset with 10 classes and images of size 28x28, similar to Fashion MNIST.
    """

    cs = ConfigurationSpace()

    # Model type: CNN or MLP
    model_type = CategoricalHyperparameter(
        "model_type", choices=["cnn", "mlp"], default_value="cnn"
    )
    cs.add_hyperparameter(model_type)

    # Batch size
    batch_size = CategoricalHyperparameter(
        "batch_size", choices=[32, 64, 128, 256], default_value=128
    )
    cs.add_hyperparameter(batch_size)

    # Optimizer
    optimizer = CategoricalHyperparameter(
        "optimizer", choices=["adam", "sgd"], default_value="adam"
    )
    cs.add_hyperparameter(optimizer)

    # Learning rate
    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-5, upper=1e-2, default_value=1e-3, log=True
    )
    cs.add_hyperparameter(learning_rate)

    # SGD-specific hyperparameters
    sgd_momentum = UniformFloatHyperparameter(
        "sgd_momentum", lower=0.0, upper=0.9, default_value=0.0
    )
    cs.add_hyperparameter(sgd_momentum)

    # Condition for SGD momentum
    momentum_condition = InCondition(sgd_momentum, optimizer, ["sgd"])
    cs.add_condition(momentum_condition)

    # CNN specific hyperparameters
    num_conv_layers = UniformIntegerHyperparameter(
        "num_conv_layers", lower=1, upper=3, default_value=2
    )
    cs.add_hyperparameter(num_conv_layers)

    # Number of filters for conv layers
    conv_filters_1 = CategoricalHyperparameter(
        "conv_filters_1", choices=[32, 64, 128], default_value=64
    )
    cs.add_hyperparameter(conv_filters_1)

    conv_filters_2 = CategoricalHyperparameter(
        "conv_filters_2", choices=[64, 128, 256], default_value=128
    )
    cs.add_hyperparameter(conv_filters_2)

    conv_filters_3 = CategoricalHyperparameter(
        "conv_filters_3", choices=[128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(conv_filters_3)

    # Kernel size for conv layers
    kernel_size = CategoricalHyperparameter(
        "kernel_size", choices=[3, 5], default_value=3
    )
    cs.add_hyperparameter(kernel_size)

    # Pooling type
    pooling_type = CategoricalHyperparameter(
        "pooling_type", choices=["max", "avg"], default_value="max"
    )
    cs.add_hyperparameter(pooling_type)

    # Number of dense layers
    num_dense_layers = UniformIntegerHyperparameter(
        "num_dense_layers", lower=1, upper=2, default_value=1
    )
    cs.add_hyperparameter(num_dense_layers)

    # Size of dense layers
    dense_size_1 = CategoricalHyperparameter(
        "dense_size_1", choices=[128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(dense_size_1)

    dense_size_2 = CategoricalHyperparameter(
        "dense_size_2", choices=[256, 512, 1024], default_value=512
    )
    cs.add_hyperparameter(dense_size_2)

    # Dropout rate
    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate", lower=0.0, upper=0.5, default_value=0.2
    )
    cs.add_hyperparameter(dropout_rate)

    # Normalization Strategy
    normalization_strategy = CategoricalHyperparameter(
        "normalization_strategy", choices=["none", "batchnorm", "layernorm"], default_value="batchnorm"
    )
    cs.add_hyperparameter(normalization_strategy)

    # CNN Condition
    cnn_condition = InCondition(child=num_conv_layers, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition)

    cnn_condition_filters1 = InCondition(child=conv_filters_1, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters1)

    cnn_condition_filters2 = InCondition(child=conv_filters_2, parent=model_type, values=['cnn'])
    
    cnn_condition_filters3 = InCondition(child=conv_filters_3, parent=model_type, values=['cnn'])
    
    cnn_condition_kernel = InCondition(child=kernel_size, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_kernel)

    cnn_condition_pool = InCondition(child=pooling_type, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_pool)

    dense_condition_1 = InCondition(child=dense_size_1, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_1)

    dense_condition_2 = InCondition(child=dense_size_2, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_2)

    dense_condition_layers = InCondition(child=num_dense_layers, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_layers)

    drop_condition = InCondition(child=dropout_rate, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(drop_condition)

    norm_condition = InCondition(child=normalization_strategy, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(norm_condition)


    # Conditions for number of conv layers
    conv_filters_2_condition = InCondition(child=conv_filters_2, parent=num_conv_layers, values=[2, 3])
    conv_filters_2_and_cnn = AndConjunction(cnn_condition_filters2, conv_filters_2_condition)
    cs.add_condition(conv_filters_2_and_cnn)


    conv_filters_3_condition = InCondition(child=conv_filters_3, parent=num_conv_layers, values=[3])
    conv_filters_3_and_cnn = AndConjunction(cnn_condition_filters3, conv_filters_3_condition)
    cs.add_condition(conv_filters_3_and_cnn)
    
    # Only add conv_filters_3 condition if num_conv_layers is 3
    cs.add_condition(cnn_condition_filters3)

    return cs
```
--------------------------------------------------------------------------------
[2025-06-15 18:53:59] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, Constant
from ConfigSpace.conditions import InCondition, AndConjunction
from ConfigSpace.hyperparameters import UnParametrizedHyperparameter

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization of a CNN
    for image classification.  The dataset is assumed to be a grayscale image
    dataset with 10 classes and images of size 28x28, similar to Fashion MNIST.
    """

    cs = ConfigurationSpace()

    # Model type: CNN or MLP
    model_type = CategoricalHyperparameter(
        "model_type", choices=["cnn", "mlp"], default_value="cnn"
    )
    cs.add_hyperparameter(model_type)

    # Batch size
    batch_size = CategoricalHyperparameter(
        "batch_size", choices=[32, 64, 128, 256], default_value=128
    )
    cs.add_hyperparameter(batch_size)

    # Optimizer
    optimizer = CategoricalHyperparameter(
        "optimizer", choices=["adam", "sgd"], default_value="adam"
    )
    cs.add_hyperparameter(optimizer)

    # Learning rate
    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-5, upper=1e-2, default_value=1e-3, log=True
    )
    cs.add_hyperparameter(learning_rate)

    # SGD-specific hyperparameters
    sgd_momentum = UniformFloatHyperparameter(
        "sgd_momentum", lower=0.0, upper=0.9, default_value=0.0
    )
    cs.add_hyperparameter(sgd_momentum)

    # Condition for SGD momentum
    momentum_condition = InCondition(sgd_momentum, optimizer, ["sgd"])
    cs.add_condition(momentum_condition)

    # CNN specific hyperparameters
    num_conv_layers = UniformIntegerHyperparameter(
        "num_conv_layers", lower=1, upper=3, default_value=2
    )
    cs.add_hyperparameter(num_conv_layers)

    # Number of filters for conv layers
    conv_filters_1 = CategoricalHyperparameter(
        "conv_filters_1", choices=[32, 64, 128], default_value=64
    )
    cs.add_hyperparameter(conv_filters_1)

    conv_filters_2 = CategoricalHyperparameter(
        "conv_filters_2", choices=[64, 128, 256], default_value=128
    )
    cs.add_hyperparameter(conv_filters_2)

    conv_filters_3 = CategoricalHyperparameter(
        "conv_filters_3", choices=[128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(conv_filters_3)

    # Kernel size for conv layers
    kernel_size = CategoricalHyperparameter(
        "kernel_size", choices=[3, 5], default_value=3
    )
    cs.add_hyperparameter(kernel_size)

    # Pooling type
    pooling_type = CategoricalHyperparameter(
        "pooling_type", choices=["max", "avg"], default_value="max"
    )
    cs.add_hyperparameter(pooling_type)

    # Number of dense layers
    num_dense_layers = UniformIntegerHyperparameter(
        "num_dense_layers", lower=1, upper=2, default_value=1
    )
    cs.add_hyperparameter(num_dense_layers)

    # Size of dense layers
    dense_size_1 = CategoricalHyperparameter(
        "dense_size_1", choices=[128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(dense_size_1)

    dense_size_2 = CategoricalHyperparameter(
        "dense_size_2", choices=[256, 512, 1024], default_value=512
    )
    cs.add_hyperparameter(dense_size_2)

    # Dropout rate
    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate", lower=0.0, upper=0.5, default_value=0.2
    )
    cs.add_hyperparameter(dropout_rate)

    # Normalization Strategy
    normalization_strategy = CategoricalHyperparameter(
        "normalization_strategy", choices=["none", "batchnorm", "layernorm"], default_value="batchnorm"
    )
    cs.add_hyperparameter(normalization_strategy)

    # CNN Condition
    cnn_condition = InCondition(child=num_conv_layers, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition)

    cnn_condition_filters1 = InCondition(child=conv_filters_1, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters1)

    cnn_condition_filters2 = InCondition(child=conv_filters_2, parent=model_type, values=['cnn'])
    
    cnn_condition_filters3 = InCondition(child=conv_filters_3, parent=model_type, values=['cnn'])
    
    cnn_condition_kernel = InCondition(child=kernel_size, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_kernel)

    cnn_condition_pool = InCondition(child=pooling_type, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_pool)

    dense_condition_1 = InCondition(child=dense_size_1, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_1)

    dense_condition_2 = InCondition(child=dense_size_2, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_2)

    dense_condition_layers = InCondition(child=num_dense_layers, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(dense_condition_layers)

    drop_condition = InCondition(child=dropout_rate, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(drop_condition)

    norm_condition = InCondition(child=normalization_strategy, parent=model_type, values=['mlp', 'cnn'])
    cs.add_condition(norm_condition)


    # Conditions for number of conv layers
    conv_filters_2_condition = InCondition(child=conv_filters_2, parent=num_conv_layers, values=[2, 3])
    conv_filters_2_and_cnn = AndConjunction(cnn_condition_filters2, conv_filters_2_condition)
    cs.add_condition(conv_filters_2_and_cnn)


    conv_filters_3_condition = InCondition(child=conv_filters_3, parent=num_conv_layers, values=[3])
    conv_filters_3_and_cnn = AndConjunction(cnn_condition_filters3, conv_filters_3_condition)
    cs.add_condition(conv_filters_3_and_cnn)
    
    # Only add conv_filters_3 condition if num_conv_layers is 3
    cs.add_condition(cnn_condition_filters3)

    return cs

--------------------------------------------------------------------------------
[2025-06-15 18:53:59] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#6): Adding a second parent condition for a for a hyperparameter is ambiguous and therefore forbidden. Use an `OrConjunction` or `AndConjunction` to combine conditions instead.
Already inserted: (conv_filters_3 | model_type in {'cnn'} && conv_filters_3 | num_conv_layers in {3})
New one: conv_filters_3 | model_type in {'cnn'}
--------------------------------------------------------------------------------
[2025-06-15 18:54:07] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, Constant
from ConfigSpace.conditions import InCondition, AndConjunction
from ConfigSpace.hyperparameters import UnParametrizedHyperparameter

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization of a CNN
    for image classification.  The dataset is assumed to be a grayscale image
    dataset with 10 classes and images of size 28x28, similar to Fashion MNIST.
    """

    cs = ConfigurationSpace()

    # Model type: CNN or MLP
    model_type = CategoricalHyperparameter(
        "model_type", choices=["cnn", "mlp"], default_value="cnn"
    )
    cs.add_hyperparameter(model_type)

    # Batch size
    batch_size = CategoricalHyperparameter(
        "batch_size", choices=[32, 64, 128, 256], default_value=128
    )
    cs.add_hyperparameter(batch_size)

    # Optimizer
    optimizer = CategoricalHyperparameter(
        "optimizer", choices=["adam", "sgd"], default_value="adam"
    )
    cs.add_hyperparameter(optimizer)

    # Learning rate
    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-5, upper=1e-2, default_value=1e-3, log=True
    )
    cs.add_hyperparameter(learning_rate)

    # SGD-specific hyperparameters
    sgd_momentum = UniformFloatHyperparameter(
        "sgd_momentum", lower=0.0, upper=0.9, default_value=0.0
    )
    cs.add_hyperparameter(sgd_momentum)

    # Condition for SGD momentum
    momentum_condition = InCondition(sgd_momentum, optimizer, ["sgd"])
    cs.add_condition(momentum_condition)

    # CNN specific hyperparameters
    num_conv_layers = UniformIntegerHyperparameter(
        "num_conv_layers", lower=1, upper=3, default_value=2
    )
    cs.add_hyperparameter(num_conv_layers)

    # Number of filters for conv layers
    conv_filters_1 = CategoricalHyperparameter(
        "conv_filters_1", choices=[32, 64, 128], default_value=64
    )
    cs.add_hyperparameter(conv_filters_1)

    conv_filters_2 = CategoricalHyperparameter(
        "conv_filters_2", choices=[64, 128, 256], default_value=128
    )
    cs.add_hyperparameter(conv_filters_2)

    conv_filters_3 = CategoricalHyperparameter(
        "conv_filters_3", choices=[128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(conv_filters_3)

    # Kernel size for conv layers
    kernel_size = CategoricalHyperparameter(
        "kernel_size", choices=[3, 5], default_value=3
    )
    cs.add_hyperparameter(kernel_size)

    # Pooling type
    pooling_type = CategoricalHyperparameter(
        "pooling_type", choices=["max", "avg"], default_value="max"
    )
    cs.add_hyperparameter(pooling_type)

    # Number of dense layers
    num_dense_layers = UniformIntegerHyperparameter(
        "num_dense_layers", lower=1, upper=2, default_value=1
    )
    cs.add_hyperparameter(num_dense_layers)

    # Size of dense layers
    dense_size_1 = CategoricalHyperparameter(
        "dense_size_1", choices=[128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(dense_size_1)

    dense_size_2 = CategoricalHyperparameter(
        "dense_size_2", choices=[256, 512, 1024], default_value=512
    )
    cs.add_hyperparameter(dense_size_2)

    # Dropout rate
    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate", lower=0.0, upper=0.5, default_value=0.2
    )
    cs.add_hyperparameter(dropout_rate)

    # Normalization Strategy
    normalization_strategy = CategoricalHyperparameter(
        "normalization_strategy", choices=["none", "batchnorm", "layernorm"], default_value="batchnorm"
    )
    cs.add_hyperparameter(normalization_strategy)

    # CNN Condition
    cnn_condition = InCondition(child=num_conv_layers, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition)

    cnn_condition_filters1 = InCondition(child=conv_filters_1, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters1)

    cnn_condition_kernel = InCondition(child=kernel_size, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_kernel)

    cnn_condition_pool = InCondition(child=pooling_type, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_pool)

    dense_condition_1 = InCondition(child=dense_size_1, parent=model_type, values=['mlp'])
    cs.add_condition(dense_condition_1)

    dense_condition_2 = InCondition(child=dense_size_2, parent=model_type, values=['mlp'])
    cs.add_condition(dense_condition_2)

    dense_condition_layers = InCondition(child=num_dense_layers, parent=model_type, values=['mlp'])
    cs.add_condition(dense_condition_layers)

    drop_condition = InCondition(child=dropout_rate, parent=model_type, values=['mlp'])
    cs.add_condition(drop_condition)

    norm_condition = InCondition(child=normalization_strategy, parent=model_type, values=['mlp'])
    cs.add_condition(norm_condition)


    # Conditions for number of conv layers
    conv_filters_2_condition = AndConjunction(
        InCondition(child=conv_filters_2, parent=num_conv_layers, values=[2, 3]),
        InCondition(child=conv_filters_2, parent=model_type, values=['cnn'])
    )
    cs.add_condition(conv_filters_2_condition)


    conv_filters_3_condition = AndConjunction(
        InCondition(child=conv_filters_3, parent=num_conv_layers, values=[3]),
        InCondition(child=conv_filters_3, parent=model_type, values=['cnn'])
    )
    cs.add_condition(conv_filters_3_condition)

    return cs
```
--------------------------------------------------------------------------------
[2025-06-15 18:54:07] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, Constant
from ConfigSpace.conditions import InCondition, AndConjunction
from ConfigSpace.hyperparameters import UnParametrizedHyperparameter

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization of a CNN
    for image classification.  The dataset is assumed to be a grayscale image
    dataset with 10 classes and images of size 28x28, similar to Fashion MNIST.
    """

    cs = ConfigurationSpace()

    # Model type: CNN or MLP
    model_type = CategoricalHyperparameter(
        "model_type", choices=["cnn", "mlp"], default_value="cnn"
    )
    cs.add_hyperparameter(model_type)

    # Batch size
    batch_size = CategoricalHyperparameter(
        "batch_size", choices=[32, 64, 128, 256], default_value=128
    )
    cs.add_hyperparameter(batch_size)

    # Optimizer
    optimizer = CategoricalHyperparameter(
        "optimizer", choices=["adam", "sgd"], default_value="adam"
    )
    cs.add_hyperparameter(optimizer)

    # Learning rate
    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-5, upper=1e-2, default_value=1e-3, log=True
    )
    cs.add_hyperparameter(learning_rate)

    # SGD-specific hyperparameters
    sgd_momentum = UniformFloatHyperparameter(
        "sgd_momentum", lower=0.0, upper=0.9, default_value=0.0
    )
    cs.add_hyperparameter(sgd_momentum)

    # Condition for SGD momentum
    momentum_condition = InCondition(sgd_momentum, optimizer, ["sgd"])
    cs.add_condition(momentum_condition)

    # CNN specific hyperparameters
    num_conv_layers = UniformIntegerHyperparameter(
        "num_conv_layers", lower=1, upper=3, default_value=2
    )
    cs.add_hyperparameter(num_conv_layers)

    # Number of filters for conv layers
    conv_filters_1 = CategoricalHyperparameter(
        "conv_filters_1", choices=[32, 64, 128], default_value=64
    )
    cs.add_hyperparameter(conv_filters_1)

    conv_filters_2 = CategoricalHyperparameter(
        "conv_filters_2", choices=[64, 128, 256], default_value=128
    )
    cs.add_hyperparameter(conv_filters_2)

    conv_filters_3 = CategoricalHyperparameter(
        "conv_filters_3", choices=[128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(conv_filters_3)

    # Kernel size for conv layers
    kernel_size = CategoricalHyperparameter(
        "kernel_size", choices=[3, 5], default_value=3
    )
    cs.add_hyperparameter(kernel_size)

    # Pooling type
    pooling_type = CategoricalHyperparameter(
        "pooling_type", choices=["max", "avg"], default_value="max"
    )
    cs.add_hyperparameter(pooling_type)

    # Number of dense layers
    num_dense_layers = UniformIntegerHyperparameter(
        "num_dense_layers", lower=1, upper=2, default_value=1
    )
    cs.add_hyperparameter(num_dense_layers)

    # Size of dense layers
    dense_size_1 = CategoricalHyperparameter(
        "dense_size_1", choices=[128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(dense_size_1)

    dense_size_2 = CategoricalHyperparameter(
        "dense_size_2", choices=[256, 512, 1024], default_value=512
    )
    cs.add_hyperparameter(dense_size_2)

    # Dropout rate
    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate", lower=0.0, upper=0.5, default_value=0.2
    )
    cs.add_hyperparameter(dropout_rate)

    # Normalization Strategy
    normalization_strategy = CategoricalHyperparameter(
        "normalization_strategy", choices=["none", "batchnorm", "layernorm"], default_value="batchnorm"
    )
    cs.add_hyperparameter(normalization_strategy)

    # CNN Condition
    cnn_condition = InCondition(child=num_conv_layers, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition)

    cnn_condition_filters1 = InCondition(child=conv_filters_1, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_filters1)

    cnn_condition_kernel = InCondition(child=kernel_size, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_kernel)

    cnn_condition_pool = InCondition(child=pooling_type, parent=model_type, values=['cnn'])
    cs.add_condition(cnn_condition_pool)

    dense_condition_1 = InCondition(child=dense_size_1, parent=model_type, values=['mlp'])
    cs.add_condition(dense_condition_1)

    dense_condition_2 = InCondition(child=dense_size_2, parent=model_type, values=['mlp'])
    cs.add_condition(dense_condition_2)

    dense_condition_layers = InCondition(child=num_dense_layers, parent=model_type, values=['mlp'])
    cs.add_condition(dense_condition_layers)

    drop_condition = InCondition(child=dropout_rate, parent=model_type, values=['mlp'])
    cs.add_condition(drop_condition)

    norm_condition = InCondition(child=normalization_strategy, parent=model_type, values=['mlp'])
    cs.add_condition(norm_condition)


    # Conditions for number of conv layers
    conv_filters_2_condition = AndConjunction(
        InCondition(child=conv_filters_2, parent=num_conv_layers, values=[2, 3]),
        InCondition(child=conv_filters_2, parent=model_type, values=['cnn'])
    )
    cs.add_condition(conv_filters_2_condition)


    conv_filters_3_condition = AndConjunction(
        InCondition(child=conv_filters_3, parent=num_conv_layers, values=[3]),
        InCondition(child=conv_filters_3, parent=model_type, values=['cnn'])
    )
    cs.add_condition(conv_filters_3_condition)

    return cs

--------------------------------------------------------------------------------
[2025-06-15 18:54:07] [Metadata: {'component': 'config', 'status': 'success'}] Configuration space generated successfully
--------------------------------------------------------------------------------
[2025-06-15 18:54:08] [Metadata: {'component': 'scenario'}] ```python
from smac import Scenario
from ConfigSpace import ConfigurationSpace
from pathlib import Path

def generate_scenario(cs: ConfigurationSpace) -> Scenario:
    """
    Generates a SMAC scenario configuration for hyperparameter optimization.

    Args:
        cs (ConfigurationSpace): The configuration space from which to sample configurations.

    Returns:
        Scenario: A configured SMAC Scenario object.
    """

    scenario = Scenario(
        configspace=cs,
        name="image_classification_experiment",
        output_directory=Path("automl_results"),
        deterministic=False,
        n_trials=10,
        n_workers=1,
        min_budget=1,
        max_budget=5
    )
    return scenario
```
--------------------------------------------------------------------------------
[2025-06-15 18:54:08] [Metadata: {'component': 'scenario', 'action': 'run'}] from smac import Scenario
from ConfigSpace import ConfigurationSpace
from pathlib import Path

def generate_scenario(cs: ConfigurationSpace) -> Scenario:
    """
    Generates a SMAC scenario configuration for hyperparameter optimization.

    Args:
        cs (ConfigurationSpace): The configuration space from which to sample configurations.

    Returns:
        Scenario: A configured SMAC Scenario object.
    """

    scenario = Scenario(
        configspace=cs,
        name="image_classification_experiment",
        output_directory=Path("automl_results"),
        deterministic=False,
        n_trials=10,
        n_workers=1,
        min_budget=1,
        max_budget=5
    )
    return scenario

--------------------------------------------------------------------------------
[2025-06-15 18:54:08] [Metadata: {'component': 'scenario', 'status': 'success'}] Scenario generated successfully
--------------------------------------------------------------------------------
[2025-06-15 18:54:22] [Metadata: {'component': 'train_function'}] ```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from ConfigSpace import Configuration
from typing import Any
import math


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """Trains a model based on the given configuration and dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dictionary containing 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: The loss value after training.
    """
    torch.manual_seed(seed)
    np.random.seed(seed)

    X, y = dataset['X'], dataset['y']

    # Determine input shape and number of classes
    n_samples = X.shape[0]
    if len(X.shape) == 2:  # Flattened input
        n_features = X.shape[1]
        height = width = int(math.sqrt(n_features))
        assert height * width == n_features, "Input features are not square."
        X = X.reshape(n_samples, 1, height, width)  # Reshape to (N, 1, H, W)
    elif len(X.shape) == 3:  # 3D input (N, H, W)
        X = X.reshape(n_samples, 1, X.shape[1], X.shape[2])  # Add channel dimension
    elif len(X.shape) == 4: # 4D input
        pass  # Assume (N, C, H, W) or (N, H, W, C) and handle in model

    num_classes = len(np.unique(y))

    # Normalize data to [0, 1]
    X = X.astype(np.float32) / 255.0

    # Convert data to PyTorch tensors
    X = torch.tensor(X)
    y = torch.tensor(y, dtype=torch.long)  # Ensure labels are long type


    # Device configuration
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    X = X.to(device)
    y = y.to(device)

    # Extract hyperparameters from configuration
    model_type = cfg.get("model_type")
    batch_size = cfg.get("batch_size")
    learning_rate = cfg.get("learning_rate")
    optimizer_name = cfg.get("optimizer")
    num_epochs = 5 # Constant epochs for faster execution

    # Model definition
    class CNN(nn.Module):
        def __init__(self, num_classes, cfg):
            super(CNN, self).__init__()
            num_conv_layers = cfg.get("num_conv_layers")
            conv_filters_1 = cfg.get("conv_filters_1")
            conv_filters_2 = cfg.get("conv_filters_2") if num_conv_layers >= 2 else 0
            conv_filters_3 = cfg.get("conv_filters_3") if num_conv_layers == 3 else 0
            kernel_size = cfg.get("kernel_size")
            pooling_type = cfg.get("pooling_type")
            normalization_strategy = cfg.get("normalization_strategy")

            self.conv1 = nn.Conv2d(1, conv_filters_1, kernel_size=kernel_size)
            self.bn1 = nn.BatchNorm2d(conv_filters_1) if normalization_strategy == "batchnorm" else nn.Identity()
            self.ln1 = nn.LayerNorm([conv_filters_1, X.shape[2] - kernel_size + 1, X.shape[3] - kernel_size + 1]) if normalization_strategy == "layernorm" else nn.Identity()

            if pooling_type == "max":
                self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
            else:
                self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)

            self.conv2 = nn.Conv2d(conv_filters_1, conv_filters_2, kernel_size=kernel_size) if num_conv_layers >= 2 else None
            self.bn2 = nn.BatchNorm2d(conv_filters_2) if num_conv_layers >= 2 and normalization_strategy == "batchnorm" else nn.Identity()
            self.ln2 = nn.LayerNorm([conv_filters_2, (X.shape[2] - kernel_size + 1) // 2 - kernel_size + 1, (X.shape[3] - kernel_size + 1) // 2 - kernel_size + 1]) if num_conv_layers >= 2 and normalization_strategy == "layernorm" else nn.Identity()

            if pooling_type == "max":
                self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) if num_conv_layers >= 2 else None
            else:
                self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2) if num_conv_layers >= 2 else None


            self.conv3 = nn.Conv2d(conv_filters_2, conv_filters_3, kernel_size=kernel_size) if num_conv_layers == 3 else None
            self.bn3 = nn.BatchNorm2d(conv_filters_3) if num_conv_layers == 3 and normalization_strategy == "batchnorm" else nn.Identity()
            self.ln3 = nn.LayerNorm([conv_filters_3, ((X.shape[2] - kernel_size + 1) // 2 - kernel_size + 1) // 2 - kernel_size + 1, ((X.shape[3] - kernel_size + 1) // 2 - kernel_size + 1) // 2 - kernel_size + 1]) if num_conv_layers == 3 and normalization_strategy == "layernorm" else nn.Identity()

            if pooling_type == "max":
                self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) if num_conv_layers == 3 else None
            else:
                self.pool3 = nn.AvgPool2d(kernel_size=2, stride=2) if num_conv_layers == 3 else None


            # Calculate the output size of the convolutional layers
            if num_conv_layers == 1:
                conv_out_size = (X.shape[2] - kernel_size + 1) // 2 * (X.shape[3] - kernel_size + 1) // 2 * conv_filters_1
            elif num_conv_layers == 2:
                 conv_out_size = ((X.shape[2] - kernel_size + 1) // 2 - kernel_size + 1) // 2 * ((X.shape[3] - kernel_size + 1) // 2 - kernel_size + 1) // 2 * conv_filters_2
            else:  # num_conv_layers == 3
                conv_out_size = (((X.shape[2] - kernel_size + 1) // 2 - kernel_size + 1) // 2 - kernel_size + 1) // 2 * (((X.shape[3] - kernel_size + 1) // 2 - kernel_size + 1) // 2 - kernel_size + 1) // 2 * conv_filters_3


            self.fc1 = nn.Linear(conv_out_size, cfg.get("dense_size_1"))
            self.bn_fc1 = nn.BatchNorm1d(cfg.get("dense_size_1")) if normalization_strategy == "batchnorm" else nn.Identity()
            self.ln_fc1 = nn.LayerNorm(cfg.get("dense_size_1")) if normalization_strategy == "layernorm" else nn.Identity()

            self.fc2 = nn.Linear(cfg.get("dense_size_1"), num_classes)



        def forward(self, x):
            x = torch.relu(self.conv1(x))
            x = self.bn1(x)
            x = self.ln1(x)
            x = self.pool1(x)

            if self.conv2 is not None:
                x = torch.relu(self.conv2(x))
                x = self.bn2(x)
                x = self.ln2(x)
                x = self.pool2(x)


            if self.conv3 is not None:
                x = torch.relu(self.conv3(x))
                x = self.bn3(x)
                x = self.ln3(x)
                x = self.pool3(x)


            x = torch.flatten(x, 1)
            x = torch.relu(self.fc1(x))
            x = self.bn_fc1(x)
            x = self.ln_fc1(x)
            x = self.fc2(x)
            return x

    class MLP(nn.Module):
        def __init__(self, num_classes, cfg):
            super(MLP, self).__init__()
            dense_size_1 = cfg.get("dense_size_1")
            dense_size_2 = cfg.get("dense_size_2")
            dropout_rate = cfg.get("dropout_rate")
            normalization_strategy = cfg.get("normalization_strategy")
            num_dense_layers = cfg.get("num_dense_layers")


            self.fc1 = nn.Linear(X.shape[1] * X.shape[2] * X.shape[3], dense_size_1)
            self.bn1 = nn.BatchNorm1d(dense_size_1) if normalization_strategy == "batchnorm" else nn.Identity()
            self.ln1 = nn.LayerNorm(dense_size_1) if normalization_strategy == "layernorm" else nn.Identity()
            self.dropout1 = nn.Dropout(dropout_rate)

            self.fc2 = nn.Linear(dense_size_1, dense_size_2) if num_dense_layers == 2 else None
            self.bn2 = nn.BatchNorm1d(dense_size_2) if num_dense_layers == 2 and normalization_strategy == "batchnorm" else nn.Identity()
            self.ln2 = nn.LayerNorm(dense_size_2) if num_dense_layers == 2 and normalization_strategy == "layernorm" else nn.Identity()
            self.dropout2 = nn.Dropout(dropout_rate) if num_dense_layers == 2 else None

            self.fc_out = nn.Linear(dense_size_2 if num_dense_layers == 2 else dense_size_1, num_classes)

        def forward(self, x):
            x = torch.flatten(x, 1)
            x = torch.relu(self.fc1(x))
            x = self.bn1(x)
            x = self.ln1(x)
            x = self.dropout1(x)

            if self.fc2 is not None:
                x = torch.relu(self.fc2(x))
                x = self.bn2(x)
                x = self.ln2(x)
                x = self.dropout2(x)

            x = self.fc_out(x)
            return x

    if model_type == "cnn":
        model = CNN(num_classes, cfg).to(device)
    else: # model_type == "mlp"
        model = MLP(num_classes, cfg).to(device)


    # Optimizer definition
    if optimizer_name == "adam":
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    else:  # optimizer_name == "sgd"
        sgd_momentum = cfg.get("sgd_momentum")
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=sgd_momentum)

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model.train()
    for epoch in range(num_epochs):
        for i in range(0, n_samples, batch_size):
            X_batch = X[i:i + batch_size]
            y_batch = y[i:i + batch_size]

            # Forward pass
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)

            # Backward and optimize
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    return loss.item()
```
--------------------------------------------------------------------------------
[2025-06-15 18:54:22] [Metadata: {'component': 'train_function', 'action': 'run'}] import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from ConfigSpace import Configuration
from typing import Any
import math


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """Trains a model based on the given configuration and dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dictionary containing 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: The loss value after training.
    """
    torch.manual_seed(seed)
    np.random.seed(seed)

    X, y = dataset['X'], dataset['y']

    # Determine input shape and number of classes
    n_samples = X.shape[0]
    if len(X.shape) == 2:  # Flattened input
        n_features = X.shape[1]
        height = width = int(math.sqrt(n_features))
        assert height * width == n_features, "Input features are not square."
        X = X.reshape(n_samples, 1, height, width)  # Reshape to (N, 1, H, W)
    elif len(X.shape) == 3:  # 3D input (N, H, W)
        X = X.reshape(n_samples, 1, X.shape[1], X.shape[2])  # Add channel dimension
    elif len(X.shape) == 4: # 4D input
        pass  # Assume (N, C, H, W) or (N, H, W, C) and handle in model

    num_classes = len(np.unique(y))

    # Normalize data to [0, 1]
    X = X.astype(np.float32) / 255.0

    # Convert data to PyTorch tensors
    X = torch.tensor(X)
    y = torch.tensor(y, dtype=torch.long)  # Ensure labels are long type


    # Device configuration
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    X = X.to(device)
    y = y.to(device)

    # Extract hyperparameters from configuration
    model_type = cfg.get("model_type")
    batch_size = cfg.get("batch_size")
    learning_rate = cfg.get("learning_rate")
    optimizer_name = cfg.get("optimizer")
    num_epochs = 5 # Constant epochs for faster execution

    # Model definition
    class CNN(nn.Module):
        def __init__(self, num_classes, cfg):
            super(CNN, self).__init__()
            num_conv_layers = cfg.get("num_conv_layers")
            conv_filters_1 = cfg.get("conv_filters_1")
            conv_filters_2 = cfg.get("conv_filters_2") if num_conv_layers >= 2 else 0
            conv_filters_3 = cfg.get("conv_filters_3") if num_conv_layers == 3 else 0
            kernel_size = cfg.get("kernel_size")
            pooling_type = cfg.get("pooling_type")
            normalization_strategy = cfg.get("normalization_strategy")

            self.conv1 = nn.Conv2d(1, conv_filters_1, kernel_size=kernel_size)
            self.bn1 = nn.BatchNorm2d(conv_filters_1) if normalization_strategy == "batchnorm" else nn.Identity()
            self.ln1 = nn.LayerNorm([conv_filters_1, X.shape[2] - kernel_size + 1, X.shape[3] - kernel_size + 1]) if normalization_strategy == "layernorm" else nn.Identity()

            if pooling_type == "max":
                self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
            else:
                self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)

            self.conv2 = nn.Conv2d(conv_filters_1, conv_filters_2, kernel_size=kernel_size) if num_conv_layers >= 2 else None
            self.bn2 = nn.BatchNorm2d(conv_filters_2) if num_conv_layers >= 2 and normalization_strategy == "batchnorm" else nn.Identity()
            self.ln2 = nn.LayerNorm([conv_filters_2, (X.shape[2] - kernel_size + 1) // 2 - kernel_size + 1, (X.shape[3] - kernel_size + 1) // 2 - kernel_size + 1]) if num_conv_layers >= 2 and normalization_strategy == "layernorm" else nn.Identity()

            if pooling_type == "max":
                self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) if num_conv_layers >= 2 else None
            else:
                self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2) if num_conv_layers >= 2 else None


            self.conv3 = nn.Conv2d(conv_filters_2, conv_filters_3, kernel_size=kernel_size) if num_conv_layers == 3 else None
            self.bn3 = nn.BatchNorm2d(conv_filters_3) if num_conv_layers == 3 and normalization_strategy == "batchnorm" else nn.Identity()
            self.ln3 = nn.LayerNorm([conv_filters_3, ((X.shape[2] - kernel_size + 1) // 2 - kernel_size + 1) // 2 - kernel_size + 1, ((X.shape[3] - kernel_size + 1) // 2 - kernel_size + 1) // 2 - kernel_size + 1]) if num_conv_layers == 3 and normalization_strategy == "layernorm" else nn.Identity()

            if pooling_type == "max":
                self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) if num_conv_layers == 3 else None
            else:
                self.pool3 = nn.AvgPool2d(kernel_size=2, stride=2) if num_conv_layers == 3 else None


            # Calculate the output size of the convolutional layers
            if num_conv_layers == 1:
                conv_out_size = (X.shape[2] - kernel_size + 1) // 2 * (X.shape[3] - kernel_size + 1) // 2 * conv_filters_1
            elif num_conv_layers == 2:
                 conv_out_size = ((X.shape[2] - kernel_size + 1) // 2 - kernel_size + 1) // 2 * ((X.shape[3] - kernel_size + 1) // 2 - kernel_size + 1) // 2 * conv_filters_2
            else:  # num_conv_layers == 3
                conv_out_size = (((X.shape[2] - kernel_size + 1) // 2 - kernel_size + 1) // 2 - kernel_size + 1) // 2 * (((X.shape[3] - kernel_size + 1) // 2 - kernel_size + 1) // 2 - kernel_size + 1) // 2 * conv_filters_3


            self.fc1 = nn.Linear(conv_out_size, cfg.get("dense_size_1"))
            self.bn_fc1 = nn.BatchNorm1d(cfg.get("dense_size_1")) if normalization_strategy == "batchnorm" else nn.Identity()
            self.ln_fc1 = nn.LayerNorm(cfg.get("dense_size_1")) if normalization_strategy == "layernorm" else nn.Identity()

            self.fc2 = nn.Linear(cfg.get("dense_size_1"), num_classes)



        def forward(self, x):
            x = torch.relu(self.conv1(x))
            x = self.bn1(x)
            x = self.ln1(x)
            x = self.pool1(x)

            if self.conv2 is not None:
                x = torch.relu(self.conv2(x))
                x = self.bn2(x)
                x = self.ln2(x)
                x = self.pool2(x)


            if self.conv3 is not None:
                x = torch.relu(self.conv3(x))
                x = self.bn3(x)
                x = self.ln3(x)
                x = self.pool3(x)


            x = torch.flatten(x, 1)
            x = torch.relu(self.fc1(x))
            x = self.bn_fc1(x)
            x = self.ln_fc1(x)
            x = self.fc2(x)
            return x

    class MLP(nn.Module):
        def __init__(self, num_classes, cfg):
            super(MLP, self).__init__()
            dense_size_1 = cfg.get("dense_size_1")
            dense_size_2 = cfg.get("dense_size_2")
            dropout_rate = cfg.get("dropout_rate")
            normalization_strategy = cfg.get("normalization_strategy")
            num_dense_layers = cfg.get("num_dense_layers")


            self.fc1 = nn.Linear(X.shape[1] * X.shape[2] * X.shape[3], dense_size_1)
            self.bn1 = nn.BatchNorm1d(dense_size_1) if normalization_strategy == "batchnorm" else nn.Identity()
            self.ln1 = nn.LayerNorm(dense_size_1) if normalization_strategy == "layernorm" else nn.Identity()
            self.dropout1 = nn.Dropout(dropout_rate)

            self.fc2 = nn.Linear(dense_size_1, dense_size_2) if num_dense_layers == 2 else None
            self.bn2 = nn.BatchNorm1d(dense_size_2) if num_dense_layers == 2 and normalization_strategy == "batchnorm" else nn.Identity()
            self.ln2 = nn.LayerNorm(dense_size_2) if num_dense_layers == 2 and normalization_strategy == "layernorm" else nn.Identity()
            self.dropout2 = nn.Dropout(dropout_rate) if num_dense_layers == 2 else None

            self.fc_out = nn.Linear(dense_size_2 if num_dense_layers == 2 else dense_size_1, num_classes)

        def forward(self, x):
            x = torch.flatten(x, 1)
            x = torch.relu(self.fc1(x))
            x = self.bn1(x)
            x = self.ln1(x)
            x = self.dropout1(x)

            if self.fc2 is not None:
                x = torch.relu(self.fc2(x))
                x = self.bn2(x)
                x = self.ln2(x)
                x = self.dropout2(x)

            x = self.fc_out(x)
            return x

    if model_type == "cnn":
        model = CNN(num_classes, cfg).to(device)
    else: # model_type == "mlp"
        model = MLP(num_classes, cfg).to(device)


    # Optimizer definition
    if optimizer_name == "adam":
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    else:  # optimizer_name == "sgd"
        sgd_momentum = cfg.get("sgd_momentum")
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=sgd_momentum)

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model.train()
    for epoch in range(num_epochs):
        for i in range(0, n_samples, batch_size):
            X_batch = X[i:i + batch_size]
            y_batch = y[i:i + batch_size]

            # Forward pass
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)

            # Backward and optimize
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    return loss.item()

--------------------------------------------------------------------------------
[2025-06-15 18:54:22] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#1): 'DataFrame' object has no attribute 'reshape'
--------------------------------------------------------------------------------
[2025-06-15 18:54:35] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from ConfigSpace import Configuration
from typing import Any
import math
import pandas as pd


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """Trains a model based on the given configuration and dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dictionary containing 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: The loss value after training.
    """
    torch.manual_seed(seed)
    np.random.seed(seed)

    X, y = dataset['X'], dataset['y']

    # Determine input shape and number of classes
    n_samples = X.shape[0]

    if isinstance(X, pd.DataFrame):
        X = X.values  # Convert DataFrame to NumPy array

    if len(X.shape) == 2:  # Flattened input
        n_features = X.shape[1]
        height = width = int(math.sqrt(n_features))
        assert height * width == n_features, "Input features are not square."
        X = X.reshape(n_samples, 1, height, width)  # Reshape to (N, 1, H, W)
    elif len(X.shape) == 3:  # 3D input (N, H, W)
        X = X.reshape(n_samples, 1, X.shape[1], X.shape[2])  # Add channel dimension
    elif len(X.shape) == 4: # 4D input
        pass  # Assume (N, C, H, W) or (N, H, W, C) and handle in model

    num_classes = len(np.unique(y))

    # Normalize data to [0, 1]
    X = X.astype(np.float32) / 255.0

    # Convert data to PyTorch tensors
    X = torch.tensor(X)
    y = torch.tensor(y, dtype=torch.long)  # Ensure labels are long type


    # Device configuration
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    X = X.to(device)
    y = y.to(device)

    # Extract hyperparameters from configuration
    model_type = cfg.get("model_type")
    batch_size = cfg.get("batch_size")
    learning_rate = cfg.get("learning_rate")
    optimizer_name = cfg.get("optimizer")
    num_epochs = 5 # Constant epochs for faster execution

    # Model definition
    class CNN(nn.Module):
        def __init__(self, num_classes, cfg):
            super(CNN, self).__init__()
            num_conv_layers = cfg.get("num_conv_layers")
            conv_filters_1 = cfg.get("conv_filters_1")
            conv_filters_2 = cfg.get("conv_filters_2") if num_conv_layers >= 2 else 0
            conv_filters_3 = cfg.get("conv_filters_3") if num_conv_layers == 3 else 0
            kernel_size = cfg.get("kernel_size")
            pooling_type = cfg.get("pooling_type")
            normalization_strategy = cfg.get("normalization_strategy")

            self.conv1 = nn.Conv2d(1, conv_filters_1, kernel_size=kernel_size)
            self.bn1 = nn.BatchNorm2d(conv_filters_1) if normalization_strategy == "batchnorm" else nn.Identity()
            self.ln1 = nn.LayerNorm([conv_filters_1, X.shape[2] - kernel_size + 1, X.shape[3] - kernel_size + 1]) if normalization_strategy == "layernorm" else nn.Identity()

            if pooling_type == "max":
                self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
            else:
                self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)

            self.conv2 = nn.Conv2d(conv_filters_1, conv_filters_2, kernel_size=kernel_size) if num_conv_layers >= 2 else None
            self.bn2 = nn.BatchNorm2d(conv_filters_2) if num_conv_layers >= 2 and normalization_strategy == "batchnorm" else nn.Identity()
            self.ln2 = nn.LayerNorm([conv_filters_2, (X.shape[2] - kernel_size + 1) // 2, (X.shape[3] - kernel_size + 1) // 2]) if num_conv_layers >= 2 and normalization_strategy == "layernorm" else nn.Identity()

            if pooling_type == "max":
                self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) if num_conv_layers >= 2 else None
            else:
                self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2) if num_conv_layers >= 2 else None


            self.conv3 = nn.Conv2d(conv_filters_2, conv_filters_3, kernel_size=kernel_size) if num_conv_layers == 3 else None
            self.bn3 = nn.BatchNorm2d(conv_filters_3) if num_conv_layers == 3 and normalization_strategy == "batchnorm" else nn.Identity()
            self.ln3 = nn.LayerNorm([conv_filters_3, ((X.shape[2] - kernel_size + 1) // 2) // 2, ((X.shape[3] - kernel_size + 1) // 2) // 2]) if num_conv_layers == 3 and normalization_strategy == "layernorm" else nn.Identity()

            if pooling_type == "max":
                self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) if num_conv_layers == 3 else None
            else:
                self.pool3 = nn.AvgPool2d(kernel_size=2, stride=2) if num_conv_layers == 3 else None


            # Calculate the output size of the convolutional layers
            if num_conv_layers == 1:
                conv_out_size = (X.shape[2] - kernel_size + 1) // 2 * (X.shape[3] - kernel_size + 1) // 2 * conv_filters_1
            elif num_conv_layers == 2:
                 conv_out_size = ((X.shape[2] - kernel_size + 1) // 2) // 2 * ((X.shape[3] - kernel_size + 1) // 2) // 2 * conv_filters_2
            else:  # num_conv_layers == 3
                conv_out_size = (((X.shape[2] - kernel_size + 1) // 2) // 2) // 2 * (((X.shape[3] - kernel_size + 1) // 2) // 2) // 2 * conv_filters_3


            self.fc1 = nn.Linear(conv_out_size, cfg.get("dense_size_1"))
            self.bn_fc1 = nn.BatchNorm1d(cfg.get("dense_size_1")) if normalization_strategy == "batchnorm" else nn.Identity()
            self.ln_fc1 = nn.LayerNorm(cfg.get("dense_size_1")) if normalization_strategy == "layernorm" else nn.Identity()

            self.fc2 = nn.Linear(cfg.get("dense_size_1"), num_classes)



        def forward(self, x):
            x = torch.relu(self.conv1(x))
            x = self.bn1(x)
            x = self.ln1(x)
            x = self.pool1(x)

            if self.conv2 is not None:
                x = torch.relu(self.conv2(x))
                x = self.bn2(x)
                x = self.ln2(x)
                x = self.pool2(x)


            if self.conv3 is not None:
                x = torch.relu(self.conv3(x))
                x = self.bn3(x)
                x = self.ln3(x)
                x = self.pool3(x)


            x = torch.flatten(x, 1)
            x = torch.relu(self.fc1(x))
            x = self.bn_fc1(x)
            x = self.ln_fc1(x)
            x = self.fc2(x)
            return x

    class MLP(nn.Module):
        def __init__(self, num_classes, cfg):
            super(MLP, self).__init__()
            dense_size_1 = cfg.get("dense_size_1")
            dense_size_2 = cfg.get("dense_size_2")
            dropout_rate = cfg.get("dropout_rate")
            normalization_strategy = cfg.get("normalization_strategy")
            num_dense_layers = cfg.get("num_dense_layers")


            self.fc1 = nn.Linear(X.shape[1] * X.shape[2] * X.shape[3], dense_size_1)
            self.bn1 = nn.BatchNorm1d(dense_size_1) if normalization_strategy == "batchnorm" else nn.Identity()
            self.ln1 = nn.LayerNorm(dense_size_1) if normalization_strategy == "layernorm" else nn.Identity()
            self.dropout1 = nn.Dropout(dropout_rate)

            self.fc2 = nn.Linear(dense_size_1, dense_size_2) if num_dense_layers == 2 else None
            self.bn2 = nn.BatchNorm1d(dense_size_2) if num_dense_layers == 2 and normalization_strategy == "batchnorm" else nn.Identity()
            self.ln2 = nn.LayerNorm(dense_size_2) if num_dense_layers == 2 and normalization_strategy == "layernorm" else nn.Identity()
            self.dropout2 = nn.Dropout(dropout_rate) if num_dense_layers == 2 else None

            self.fc_out = nn.Linear(dense_size_2 if num_dense_layers == 2 else dense_size_1, num_classes)

        def forward(self, x):
            x = torch.flatten(x, 1)
            x = torch.relu(self.fc1(x))
            x = self.bn1(x)
            x = self.ln1(x)
            x = self.dropout1(x)

            if self.fc2 is not None:
                x = torch.relu(self.fc2(x))
                x = self.bn2(x)
                x = self.ln2(x)
                x = self.dropout2(x)

            x = self.fc_out(x)
            return x

    if model_type == "cnn":
        model = CNN(num_classes, cfg).to(device)
    else: # model_type == "mlp"
        model = MLP(num_classes, cfg).to(device)


    # Optimizer definition
    if optimizer_name == "adam":
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    else:  # optimizer_name == "sgd"
        sgd_momentum = cfg.get("sgd_momentum")
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=sgd_momentum)

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model.train()
    for epoch in range(num_epochs):
        for i in range(0, n_samples, batch_size):
            X_batch = X[i:i + batch_size]
            y_batch = y[i:i + batch_size]

            # Forward pass
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)

            # Backward and optimize
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    return loss.item()
```
--------------------------------------------------------------------------------
[2025-06-15 18:54:35] [Metadata: {'component': 'train_function', 'action': 'run'}] import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from ConfigSpace import Configuration
from typing import Any
import math
import pandas as pd


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """Trains a model based on the given configuration and dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dictionary containing 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: The loss value after training.
    """
    torch.manual_seed(seed)
    np.random.seed(seed)

    X, y = dataset['X'], dataset['y']

    # Determine input shape and number of classes
    n_samples = X.shape[0]

    if isinstance(X, pd.DataFrame):
        X = X.values  # Convert DataFrame to NumPy array

    if len(X.shape) == 2:  # Flattened input
        n_features = X.shape[1]
        height = width = int(math.sqrt(n_features))
        assert height * width == n_features, "Input features are not square."
        X = X.reshape(n_samples, 1, height, width)  # Reshape to (N, 1, H, W)
    elif len(X.shape) == 3:  # 3D input (N, H, W)
        X = X.reshape(n_samples, 1, X.shape[1], X.shape[2])  # Add channel dimension
    elif len(X.shape) == 4: # 4D input
        pass  # Assume (N, C, H, W) or (N, H, W, C) and handle in model

    num_classes = len(np.unique(y))

    # Normalize data to [0, 1]
    X = X.astype(np.float32) / 255.0

    # Convert data to PyTorch tensors
    X = torch.tensor(X)
    y = torch.tensor(y, dtype=torch.long)  # Ensure labels are long type


    # Device configuration
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    X = X.to(device)
    y = y.to(device)

    # Extract hyperparameters from configuration
    model_type = cfg.get("model_type")
    batch_size = cfg.get("batch_size")
    learning_rate = cfg.get("learning_rate")
    optimizer_name = cfg.get("optimizer")
    num_epochs = 5 # Constant epochs for faster execution

    # Model definition
    class CNN(nn.Module):
        def __init__(self, num_classes, cfg):
            super(CNN, self).__init__()
            num_conv_layers = cfg.get("num_conv_layers")
            conv_filters_1 = cfg.get("conv_filters_1")
            conv_filters_2 = cfg.get("conv_filters_2") if num_conv_layers >= 2 else 0
            conv_filters_3 = cfg.get("conv_filters_3") if num_conv_layers == 3 else 0
            kernel_size = cfg.get("kernel_size")
            pooling_type = cfg.get("pooling_type")
            normalization_strategy = cfg.get("normalization_strategy")

            self.conv1 = nn.Conv2d(1, conv_filters_1, kernel_size=kernel_size)
            self.bn1 = nn.BatchNorm2d(conv_filters_1) if normalization_strategy == "batchnorm" else nn.Identity()
            self.ln1 = nn.LayerNorm([conv_filters_1, X.shape[2] - kernel_size + 1, X.shape[3] - kernel_size + 1]) if normalization_strategy == "layernorm" else nn.Identity()

            if pooling_type == "max":
                self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
            else:
                self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)

            self.conv2 = nn.Conv2d(conv_filters_1, conv_filters_2, kernel_size=kernel_size) if num_conv_layers >= 2 else None
            self.bn2 = nn.BatchNorm2d(conv_filters_2) if num_conv_layers >= 2 and normalization_strategy == "batchnorm" else nn.Identity()
            self.ln2 = nn.LayerNorm([conv_filters_2, (X.shape[2] - kernel_size + 1) // 2, (X.shape[3] - kernel_size + 1) // 2]) if num_conv_layers >= 2 and normalization_strategy == "layernorm" else nn.Identity()

            if pooling_type == "max":
                self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) if num_conv_layers >= 2 else None
            else:
                self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2) if num_conv_layers >= 2 else None


            self.conv3 = nn.Conv2d(conv_filters_2, conv_filters_3, kernel_size=kernel_size) if num_conv_layers == 3 else None
            self.bn3 = nn.BatchNorm2d(conv_filters_3) if num_conv_layers == 3 and normalization_strategy == "batchnorm" else nn.Identity()
            self.ln3 = nn.LayerNorm([conv_filters_3, ((X.shape[2] - kernel_size + 1) // 2) // 2, ((X.shape[3] - kernel_size + 1) // 2) // 2]) if num_conv_layers == 3 and normalization_strategy == "layernorm" else nn.Identity()

            if pooling_type == "max":
                self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) if num_conv_layers == 3 else None
            else:
                self.pool3 = nn.AvgPool2d(kernel_size=2, stride=2) if num_conv_layers == 3 else None


            # Calculate the output size of the convolutional layers
            if num_conv_layers == 1:
                conv_out_size = (X.shape[2] - kernel_size + 1) // 2 * (X.shape[3] - kernel_size + 1) // 2 * conv_filters_1
            elif num_conv_layers == 2:
                 conv_out_size = ((X.shape[2] - kernel_size + 1) // 2) // 2 * ((X.shape[3] - kernel_size + 1) // 2) // 2 * conv_filters_2
            else:  # num_conv_layers == 3
                conv_out_size = (((X.shape[2] - kernel_size + 1) // 2) // 2) // 2 * (((X.shape[3] - kernel_size + 1) // 2) // 2) // 2 * conv_filters_3


            self.fc1 = nn.Linear(conv_out_size, cfg.get("dense_size_1"))
            self.bn_fc1 = nn.BatchNorm1d(cfg.get("dense_size_1")) if normalization_strategy == "batchnorm" else nn.Identity()
            self.ln_fc1 = nn.LayerNorm(cfg.get("dense_size_1")) if normalization_strategy == "layernorm" else nn.Identity()

            self.fc2 = nn.Linear(cfg.get("dense_size_1"), num_classes)



        def forward(self, x):
            x = torch.relu(self.conv1(x))
            x = self.bn1(x)
            x = self.ln1(x)
            x = self.pool1(x)

            if self.conv2 is not None:
                x = torch.relu(self.conv2(x))
                x = self.bn2(x)
                x = self.ln2(x)
                x = self.pool2(x)


            if self.conv3 is not None:
                x = torch.relu(self.conv3(x))
                x = self.bn3(x)
                x = self.ln3(x)
                x = self.pool3(x)


            x = torch.flatten(x, 1)
            x = torch.relu(self.fc1(x))
            x = self.bn_fc1(x)
            x = self.ln_fc1(x)
            x = self.fc2(x)
            return x

    class MLP(nn.Module):
        def __init__(self, num_classes, cfg):
            super(MLP, self).__init__()
            dense_size_1 = cfg.get("dense_size_1")
            dense_size_2 = cfg.get("dense_size_2")
            dropout_rate = cfg.get("dropout_rate")
            normalization_strategy = cfg.get("normalization_strategy")
            num_dense_layers = cfg.get("num_dense_layers")


            self.fc1 = nn.Linear(X.shape[1] * X.shape[2] * X.shape[3], dense_size_1)
            self.bn1 = nn.BatchNorm1d(dense_size_1) if normalization_strategy == "batchnorm" else nn.Identity()
            self.ln1 = nn.LayerNorm(dense_size_1) if normalization_strategy == "layernorm" else nn.Identity()
            self.dropout1 = nn.Dropout(dropout_rate)

            self.fc2 = nn.Linear(dense_size_1, dense_size_2) if num_dense_layers == 2 else None
            self.bn2 = nn.BatchNorm1d(dense_size_2) if num_dense_layers == 2 and normalization_strategy == "batchnorm" else nn.Identity()
            self.ln2 = nn.LayerNorm(dense_size_2) if num_dense_layers == 2 and normalization_strategy == "layernorm" else nn.Identity()
            self.dropout2 = nn.Dropout(dropout_rate) if num_dense_layers == 2 else None

            self.fc_out = nn.Linear(dense_size_2 if num_dense_layers == 2 else dense_size_1, num_classes)

        def forward(self, x):
            x = torch.flatten(x, 1)
            x = torch.relu(self.fc1(x))
            x = self.bn1(x)
            x = self.ln1(x)
            x = self.dropout1(x)

            if self.fc2 is not None:
                x = torch.relu(self.fc2(x))
                x = self.bn2(x)
                x = self.ln2(x)
                x = self.dropout2(x)

            x = self.fc_out(x)
            return x

    if model_type == "cnn":
        model = CNN(num_classes, cfg).to(device)
    else: # model_type == "mlp"
        model = MLP(num_classes, cfg).to(device)


    # Optimizer definition
    if optimizer_name == "adam":
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    else:  # optimizer_name == "sgd"
        sgd_momentum = cfg.get("sgd_momentum")
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=sgd_momentum)

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model.train()
    for epoch in range(num_epochs):
        for i in range(0, n_samples, batch_size):
            X_batch = X[i:i + batch_size]
            y_batch = y[i:i + batch_size]

            # Forward pass
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)

            # Backward and optimize
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    return loss.item()

--------------------------------------------------------------------------------
[2025-06-15 18:54:35] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#2): empty() received an invalid combination of arguments - got (tuple, dtype=NoneType, device=NoneType), but expected one of:
 * (tuple of ints size, *, tuple of names names, torch.memory_format memory_format = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)
 * (tuple of ints size, *, torch.memory_format memory_format = None, Tensor out = None, torch.dtype dtype = None, torch.layout layout = None, torch.device device = None, bool pin_memory = False, bool requires_grad = False)

--------------------------------------------------------------------------------
[2025-06-15 18:54:48] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from ConfigSpace import Configuration
from typing import Any
import math
import pandas as pd


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """Trains a model based on the given configuration and dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dictionary containing 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: The loss value after training.
    """
    torch.manual_seed(seed)
    np.random.seed(seed)

    X, y = dataset['X'], dataset['y']

    # Determine input shape and number of classes
    n_samples = X.shape[0]

    if isinstance(X, pd.DataFrame):
        X = X.values  # Convert DataFrame to NumPy array

    if len(X.shape) == 2:  # Flattened input
        n_features = X.shape[1]
        height = width = int(math.sqrt(n_features))
        assert height * width == n_features, "Input features are not square."
        X = X.reshape(n_samples, 1, height, width)  # Reshape to (N, 1, H, W)
    elif len(X.shape) == 3:  # 3D input (N, H, W)
        X = X.reshape(n_samples, 1, X.shape[1], X.shape[2])  # Add channel dimension
    elif len(X.shape) == 4: # 4D input
        pass  # Assume (N, C, H, W) or (N, H, W, C) and handle in model

    num_classes = len(np.unique(y))

    # Normalize data to [0, 1]
    X = X.astype(np.float32) / 255.0

    # Convert data to PyTorch tensors
    X = torch.tensor(X)
    y = torch.tensor(y, dtype=torch.long)  # Ensure labels are long type


    # Device configuration
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    X = X.to(device)
    y = y.to(device)

    # Extract hyperparameters from configuration
    model_type = cfg.get("model_type")
    batch_size = cfg.get("batch_size")
    learning_rate = cfg.get("learning_rate")
    optimizer_name = cfg.get("optimizer")
    num_epochs = 5 # Constant epochs for faster execution

    # Model definition
    class CNN(nn.Module):
        def __init__(self, num_classes, cfg):
            super(CNN, self).__init__()
            num_conv_layers = cfg.get("num_conv_layers")
            conv_filters_1 = cfg.get("conv_filters_1")
            conv_filters_2 = cfg.get("conv_filters_2") if num_conv_layers >= 2 else 0
            conv_filters_3 = cfg.get("conv_filters_3") if num_conv_layers == 3 else 0
            kernel_size = cfg.get("kernel_size")
            pooling_type = cfg.get("pooling_type")
            normalization_strategy = cfg.get("normalization_strategy")

            self.conv1 = nn.Conv2d(1, conv_filters_1, kernel_size=kernel_size)
            self.bn1 = nn.BatchNorm2d(conv_filters_1) if normalization_strategy == "batchnorm" else nn.Identity()
            self.ln1 = nn.LayerNorm([conv_filters_1, X.shape[2] - kernel_size + 1, X.shape[3] - kernel_size + 1]) if normalization_strategy == "layernorm" else nn.Identity()

            if pooling_type == "max":
                self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
            else:
                self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)

            self.conv2 = nn.Conv2d(conv_filters_1, conv_filters_2, kernel_size=kernel_size) if num_conv_layers >= 2 else None
            self.bn2 = nn.BatchNorm2d(conv_filters_2) if num_conv_layers >= 2 and normalization_strategy == "batchnorm" else nn.Identity()
            self.ln2 = nn.LayerNorm([conv_filters_2, (X.shape[2] - kernel_size + 1) // 2, (X.shape[3] - kernel_size + 1) // 2]) if num_conv_layers >= 2 and normalization_strategy == "layernorm" else nn.Identity()

            if pooling_type == "max":
                self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) if num_conv_layers >= 2 else None
            else:
                self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2) if num_conv_layers >= 2 else None


            self.conv3 = nn.Conv2d(conv_filters_2, conv_filters_3, kernel_size=kernel_size) if num_conv_layers == 3 else None
            self.bn3 = nn.BatchNorm2d(conv_filters_3) if num_conv_layers == 3 and normalization_strategy == "batchnorm" else nn.Identity()
            self.ln3 = nn.LayerNorm([conv_filters_3, ((X.shape[2] - kernel_size + 1) // 2) // 2, ((X.shape[3] - kernel_size + 1) // 2) // 2]) if num_conv_layers == 3 and normalization_strategy == "layernorm" else nn.Identity()

            if pooling_type == "max":
                self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) if num_conv_layers == 3 else None
            else:
                self.pool3 = nn.AvgPool2d(kernel_size=2, stride=2) if num_conv_layers == 3 else None


            # Calculate the output size of the convolutional layers
            if num_conv_layers == 1:
                conv_out_size = (X.shape[2] - kernel_size + 1) // 2 * (X.shape[3] - kernel_size + 1) // 2 * conv_filters_1
            elif num_conv_layers == 2:
                 conv_out_size = ((X.shape[2] - kernel_size + 1) // 2) // 2 * ((X.shape[3] - kernel_size + 1) // 2) // 2 * conv_filters_2
            else:  # num_conv_layers == 3
                conv_out_size = (((X.shape[2] - kernel_size + 1) // 2) // 2) // 2 * (((X.shape[3] - kernel_size + 1) // 2) // 2) // 2 * conv_filters_3


            self.fc1 = nn.Linear(conv_out_size, cfg.get("dense_size_1"))
            self.bn_fc1 = nn.BatchNorm1d(cfg.get("dense_size_1")) if normalization_strategy == "batchnorm" else nn.Identity()
            self.ln_fc1 = nn.LayerNorm(cfg.get("dense_size_1")) if normalization_strategy == "layernorm" else nn.Identity()

            self.fc2 = nn.Linear(cfg.get("dense_size_1"), num_classes)



        def forward(self, x):
            x = torch.relu(self.conv1(x))
            x = self.bn1(x)
            x = self.ln1(x)
            x = self.pool1(x)

            if self.conv2 is not None:
                x = torch.relu(self.conv2(x))
                x = self.bn2(x)
                x = self.ln2(x)
                x = self.pool2(x)


            if self.conv3 is not None:
                x = torch.relu(self.conv3(x))
                x = self.bn3(x)
                x = self.ln3(x)
                x = self.pool3(x)


            x = torch.flatten(x, 1)
            x = torch.relu(self.fc1(x))
            x = self.bn_fc1(x)
            x = self.ln_fc1(x)
            x = self.fc2(x)
            return x

    class MLP(nn.Module):
        def __init__(self, num_classes, cfg):
            super(MLP, self).__init__()
            dense_size_1 = cfg.get("dense_size_1")
            dense_size_2 = cfg.get("dense_size_2")
            dropout_rate = cfg.get("dropout_rate")
            normalization_strategy = cfg.get("normalization_strategy")
            num_dense_layers = cfg.get("num_dense_layers")


            self.fc1 = nn.Linear(X.shape[1] * X.shape[2] * X.shape[3], dense_size_1)
            self.bn1 = nn.BatchNorm1d(dense_size_1) if normalization_strategy == "batchnorm" else nn.Identity()
            self.ln1 = nn.LayerNorm(dense_size_1) if normalization_strategy == "layernorm" else nn.Identity()
            self.dropout1 = nn.Dropout(dropout_rate)

            self.fc2 = nn.Linear(dense_size_1, dense_size_2) if num_dense_layers == 2 else None
            self.bn2 = nn.BatchNorm1d(dense_size_2) if num_dense_layers == 2 and normalization_strategy == "batchnorm" else nn.Identity()
            self.ln2 = nn.LayerNorm(dense_size_2) if num_dense_layers == 2 and normalization_strategy == "layernorm" else nn.Identity()
            self.dropout2 = nn.Dropout(dropout_rate) if num_dense_layers == 2 else None

            self.fc_out = nn.Linear(dense_size_2 if num_dense_layers == 2 else dense_size_1, num_classes)

        def forward(self, x):
            x = torch.flatten(x, 1)
            x = torch.relu(self.fc1(x))
            x = self.bn1(x)
            x = self.ln1(x)
            x = self.dropout1(x)

            if self.fc2 is not None:
                x = torch.relu(self.fc2(x))
                x = self.bn2(x)
                x = self.ln2(x)
                x = self.dropout2(x)

            x = self.fc_out(x)
            return x

    if model_type == "cnn":
        model = CNN(num_classes, cfg).to(device)
    else: # model_type == "mlp"
        model = MLP(num_classes, cfg).to(device)


    # Optimizer definition
    if optimizer_name == "adam":
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    else:  # optimizer_name == "sgd"
        sgd_momentum = cfg.get("sgd_momentum")
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=sgd_momentum)

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model.train()
    for epoch in range(num_epochs):
        for i in range(0, n_samples, batch_size):
            X_batch = X[i:i + batch_size]
            y_batch = y[i:i + batch_size]

            # Forward pass
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)

            # Backward and optimize
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    return loss.item()
```
--------------------------------------------------------------------------------
[2025-06-15 18:54:48] [Metadata: {'component': 'train_function', 'action': 'run'}] import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from ConfigSpace import Configuration
from typing import Any
import math
import pandas as pd


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """Trains a model based on the given configuration and dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dictionary containing 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: The loss value after training.
    """
    torch.manual_seed(seed)
    np.random.seed(seed)

    X, y = dataset['X'], dataset['y']

    # Determine input shape and number of classes
    n_samples = X.shape[0]

    if isinstance(X, pd.DataFrame):
        X = X.values  # Convert DataFrame to NumPy array

    if len(X.shape) == 2:  # Flattened input
        n_features = X.shape[1]
        height = width = int(math.sqrt(n_features))
        assert height * width == n_features, "Input features are not square."
        X = X.reshape(n_samples, 1, height, width)  # Reshape to (N, 1, H, W)
    elif len(X.shape) == 3:  # 3D input (N, H, W)
        X = X.reshape(n_samples, 1, X.shape[1], X.shape[2])  # Add channel dimension
    elif len(X.shape) == 4: # 4D input
        pass  # Assume (N, C, H, W) or (N, H, W, C) and handle in model

    num_classes = len(np.unique(y))

    # Normalize data to [0, 1]
    X = X.astype(np.float32) / 255.0

    # Convert data to PyTorch tensors
    X = torch.tensor(X)
    y = torch.tensor(y, dtype=torch.long)  # Ensure labels are long type


    # Device configuration
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    X = X.to(device)
    y = y.to(device)

    # Extract hyperparameters from configuration
    model_type = cfg.get("model_type")
    batch_size = cfg.get("batch_size")
    learning_rate = cfg.get("learning_rate")
    optimizer_name = cfg.get("optimizer")
    num_epochs = 5 # Constant epochs for faster execution

    # Model definition
    class CNN(nn.Module):
        def __init__(self, num_classes, cfg):
            super(CNN, self).__init__()
            num_conv_layers = cfg.get("num_conv_layers")
            conv_filters_1 = cfg.get("conv_filters_1")
            conv_filters_2 = cfg.get("conv_filters_2") if num_conv_layers >= 2 else 0
            conv_filters_3 = cfg.get("conv_filters_3") if num_conv_layers == 3 else 0
            kernel_size = cfg.get("kernel_size")
            pooling_type = cfg.get("pooling_type")
            normalization_strategy = cfg.get("normalization_strategy")

            self.conv1 = nn.Conv2d(1, conv_filters_1, kernel_size=kernel_size)
            self.bn1 = nn.BatchNorm2d(conv_filters_1) if normalization_strategy == "batchnorm" else nn.Identity()
            self.ln1 = nn.LayerNorm([conv_filters_1, X.shape[2] - kernel_size + 1, X.shape[3] - kernel_size + 1]) if normalization_strategy == "layernorm" else nn.Identity()

            if pooling_type == "max":
                self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
            else:
                self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)

            self.conv2 = nn.Conv2d(conv_filters_1, conv_filters_2, kernel_size=kernel_size) if num_conv_layers >= 2 else None
            self.bn2 = nn.BatchNorm2d(conv_filters_2) if num_conv_layers >= 2 and normalization_strategy == "batchnorm" else nn.Identity()
            self.ln2 = nn.LayerNorm([conv_filters_2, (X.shape[2] - kernel_size + 1) // 2, (X.shape[3] - kernel_size + 1) // 2]) if num_conv_layers >= 2 and normalization_strategy == "layernorm" else nn.Identity()

            if pooling_type == "max":
                self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2) if num_conv_layers >= 2 else None
            else:
                self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2) if num_conv_layers >= 2 else None


            self.conv3 = nn.Conv2d(conv_filters_2, conv_filters_3, kernel_size=kernel_size) if num_conv_layers == 3 else None
            self.bn3 = nn.BatchNorm2d(conv_filters_3) if num_conv_layers == 3 and normalization_strategy == "batchnorm" else nn.Identity()
            self.ln3 = nn.LayerNorm([conv_filters_3, ((X.shape[2] - kernel_size + 1) // 2) // 2, ((X.shape[3] - kernel_size + 1) // 2) // 2]) if num_conv_layers == 3 and normalization_strategy == "layernorm" else nn.Identity()

            if pooling_type == "max":
                self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2) if num_conv_layers == 3 else None
            else:
                self.pool3 = nn.AvgPool2d(kernel_size=2, stride=2) if num_conv_layers == 3 else None


            # Calculate the output size of the convolutional layers
            if num_conv_layers == 1:
                conv_out_size = (X.shape[2] - kernel_size + 1) // 2 * (X.shape[3] - kernel_size + 1) // 2 * conv_filters_1
            elif num_conv_layers == 2:
                 conv_out_size = ((X.shape[2] - kernel_size + 1) // 2) // 2 * ((X.shape[3] - kernel_size + 1) // 2) // 2 * conv_filters_2
            else:  # num_conv_layers == 3
                conv_out_size = (((X.shape[2] - kernel_size + 1) // 2) // 2) // 2 * (((X.shape[3] - kernel_size + 1) // 2) // 2) // 2 * conv_filters_3


            self.fc1 = nn.Linear(conv_out_size, cfg.get("dense_size_1"))
            self.bn_fc1 = nn.BatchNorm1d(cfg.get("dense_size_1")) if normalization_strategy == "batchnorm" else nn.Identity()
            self.ln_fc1 = nn.LayerNorm(cfg.get("dense_size_1")) if normalization_strategy == "layernorm" else nn.Identity()

            self.fc2 = nn.Linear(cfg.get("dense_size_1"), num_classes)



        def forward(self, x):
            x = torch.relu(self.conv1(x))
            x = self.bn1(x)
            x = self.ln1(x)
            x = self.pool1(x)

            if self.conv2 is not None:
                x = torch.relu(self.conv2(x))
                x = self.bn2(x)
                x = self.ln2(x)
                x = self.pool2(x)


            if self.conv3 is not None:
                x = torch.relu(self.conv3(x))
                x = self.bn3(x)
                x = self.ln3(x)
                x = self.pool3(x)


            x = torch.flatten(x, 1)
            x = torch.relu(self.fc1(x))
            x = self.bn_fc1(x)
            x = self.ln_fc1(x)
            x = self.fc2(x)
            return x

    class MLP(nn.Module):
        def __init__(self, num_classes, cfg):
            super(MLP, self).__init__()
            dense_size_1 = cfg.get("dense_size_1")
            dense_size_2 = cfg.get("dense_size_2")
            dropout_rate = cfg.get("dropout_rate")
            normalization_strategy = cfg.get("normalization_strategy")
            num_dense_layers = cfg.get("num_dense_layers")


            self.fc1 = nn.Linear(X.shape[1] * X.shape[2] * X.shape[3], dense_size_1)
            self.bn1 = nn.BatchNorm1d(dense_size_1) if normalization_strategy == "batchnorm" else nn.Identity()
            self.ln1 = nn.LayerNorm(dense_size_1) if normalization_strategy == "layernorm" else nn.Identity()
            self.dropout1 = nn.Dropout(dropout_rate)

            self.fc2 = nn.Linear(dense_size_1, dense_size_2) if num_dense_layers == 2 else None
            self.bn2 = nn.BatchNorm1d(dense_size_2) if num_dense_layers == 2 and normalization_strategy == "batchnorm" else nn.Identity()
            self.ln2 = nn.LayerNorm(dense_size_2) if num_dense_layers == 2 and normalization_strategy == "layernorm" else nn.Identity()
            self.dropout2 = nn.Dropout(dropout_rate) if num_dense_layers == 2 else None

            self.fc_out = nn.Linear(dense_size_2 if num_dense_layers == 2 else dense_size_1, num_classes)

        def forward(self, x):
            x = torch.flatten(x, 1)
            x = torch.relu(self.fc1(x))
            x = self.bn1(x)
            x = self.ln1(x)
            x = self.dropout1(x)

            if self.fc2 is not None:
                x = torch.relu(self.fc2(x))
                x = self.bn2(x)
                x = self.ln2(x)
                x = self.dropout2(x)

            x = self.fc_out(x)
            return x

    if model_type == "cnn":
        model = CNN(num_classes, cfg).to(device)
    else: # model_type == "mlp"
        model = MLP(num_classes, cfg).to(device)


    # Optimizer definition
    if optimizer_name == "adam":
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    else:  # optimizer_name == "sgd"
        sgd_momentum = cfg.get("sgd_momentum")
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=sgd_momentum)

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model.train()
    for epoch in range(num_epochs):
        for i in range(0, n_samples, batch_size):
            X_batch = X[i:i + batch_size]
            y_batch = y[i:i + batch_size]

            # Forward pass
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)

            # Backward and optimize
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

    return loss.item()

--------------------------------------------------------------------------------
[2025-06-15 18:54:50] [Metadata: {'component': 'train_function', 'status': 'success', 'loss': 0.24597209692001343}] Training executed successfully, loss: 0.24597209692001343
--------------------------------------------------------------------------------
