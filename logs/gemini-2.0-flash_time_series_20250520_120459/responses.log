[2025-05-20 12:05:03] [Metadata: {'component': 'config'}] ```python
from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause

def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical('model_type', ['MLP', 'LSTM', 'GRU', 'RNN', 'TCN'], default='MLP')
    learning_rate = Float('learning_rate', (1e-5, 1e-2), default=1e-3, log=True)
    batch_size = Categorical('batch_size', [16, 32, 64, 128], default=32)
    optimizer = Categorical('optimizer', ['Adam', 'SGD', 'RMSprop'], default='Adam')
    num_layers = Integer('num_layers', (1, 3), default=2)
    dropout_rate = Float('dropout_rate', (0.0, 0.9), default=0.5)

    # MLP Specific
    mlp_hidden_dim = Integer('mlp_hidden_dim', (32, 256), default=128, log=True)

    # RNN Specific
    rnn_hidden_dim = Integer('rnn_hidden_dim', (32, 256), default=64, log=True)
    
    # TCN Specific
    tcn_num_filters = Integer('tcn_num_filters', (32, 256), default=64, log=True)
    tcn_kernel_size = Integer('tcn_kernel_size', (2, 8), default=3)

    # Add hyperparameters to the configuration space
    cs.add([model_type, learning_rate, batch_size, optimizer, num_layers, dropout_rate])

    # Add hyperparameters specific to MLP
    cs.add([mlp_hidden_dim])
    
    # Add hyperparameters specific to RNN
    cs.add([rnn_hidden_dim])
    
    # Add hyperparameters specific to TCN
    cs.add([tcn_num_filters, tcn_kernel_size])

    # Add Forbidden Clauses
    forbidden_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'LSTM'),
        ForbiddenEqualsClause(mlp_hidden_dim, mlp_hidden_dim.default)
    )

    forbidden_lstm = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'MLP'),
        ForbiddenEqualsClause(rnn_hidden_dim, rnn_hidden_dim.default)
    )
    
    forbidden_tcn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'MLP'),
        ForbiddenEqualsClause(tcn_num_filters, tcn_num_filters.default)
    )

    cs.add_forbidden_clause(forbidden_mlp)
    cs.add_forbidden_clause(forbidden_lstm)
    cs.add_forbidden_clause(forbidden_tcn)

    return cs
```
--------------------------------------------------------------------------------
[2025-05-20 12:05:03] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause

def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical('model_type', ['MLP', 'LSTM', 'GRU', 'RNN', 'TCN'], default='MLP')
    learning_rate = Float('learning_rate', (1e-5, 1e-2), default=1e-3, log=True)
    batch_size = Categorical('batch_size', [16, 32, 64, 128], default=32)
    optimizer = Categorical('optimizer', ['Adam', 'SGD', 'RMSprop'], default='Adam')
    num_layers = Integer('num_layers', (1, 3), default=2)
    dropout_rate = Float('dropout_rate', (0.0, 0.9), default=0.5)

    # MLP Specific
    mlp_hidden_dim = Integer('mlp_hidden_dim', (32, 256), default=128, log=True)

    # RNN Specific
    rnn_hidden_dim = Integer('rnn_hidden_dim', (32, 256), default=64, log=True)
    
    # TCN Specific
    tcn_num_filters = Integer('tcn_num_filters', (32, 256), default=64, log=True)
    tcn_kernel_size = Integer('tcn_kernel_size', (2, 8), default=3)

    # Add hyperparameters to the configuration space
    cs.add([model_type, learning_rate, batch_size, optimizer, num_layers, dropout_rate])

    # Add hyperparameters specific to MLP
    cs.add([mlp_hidden_dim])
    
    # Add hyperparameters specific to RNN
    cs.add([rnn_hidden_dim])
    
    # Add hyperparameters specific to TCN
    cs.add([tcn_num_filters, tcn_kernel_size])

    # Add Forbidden Clauses
    forbidden_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'LSTM'),
        ForbiddenEqualsClause(mlp_hidden_dim, mlp_hidden_dim.default)
    )

    forbidden_lstm = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'MLP'),
        ForbiddenEqualsClause(rnn_hidden_dim, rnn_hidden_dim.default)
    )
    
    forbidden_tcn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'MLP'),
        ForbiddenEqualsClause(tcn_num_filters, tcn_num_filters.default)
    )

    cs.add_forbidden_clause(forbidden_mlp)
    cs.add_forbidden_clause(forbidden_lstm)
    cs.add_forbidden_clause(forbidden_tcn)

    return cs

--------------------------------------------------------------------------------
[2025-05-20 12:05:03] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#1): 'UniformIntegerHyperparameter' object has no attribute 'default'
--------------------------------------------------------------------------------
[2025-05-20 12:05:06] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause

def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical('model_type', ['MLP', 'LSTM', 'GRU', 'RNN', 'TCN'], default='MLP')
    learning_rate = Float('learning_rate', 1e-5, 1e-2, default=1e-3, log=True)
    batch_size = Categorical('batch_size', [16, 32, 64, 128], default=32)
    optimizer = Categorical('optimizer', ['Adam', 'SGD', 'RMSprop'], default='Adam')
    num_layers = Integer('num_layers', 1, 3, default=2)
    dropout_rate = Float('dropout_rate', 0.0, 0.9, default=0.5)

    # MLP Specific
    mlp_hidden_dim = Integer('mlp_hidden_dim', 32, 256, default=128, log=True)

    # RNN Specific
    rnn_hidden_dim = Integer('rnn_hidden_dim', 32, 256, default=64, log=True)
    
    # TCN Specific
    tcn_num_filters = Integer('tcn_num_filters', 32, 256, default=64, log=True)
    tcn_kernel_size = Integer('tcn_kernel_size', 2, 8, default=3)

    # Add hyperparameters to the configuration space
    cs.add_hyperparameters([model_type, learning_rate, batch_size, optimizer, num_layers, dropout_rate])

    # Add hyperparameters specific to MLP
    cs.add_hyperparameters([mlp_hidden_dim])
    
    # Add hyperparameters specific to RNN
    cs.add_hyperparameters([rnn_hidden_dim])
    
    # Add hyperparameters specific to TCN
    cs.add_hyperparameters([tcn_num_filters, tcn_kernel_size])

    # Add Forbidden Clauses
    forbidden_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'LSTM'),
        ForbiddenEqualsClause(mlp_hidden_dim, 128)
    )

    forbidden_lstm = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'MLP'),
        ForbiddenEqualsClause(rnn_hidden_dim, 64)
    )
    
    forbidden_tcn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'MLP'),
        ForbiddenEqualsClause(tcn_num_filters, 64)
    )

    cs.add_forbidden_clause(forbidden_mlp)
    cs.add_forbidden_clause(forbidden_lstm)
    cs.add_forbidden_clause(forbidden_tcn)

    return cs
```
--------------------------------------------------------------------------------
[2025-05-20 12:05:06] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause

def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical('model_type', ['MLP', 'LSTM', 'GRU', 'RNN', 'TCN'], default='MLP')
    learning_rate = Float('learning_rate', 1e-5, 1e-2, default=1e-3, log=True)
    batch_size = Categorical('batch_size', [16, 32, 64, 128], default=32)
    optimizer = Categorical('optimizer', ['Adam', 'SGD', 'RMSprop'], default='Adam')
    num_layers = Integer('num_layers', 1, 3, default=2)
    dropout_rate = Float('dropout_rate', 0.0, 0.9, default=0.5)

    # MLP Specific
    mlp_hidden_dim = Integer('mlp_hidden_dim', 32, 256, default=128, log=True)

    # RNN Specific
    rnn_hidden_dim = Integer('rnn_hidden_dim', 32, 256, default=64, log=True)
    
    # TCN Specific
    tcn_num_filters = Integer('tcn_num_filters', 32, 256, default=64, log=True)
    tcn_kernel_size = Integer('tcn_kernel_size', 2, 8, default=3)

    # Add hyperparameters to the configuration space
    cs.add_hyperparameters([model_type, learning_rate, batch_size, optimizer, num_layers, dropout_rate])

    # Add hyperparameters specific to MLP
    cs.add_hyperparameters([mlp_hidden_dim])
    
    # Add hyperparameters specific to RNN
    cs.add_hyperparameters([rnn_hidden_dim])
    
    # Add hyperparameters specific to TCN
    cs.add_hyperparameters([tcn_num_filters, tcn_kernel_size])

    # Add Forbidden Clauses
    forbidden_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'LSTM'),
        ForbiddenEqualsClause(mlp_hidden_dim, 128)
    )

    forbidden_lstm = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'MLP'),
        ForbiddenEqualsClause(rnn_hidden_dim, 64)
    )
    
    forbidden_tcn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'MLP'),
        ForbiddenEqualsClause(tcn_num_filters, 64)
    )

    cs.add_forbidden_clause(forbidden_mlp)
    cs.add_forbidden_clause(forbidden_lstm)
    cs.add_forbidden_clause(forbidden_tcn)

    return cs

--------------------------------------------------------------------------------
[2025-05-20 12:05:06] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#2): Float() takes 2 positional arguments but 3 positional arguments (and 2 keyword-only arguments) were given
--------------------------------------------------------------------------------
[2025-05-20 12:05:10] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause

def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical('model_type', ['MLP', 'LSTM', 'GRU', 'RNN', 'TCN'], default_value='MLP')
    learning_rate = Float('learning_rate', lower=1e-5, upper=1e-2, default_value=1e-3, log=True)
    batch_size = Categorical('batch_size', [16, 32, 64, 128], default_value=32)
    optimizer = Categorical('optimizer', ['Adam', 'SGD', 'RMSprop'], default_value='Adam')
    num_layers = Integer('num_layers', lower=1, upper=3, default_value=2)
    dropout_rate = Float('dropout_rate', lower=0.0, upper=0.9, default_value=0.5)

    # MLP Specific
    mlp_hidden_dim = Integer('mlp_hidden_dim', lower=32, upper=256, default_value=128, log=True)

    # RNN Specific
    rnn_hidden_dim = Integer('rnn_hidden_dim', lower=32, upper=256, default_value=64, log=True)
    
    # TCN Specific
    tcn_num_filters = Integer('tcn_num_filters', lower=32, upper=256, default_value=64, log=True)
    tcn_kernel_size = Integer('tcn_kernel_size', lower=2, upper=8, default_value=3)

    # Add hyperparameters to the configuration space
    cs.add_hyperparameters([model_type, learning_rate, batch_size, optimizer, num_layers, dropout_rate])

    # Add hyperparameters specific to MLP
    cs.add_hyperparameters([mlp_hidden_dim])
    
    # Add hyperparameters specific to RNN
    cs.add_hyperparameters([rnn_hidden_dim])
    
    # Add hyperparameters specific to TCN
    cs.add_hyperparameters([tcn_num_filters, tcn_kernel_size])

    # Add Forbidden Clauses
    forbidden_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'LSTM'),
        ForbiddenEqualsClause(mlp_hidden_dim, 128)
    )

    forbidden_lstm = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'MLP'),
        ForbiddenEqualsClause(rnn_hidden_dim, 64)
    )
    
    forbidden_tcn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'MLP'),
        ForbiddenEqualsClause(tcn_num_filters, 64)
    )

    cs.add_forbidden_clause(forbidden_mlp)
    cs.add_forbidden_clause(forbidden_lstm)
    cs.add_forbidden_clause(forbidden_tcn)

    return cs
```
--------------------------------------------------------------------------------
[2025-05-20 12:05:10] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause

def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical('model_type', ['MLP', 'LSTM', 'GRU', 'RNN', 'TCN'], default_value='MLP')
    learning_rate = Float('learning_rate', lower=1e-5, upper=1e-2, default_value=1e-3, log=True)
    batch_size = Categorical('batch_size', [16, 32, 64, 128], default_value=32)
    optimizer = Categorical('optimizer', ['Adam', 'SGD', 'RMSprop'], default_value='Adam')
    num_layers = Integer('num_layers', lower=1, upper=3, default_value=2)
    dropout_rate = Float('dropout_rate', lower=0.0, upper=0.9, default_value=0.5)

    # MLP Specific
    mlp_hidden_dim = Integer('mlp_hidden_dim', lower=32, upper=256, default_value=128, log=True)

    # RNN Specific
    rnn_hidden_dim = Integer('rnn_hidden_dim', lower=32, upper=256, default_value=64, log=True)
    
    # TCN Specific
    tcn_num_filters = Integer('tcn_num_filters', lower=32, upper=256, default_value=64, log=True)
    tcn_kernel_size = Integer('tcn_kernel_size', lower=2, upper=8, default_value=3)

    # Add hyperparameters to the configuration space
    cs.add_hyperparameters([model_type, learning_rate, batch_size, optimizer, num_layers, dropout_rate])

    # Add hyperparameters specific to MLP
    cs.add_hyperparameters([mlp_hidden_dim])
    
    # Add hyperparameters specific to RNN
    cs.add_hyperparameters([rnn_hidden_dim])
    
    # Add hyperparameters specific to TCN
    cs.add_hyperparameters([tcn_num_filters, tcn_kernel_size])

    # Add Forbidden Clauses
    forbidden_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'LSTM'),
        ForbiddenEqualsClause(mlp_hidden_dim, 128)
    )

    forbidden_lstm = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'MLP'),
        ForbiddenEqualsClause(rnn_hidden_dim, 64)
    )
    
    forbidden_tcn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'MLP'),
        ForbiddenEqualsClause(tcn_num_filters, 64)
    )

    cs.add_forbidden_clause(forbidden_mlp)
    cs.add_forbidden_clause(forbidden_lstm)
    cs.add_forbidden_clause(forbidden_tcn)

    return cs

--------------------------------------------------------------------------------
[2025-05-20 12:05:10] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#3): Categorical() got an unexpected keyword argument 'default_value'
--------------------------------------------------------------------------------
[2025-05-20 12:05:13] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause

def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical('model_type', ['MLP', 'LSTM', 'GRU', 'RNN', 'TCN'])
    learning_rate = Float('learning_rate', lower=1e-5, upper=1e-2, log=True)
    batch_size = Categorical('batch_size', [16, 32, 64, 128])
    optimizer = Categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])
    num_layers = Integer('num_layers', lower=1, upper=3)
    dropout_rate = Float('dropout_rate', lower=0.0, upper=0.9)

    # MLP Specific
    mlp_hidden_dim = Integer('mlp_hidden_dim', lower=32, upper=256, log=True)

    # RNN Specific
    rnn_hidden_dim = Integer('rnn_hidden_dim', lower=32, upper=256, log=True)
    
    # TCN Specific
    tcn_num_filters = Integer('tcn_num_filters', lower=32, upper=256, log=True)
    tcn_kernel_size = Integer('tcn_kernel_size', lower=2, upper=8)

    # Add hyperparameters to the configuration space
    cs.add_hyperparameters([model_type, learning_rate, batch_size, optimizer, num_layers, dropout_rate])

    # Add hyperparameters specific to MLP
    cs.add_hyperparameters([mlp_hidden_dim])
    
    # Add hyperparameters specific to RNN
    cs.add_hyperparameters([rnn_hidden_dim])
    
    # Add hyperparameters specific to TCN
    cs.add_hyperparameters([tcn_num_filters, tcn_kernel_size])

    # Add Forbidden Clauses
    forbidden_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'LSTM'),
        ForbiddenEqualsClause(mlp_hidden_dim, 128)
    )

    forbidden_lstm = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'MLP'),
        ForbiddenEqualsClause(rnn_hidden_dim, 64)
    )
    
    forbidden_tcn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'MLP'),
        ForbiddenEqualsClause(tcn_num_filters, 64)
    )

    cs.add_forbidden_clause(forbidden_mlp)
    cs.add_forbidden_clause(forbidden_lstm)
    cs.add_forbidden_clause(forbidden_tcn)

    return cs
```
--------------------------------------------------------------------------------
[2025-05-20 12:05:13] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause

def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical('model_type', ['MLP', 'LSTM', 'GRU', 'RNN', 'TCN'])
    learning_rate = Float('learning_rate', lower=1e-5, upper=1e-2, log=True)
    batch_size = Categorical('batch_size', [16, 32, 64, 128])
    optimizer = Categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])
    num_layers = Integer('num_layers', lower=1, upper=3)
    dropout_rate = Float('dropout_rate', lower=0.0, upper=0.9)

    # MLP Specific
    mlp_hidden_dim = Integer('mlp_hidden_dim', lower=32, upper=256, log=True)

    # RNN Specific
    rnn_hidden_dim = Integer('rnn_hidden_dim', lower=32, upper=256, log=True)
    
    # TCN Specific
    tcn_num_filters = Integer('tcn_num_filters', lower=32, upper=256, log=True)
    tcn_kernel_size = Integer('tcn_kernel_size', lower=2, upper=8)

    # Add hyperparameters to the configuration space
    cs.add_hyperparameters([model_type, learning_rate, batch_size, optimizer, num_layers, dropout_rate])

    # Add hyperparameters specific to MLP
    cs.add_hyperparameters([mlp_hidden_dim])
    
    # Add hyperparameters specific to RNN
    cs.add_hyperparameters([rnn_hidden_dim])
    
    # Add hyperparameters specific to TCN
    cs.add_hyperparameters([tcn_num_filters, tcn_kernel_size])

    # Add Forbidden Clauses
    forbidden_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'LSTM'),
        ForbiddenEqualsClause(mlp_hidden_dim, 128)
    )

    forbidden_lstm = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'MLP'),
        ForbiddenEqualsClause(rnn_hidden_dim, 64)
    )
    
    forbidden_tcn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'MLP'),
        ForbiddenEqualsClause(tcn_num_filters, 64)
    )

    cs.add_forbidden_clause(forbidden_mlp)
    cs.add_forbidden_clause(forbidden_lstm)
    cs.add_forbidden_clause(forbidden_tcn)

    return cs

--------------------------------------------------------------------------------
[2025-05-20 12:05:13] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#4): Float() got an unexpected keyword argument 'lower'
--------------------------------------------------------------------------------
[2025-05-20 12:05:17] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause

def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical('model_type', choices=['MLP', 'LSTM', 'GRU', 'RNN', 'TCN'])
    learning_rate = Float('learning_rate', low=1e-5, high=1e-2, log=True)
    batch_size = Categorical('batch_size', choices=[16, 32, 64, 128])
    optimizer = Categorical('optimizer', choices=['Adam', 'SGD', 'RMSprop'])
    num_layers = Integer('num_layers', lower=1, upper=3)
    dropout_rate = Float('dropout_rate', low=0.0, high=0.9)

    # MLP Specific
    mlp_hidden_dim = Integer('mlp_hidden_dim', lower=32, upper=256, log=True)

    # RNN Specific
    rnn_hidden_dim = Integer('rnn_hidden_dim', lower=32, upper=256, log=True)
    
    # TCN Specific
    tcn_num_filters = Integer('tcn_num_filters', lower=32, upper=256, log=True)
    tcn_kernel_size = Integer('tcn_kernel_size', lower=2, upper=8)

    # Add hyperparameters to the configuration space
    cs.add_hyperparameters([model_type, learning_rate, batch_size, optimizer, num_layers, dropout_rate])

    # Add hyperparameters specific to MLP
    cs.add_hyperparameters([mlp_hidden_dim])
    
    # Add hyperparameters specific to RNN
    cs.add_hyperparameters([rnn_hidden_dim])
    
    # Add hyperparameters specific to TCN
    cs.add_hyperparameters([tcn_num_filters, tcn_kernel_size])

    # Add Forbidden Clauses
    forbidden_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'LSTM'),
        ForbiddenEqualsClause(mlp_hidden_dim, 128)
    )

    forbidden_lstm = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'MLP'),
        ForbiddenEqualsClause(rnn_hidden_dim, 64)
    )
    
    forbidden_tcn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'MLP'),
        ForbiddenEqualsClause(tcn_num_filters, 64)
    )

    cs.add_forbidden_clause(forbidden_mlp)
    cs.add_forbidden_clause(forbidden_lstm)
    cs.add_forbidden_clause(forbidden_tcn)

    return cs
```
--------------------------------------------------------------------------------
[2025-05-20 12:05:17] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause

def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical('model_type', choices=['MLP', 'LSTM', 'GRU', 'RNN', 'TCN'])
    learning_rate = Float('learning_rate', low=1e-5, high=1e-2, log=True)
    batch_size = Categorical('batch_size', choices=[16, 32, 64, 128])
    optimizer = Categorical('optimizer', choices=['Adam', 'SGD', 'RMSprop'])
    num_layers = Integer('num_layers', lower=1, upper=3)
    dropout_rate = Float('dropout_rate', low=0.0, high=0.9)

    # MLP Specific
    mlp_hidden_dim = Integer('mlp_hidden_dim', lower=32, upper=256, log=True)

    # RNN Specific
    rnn_hidden_dim = Integer('rnn_hidden_dim', lower=32, upper=256, log=True)
    
    # TCN Specific
    tcn_num_filters = Integer('tcn_num_filters', lower=32, upper=256, log=True)
    tcn_kernel_size = Integer('tcn_kernel_size', lower=2, upper=8)

    # Add hyperparameters to the configuration space
    cs.add_hyperparameters([model_type, learning_rate, batch_size, optimizer, num_layers, dropout_rate])

    # Add hyperparameters specific to MLP
    cs.add_hyperparameters([mlp_hidden_dim])
    
    # Add hyperparameters specific to RNN
    cs.add_hyperparameters([rnn_hidden_dim])
    
    # Add hyperparameters specific to TCN
    cs.add_hyperparameters([tcn_num_filters, tcn_kernel_size])

    # Add Forbidden Clauses
    forbidden_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'LSTM'),
        ForbiddenEqualsClause(mlp_hidden_dim, 128)
    )

    forbidden_lstm = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'MLP'),
        ForbiddenEqualsClause(rnn_hidden_dim, 64)
    )
    
    forbidden_tcn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'MLP'),
        ForbiddenEqualsClause(tcn_num_filters, 64)
    )

    cs.add_forbidden_clause(forbidden_mlp)
    cs.add_forbidden_clause(forbidden_lstm)
    cs.add_forbidden_clause(forbidden_tcn)

    return cs

--------------------------------------------------------------------------------
[2025-05-20 12:05:17] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#5): Categorical() got an unexpected keyword argument 'choices'
--------------------------------------------------------------------------------
[2025-05-20 12:05:21] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import (
    ConfigurationSpace,
    Categorical,
    Float,
    Integer,
    ForbiddenAndConjunction,
    ForbiddenEqualsClause,
)


def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical("model_type", ["LSTM", "GRU", "MLP", "Linear"], default="MLP")
    learning_rate = Float("learning_rate", (1e-5, 1e-2), default=1e-3, log=True)
    batch_size = Categorical("batch_size", [16, 32, 64], default=32)
    epochs = Integer("epochs", (10, 100), default=50)

    # MLP specific hyperparameters
    num_layers = Integer("num_layers", (1, 3), default=2)
    hidden_size = Integer("hidden_size", (32, 256), default=128, log=True)
    dropout = Float("dropout", (0.0, 0.99), default=0.5)

    # RNN specific hyperparameters
    rnn_hidden_size = Integer("rnn_hidden_size", (32, 256), default=64, log=True)
    rnn_num_layers = Integer("rnn_num_layers", (1, 3), default=1)

    # Add hyperparameters to the ConfigurationSpace
    cs.add_hyperparameters([model_type, learning_rate, batch_size, epochs])
    cs.add_hyperparameters([num_layers, hidden_size, dropout])  # MLP
    cs.add_hyperparameters([rnn_hidden_size, rnn_num_layers])  # RNN

    # Add forbidden clauses to avoid invalid configurations
    # e.g., MLP specific params when using RNN
    forbidden_mlp_rnn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "LSTM"),
        ForbiddenEqualsClause(model_type, "GRU"),
        ForbiddenEqualsClause(num_layers, 2)
    )

    cs.add_forbidden_clause(forbidden_mlp_rnn)

    forbidden_rnn_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "MLP"),
        ForbiddenEqualsClause(rnn_hidden_size, 64)
    )

    cs.add_forbidden_clause(forbidden_rnn_mlp)

    forbidden_linear_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "Linear"),
        ForbiddenEqualsClause(num_layers, 2)
    )

    cs.add_forbidden_clause(forbidden_linear_mlp)
    
    forbidden_linear_rnn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "Linear"),
        ForbiddenEqualsClause(rnn_hidden_size, 64)
    )

    cs.add_forbidden_clause(forbidden_linear_rnn)

    return cs

--------------------------------------------------------------------------------
[2025-05-20 12:05:21] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#1): Given vector violates forbidden clause: (Forbidden: model_type == 'MLP' && Forbidden: rnn_hidden_size == 64)
--------------------------------------------------------------------------------
[2025-05-20 12:05:25] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import (
    ConfigurationSpace,
    Categorical,
    Float,
    Integer,
    ForbiddenAndConjunction,
    ForbiddenEqualsClause,
)


def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical("model_type", choices=["LSTM", "GRU", "MLP", "Linear"], default="MLP")
    learning_rate = Float("learning_rate", low=1e-5, high=1e-2, default=1e-3, log=True)
    batch_size = Categorical("batch_size", choices=[16, 32, 64], default=32)
    epochs = Integer("epochs", lower=10, upper=100, default=50)

    # MLP specific hyperparameters
    num_layers = Integer("num_layers", lower=1, upper=3, default=2)
    hidden_size = Integer("hidden_size", lower=32, upper=256, default=128, log=True)
    dropout = Float("dropout", low=0.0, high=0.99, default=0.5)

    # RNN specific hyperparameters
    rnn_hidden_size = Integer("rnn_hidden_size", lower=32, upper=256, default=64, log=True)
    rnn_num_layers = Integer("rnn_num_layers", lower=1, upper=3, default=1)

    # Add hyperparameters to the ConfigurationSpace
    cs.add_hyperparameters([model_type, learning_rate, batch_size, epochs])
    cs.add_hyperparameters([num_layers, hidden_size, dropout])  # MLP
    cs.add_hyperparameters([rnn_hidden_size, rnn_num_layers])  # RNN

    # Add forbidden clauses to avoid invalid configurations
    # e.g., MLP specific params when using RNN
    forbidden_mlp_rnn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "LSTM"),
        ForbiddenEqualsClause(model_type, "GRU"),
        ForbiddenEqualsClause(model_type, "Linear")
    )

    cs.add_forbidden_clause(forbidden_mlp_rnn)

    forbidden_rnn_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "MLP"),
        ForbiddenEqualsClause(rnn_hidden_size, 64)
    )

    cs.add_forbidden_clause(forbidden_rnn_mlp)
    
    return cs
```
--------------------------------------------------------------------------------
[2025-05-20 12:05:25] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import (
    ConfigurationSpace,
    Categorical,
    Float,
    Integer,
    ForbiddenAndConjunction,
    ForbiddenEqualsClause,
)


def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical("model_type", choices=["LSTM", "GRU", "MLP", "Linear"], default="MLP")
    learning_rate = Float("learning_rate", low=1e-5, high=1e-2, default=1e-3, log=True)
    batch_size = Categorical("batch_size", choices=[16, 32, 64], default=32)
    epochs = Integer("epochs", lower=10, upper=100, default=50)

    # MLP specific hyperparameters
    num_layers = Integer("num_layers", lower=1, upper=3, default=2)
    hidden_size = Integer("hidden_size", lower=32, upper=256, default=128, log=True)
    dropout = Float("dropout", low=0.0, high=0.99, default=0.5)

    # RNN specific hyperparameters
    rnn_hidden_size = Integer("rnn_hidden_size", lower=32, upper=256, default=64, log=True)
    rnn_num_layers = Integer("rnn_num_layers", lower=1, upper=3, default=1)

    # Add hyperparameters to the ConfigurationSpace
    cs.add_hyperparameters([model_type, learning_rate, batch_size, epochs])
    cs.add_hyperparameters([num_layers, hidden_size, dropout])  # MLP
    cs.add_hyperparameters([rnn_hidden_size, rnn_num_layers])  # RNN

    # Add forbidden clauses to avoid invalid configurations
    # e.g., MLP specific params when using RNN
    forbidden_mlp_rnn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "LSTM"),
        ForbiddenEqualsClause(model_type, "GRU"),
        ForbiddenEqualsClause(model_type, "Linear")
    )

    cs.add_forbidden_clause(forbidden_mlp_rnn)

    forbidden_rnn_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "MLP"),
        ForbiddenEqualsClause(rnn_hidden_size, 64)
    )

    cs.add_forbidden_clause(forbidden_rnn_mlp)
    
    return cs

--------------------------------------------------------------------------------
[2025-05-20 12:05:25] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#2): Categorical() got an unexpected keyword argument 'choices'
--------------------------------------------------------------------------------
[2025-05-20 12:05:28] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import (
    ConfigurationSpace,
    Categorical,
    Float,
    Integer,
    ForbiddenAndConjunction,
    ForbiddenEqualsClause,
)


def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical("model_type", choices=["LSTM", "GRU", "MLP", "Linear"])
    learning_rate = Float("learning_rate", lower=1e-5, upper=1e-2, log=True)
    batch_size = Categorical("batch_size", choices=[16, 32, 64])
    epochs = Integer("epochs", lower=10, upper=100)

    # MLP specific hyperparameters
    num_layers = Integer("num_layers", lower=1, upper=3)
    hidden_size = Integer("hidden_size", lower=32, upper=256, log=True)
    dropout = Float("dropout", lower=0.0, upper=0.99)

    # RNN specific hyperparameters
    rnn_hidden_size = Integer("rnn_hidden_size", lower=32, upper=256, log=True)
    rnn_num_layers = Integer("rnn_num_layers", lower=1, upper=3)

    # Add hyperparameters to the ConfigurationSpace
    cs.add_hyperparameters([model_type, learning_rate, batch_size, epochs])
    cs.add_hyperparameters([num_layers, hidden_size, dropout])  # MLP
    cs.add_hyperparameters([rnn_hidden_size, rnn_num_layers])  # RNN

    # Add forbidden clauses to avoid invalid configurations
    # e.g., MLP specific params when using RNN
    forbidden_mlp_rnn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "LSTM"),
        ForbiddenEqualsClause(model_type, "GRU"),
        ForbiddenEqualsClause(model_type, "Linear")
    )

    cs.add_forbidden_clause(forbidden_mlp_rnn)

    forbidden_rnn_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "MLP"),
        ForbiddenEqualsClause(rnn_hidden_size, 64)
    )

    cs.add_forbidden_clause(forbidden_rnn_mlp)
    
    return cs
```
--------------------------------------------------------------------------------
[2025-05-20 12:05:28] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import (
    ConfigurationSpace,
    Categorical,
    Float,
    Integer,
    ForbiddenAndConjunction,
    ForbiddenEqualsClause,
)


def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical("model_type", choices=["LSTM", "GRU", "MLP", "Linear"])
    learning_rate = Float("learning_rate", lower=1e-5, upper=1e-2, log=True)
    batch_size = Categorical("batch_size", choices=[16, 32, 64])
    epochs = Integer("epochs", lower=10, upper=100)

    # MLP specific hyperparameters
    num_layers = Integer("num_layers", lower=1, upper=3)
    hidden_size = Integer("hidden_size", lower=32, upper=256, log=True)
    dropout = Float("dropout", lower=0.0, upper=0.99)

    # RNN specific hyperparameters
    rnn_hidden_size = Integer("rnn_hidden_size", lower=32, upper=256, log=True)
    rnn_num_layers = Integer("rnn_num_layers", lower=1, upper=3)

    # Add hyperparameters to the ConfigurationSpace
    cs.add_hyperparameters([model_type, learning_rate, batch_size, epochs])
    cs.add_hyperparameters([num_layers, hidden_size, dropout])  # MLP
    cs.add_hyperparameters([rnn_hidden_size, rnn_num_layers])  # RNN

    # Add forbidden clauses to avoid invalid configurations
    # e.g., MLP specific params when using RNN
    forbidden_mlp_rnn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "LSTM"),
        ForbiddenEqualsClause(model_type, "GRU"),
        ForbiddenEqualsClause(model_type, "Linear")
    )

    cs.add_forbidden_clause(forbidden_mlp_rnn)

    forbidden_rnn_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "MLP"),
        ForbiddenEqualsClause(rnn_hidden_size, 64)
    )

    cs.add_forbidden_clause(forbidden_rnn_mlp)
    
    return cs

--------------------------------------------------------------------------------
[2025-05-20 12:05:28] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#3): Categorical() got an unexpected keyword argument 'choices'
--------------------------------------------------------------------------------
[2025-05-20 12:05:31] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import (
    ConfigurationSpace,
    Categorical,
    Float,
    Integer,
    ForbiddenAndConjunction,
    ForbiddenEqualsClause,
)


def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical("model_type", choices=["LSTM", "GRU", "MLP", "Linear"])
    learning_rate = Float("learning_rate", lower=1e-5, upper=1e-2, log=True)
    batch_size = Categorical("batch_size", choices=[16, 32, 64])
    epochs = Integer("epochs", lower=10, upper=100)

    # MLP specific hyperparameters
    num_layers = Integer("num_layers", lower=1, upper=3)
    hidden_size = Integer("hidden_size", lower=32, upper=256, log=True)
    dropout = Float("dropout", lower=0.0, upper=0.99)

    # RNN specific hyperparameters
    rnn_hidden_size = Integer("rnn_hidden_size", lower=32, upper=256, log=True)
    rnn_num_layers = Integer("rnn_num_layers", lower=1, upper=3)

    # Add hyperparameters to the ConfigurationSpace
    cs.add_hyperparameters([model_type, learning_rate, batch_size, epochs])
    cs.add_hyperparameters([num_layers, hidden_size, dropout])  # MLP
    cs.add_hyperparameters([rnn_hidden_size, rnn_num_layers])  # RNN

    # Add forbidden clauses to avoid invalid configurations
    # e.g., MLP specific params when using RNN
    forbidden_mlp_rnn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "LSTM"),
        ForbiddenEqualsClause(model_type, "GRU"),
        ForbiddenEqualsClause(model_type, "Linear"),
        
    )

    cs.add_forbidden_clause(forbidden_mlp_rnn)

    forbidden_rnn_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "MLP"),
    )

    cs.add_forbidden_clause(forbidden_rnn_mlp)
    
    return cs
```
--------------------------------------------------------------------------------
[2025-05-20 12:05:31] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import (
    ConfigurationSpace,
    Categorical,
    Float,
    Integer,
    ForbiddenAndConjunction,
    ForbiddenEqualsClause,
)


def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical("model_type", choices=["LSTM", "GRU", "MLP", "Linear"])
    learning_rate = Float("learning_rate", lower=1e-5, upper=1e-2, log=True)
    batch_size = Categorical("batch_size", choices=[16, 32, 64])
    epochs = Integer("epochs", lower=10, upper=100)

    # MLP specific hyperparameters
    num_layers = Integer("num_layers", lower=1, upper=3)
    hidden_size = Integer("hidden_size", lower=32, upper=256, log=True)
    dropout = Float("dropout", lower=0.0, upper=0.99)

    # RNN specific hyperparameters
    rnn_hidden_size = Integer("rnn_hidden_size", lower=32, upper=256, log=True)
    rnn_num_layers = Integer("rnn_num_layers", lower=1, upper=3)

    # Add hyperparameters to the ConfigurationSpace
    cs.add_hyperparameters([model_type, learning_rate, batch_size, epochs])
    cs.add_hyperparameters([num_layers, hidden_size, dropout])  # MLP
    cs.add_hyperparameters([rnn_hidden_size, rnn_num_layers])  # RNN

    # Add forbidden clauses to avoid invalid configurations
    # e.g., MLP specific params when using RNN
    forbidden_mlp_rnn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "LSTM"),
        ForbiddenEqualsClause(model_type, "GRU"),
        ForbiddenEqualsClause(model_type, "Linear"),
        
    )

    cs.add_forbidden_clause(forbidden_mlp_rnn)

    forbidden_rnn_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "MLP"),
    )

    cs.add_forbidden_clause(forbidden_rnn_mlp)
    
    return cs

--------------------------------------------------------------------------------
[2025-05-20 12:05:31] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#4): Categorical() got an unexpected keyword argument 'choices'
--------------------------------------------------------------------------------
[2025-05-20 12:05:34] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import (
    ConfigurationSpace,
    Categorical,
    Float,
    Integer,
    ForbiddenAndConjunction,
    ForbiddenEqualsClause,
)


def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical("model_type", choices=["LSTM", "GRU", "MLP", "Linear"])
    learning_rate = Float("learning_rate", lower=1e-5, upper=1e-2, log=True)
    batch_size = Categorical("batch_size", choices=[16, 32, 64])
    epochs = Integer("epochs", lower=10, upper=100)

    # MLP specific hyperparameters
    num_layers = Integer("num_layers", lower=1, upper=3)
    hidden_size = Integer("hidden_size", lower=32, upper=256, log=True)
    dropout = Float("dropout", lower=0.0, upper=0.99)

    # RNN specific hyperparameters
    rnn_hidden_size = Integer("rnn_hidden_size", lower=32, upper=256, log=True)
    rnn_num_layers = Integer("rnn_num_layers", lower=1, upper=3)

    # Add hyperparameters to the ConfigurationSpace
    cs.add_hyperparameters([model_type, learning_rate, batch_size, epochs])
    cs.add_hyperparameters([num_layers, hidden_size, dropout])  # MLP
    cs.add_hyperparameters([rnn_hidden_size, rnn_num_layers])  # RNN

    # Add forbidden clauses to avoid invalid configurations
    # e.g., MLP specific params when using RNN
    forbidden_mlp_rnn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "LSTM"),
        ForbiddenEqualsClause(num_layers, 1),
    )

    forbidden_rnn_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "MLP"),
        ForbiddenEqualsClause(rnn_hidden_size, 32),
    )
    
    cs.add_forbidden_clause(forbidden_mlp_rnn)
    cs.add_forbidden_clause(forbidden_rnn_mlp)
    
    return cs
```
--------------------------------------------------------------------------------
[2025-05-20 12:05:34] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import (
    ConfigurationSpace,
    Categorical,
    Float,
    Integer,
    ForbiddenAndConjunction,
    ForbiddenEqualsClause,
)


def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical("model_type", choices=["LSTM", "GRU", "MLP", "Linear"])
    learning_rate = Float("learning_rate", lower=1e-5, upper=1e-2, log=True)
    batch_size = Categorical("batch_size", choices=[16, 32, 64])
    epochs = Integer("epochs", lower=10, upper=100)

    # MLP specific hyperparameters
    num_layers = Integer("num_layers", lower=1, upper=3)
    hidden_size = Integer("hidden_size", lower=32, upper=256, log=True)
    dropout = Float("dropout", lower=0.0, upper=0.99)

    # RNN specific hyperparameters
    rnn_hidden_size = Integer("rnn_hidden_size", lower=32, upper=256, log=True)
    rnn_num_layers = Integer("rnn_num_layers", lower=1, upper=3)

    # Add hyperparameters to the ConfigurationSpace
    cs.add_hyperparameters([model_type, learning_rate, batch_size, epochs])
    cs.add_hyperparameters([num_layers, hidden_size, dropout])  # MLP
    cs.add_hyperparameters([rnn_hidden_size, rnn_num_layers])  # RNN

    # Add forbidden clauses to avoid invalid configurations
    # e.g., MLP specific params when using RNN
    forbidden_mlp_rnn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "LSTM"),
        ForbiddenEqualsClause(num_layers, 1),
    )

    forbidden_rnn_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "MLP"),
        ForbiddenEqualsClause(rnn_hidden_size, 32),
    )
    
    cs.add_forbidden_clause(forbidden_mlp_rnn)
    cs.add_forbidden_clause(forbidden_rnn_mlp)
    
    return cs

--------------------------------------------------------------------------------
[2025-05-20 12:05:34] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#5): Categorical() got an unexpected keyword argument 'choices'
--------------------------------------------------------------------------------
[2025-05-20 12:05:36] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause


def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical('model_type', ['LSTM', 'GRU', 'RNN', 'MLP'], default='LSTM')
    learning_rate = Float('learning_rate', (1e-5, 1e-2), default=1e-3, log=True)
    batch_size = Integer('batch_size', (8, 64), default=32, log=True)
    num_layers = Integer('num_layers', (1, 3), default=2)
    hidden_size = Integer('hidden_size', (32, 256), default=128, log=True)
    dropout = Float('dropout', (0.0, 0.5), default=0.2)
    optimizer = Categorical('optimizer', ['Adam', 'SGD'], default='Adam')

    cs.add([model_type, learning_rate, batch_size, num_layers, hidden_size, dropout, optimizer])

    # Add forbidden clauses
    # Example: if optimizer is SGD, dropout cannot be greater than 0.3
    forbidden_optimizer_dropout = ForbiddenAndConjunction(
        ForbiddenEqualsClause(optimizer, 'SGD'),
        ForbiddenEqualsClause(dropout, 0.5)
    )
    cs.add_forbidden_clause(forbidden_optimizer_dropout)

    return cs

--------------------------------------------------------------------------------
[2025-05-20 12:05:36] [Metadata: {'component': 'config', 'status': 'success'}] Configuration space generated successfully
--------------------------------------------------------------------------------
[2025-05-20 12:05:37] [Metadata: {'component': 'scenario'}] ```python
from smac import Scenario
from ConfigSpace import ConfigurationSpace

def generate_scenario(cs):
    scenario = Scenario(
        configspace=cs,
        output_directory="./automl_results",
        deterministic=False,
        min_budget=1,
        max_budget=100,
        n_workers=4
    )
    return scenario
```
--------------------------------------------------------------------------------
[2025-05-20 12:05:37] [Metadata: {'component': 'scenario', 'action': 'run'}] from smac import Scenario
from ConfigSpace import ConfigurationSpace

def generate_scenario(cs):
    scenario = Scenario(
        configspace=cs,
        output_directory="./automl_results",
        deterministic=False,
        min_budget=1,
        max_budget=100,
        n_workers=4
    )
    return scenario

--------------------------------------------------------------------------------
[2025-05-20 12:05:37] [Metadata: {'component': 'scenario', 'status': 'success'}] Scenario generated successfully
--------------------------------------------------------------------------------
[2025-05-20 12:05:45] [Metadata: {'component': 'train_function'}] ```python
from typing import Any
from ConfigSpace import Configuration
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): Configuration object specifying hyperparameters.
        dataset (Any): Dictionary containing 'X' (features) and 'y' (labels).

    Returns:
        float: Average training loss over 10 epochs.
    """

    X = dataset['X']
    y = dataset['y']

    model_type = cfg.get('model_type')
    learning_rate = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    optimizer_name = cfg.get('optimizer')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)
    y_tensor = torch.tensor(y, dtype=torch.long).to(device)

    input_size = X_tensor.shape[-1]  #infer input size
    num_classes = len(torch.unique(y_tensor))
    if len(X_tensor.shape) == 3:
        sequence_length = X_tensor.shape[1]
        input_size = X_tensor.shape[2]

    if model_type in ['LSTM', 'GRU', 'RNN'] and X_tensor.ndim != 3:
        raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")

    # Define the model
    if model_type == 'MLP':
        model = nn.Sequential(
            nn.Linear(input_size, cfg.get('hidden_size')),
            nn.ReLU(),
            nn.Dropout(cfg.get('dropout')),
            nn.Linear(cfg.get('hidden_size'), num_classes)
        ).to(device)
    elif model_type == 'LSTM':
        model = nn.LSTM(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                          num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                          batch_first=True).to(device)
        # Modify output layer to handle LSTM output
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    elif model_type == 'GRU':
        model = nn.GRU(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                         num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                         batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    elif model_type == 'RNN':
        model = nn.RNN(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                         num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                         batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Define the optimizer
    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Unsupported optimizer: {optimizer_name}")

    # Define the loss function
    criterion = nn.CrossEntropyLoss()

    # Create DataLoader
    dataset = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Training loop
    num_epochs = 10
    total_loss = 0.0
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for inputs, labels in dataloader:

            if model_type in ['LSTM', 'GRU', 'RNN']:
                # Handle RNN inputs
                outputs = model(inputs)[0] if isinstance(model[0], (nn.LSTM, nn.GRU, nn.RNN)) else model(inputs)
            else:
                outputs = model(inputs)

            loss = criterion(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_epoch_loss = epoch_loss / len(dataloader)
        total_loss += avg_epoch_loss

    avg_loss = total_loss / num_epochs

    return float(avg_loss)
```
--------------------------------------------------------------------------------
[2025-05-20 12:05:45] [Metadata: {'component': 'train_function', 'action': 'run'}] from typing import Any
from ConfigSpace import Configuration
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): Configuration object specifying hyperparameters.
        dataset (Any): Dictionary containing 'X' (features) and 'y' (labels).

    Returns:
        float: Average training loss over 10 epochs.
    """

    X = dataset['X']
    y = dataset['y']

    model_type = cfg.get('model_type')
    learning_rate = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    optimizer_name = cfg.get('optimizer')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)
    y_tensor = torch.tensor(y, dtype=torch.long).to(device)

    input_size = X_tensor.shape[-1]  #infer input size
    num_classes = len(torch.unique(y_tensor))
    if len(X_tensor.shape) == 3:
        sequence_length = X_tensor.shape[1]
        input_size = X_tensor.shape[2]

    if model_type in ['LSTM', 'GRU', 'RNN'] and X_tensor.ndim != 3:
        raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")

    # Define the model
    if model_type == 'MLP':
        model = nn.Sequential(
            nn.Linear(input_size, cfg.get('hidden_size')),
            nn.ReLU(),
            nn.Dropout(cfg.get('dropout')),
            nn.Linear(cfg.get('hidden_size'), num_classes)
        ).to(device)
    elif model_type == 'LSTM':
        model = nn.LSTM(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                          num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                          batch_first=True).to(device)
        # Modify output layer to handle LSTM output
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    elif model_type == 'GRU':
        model = nn.GRU(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                         num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                         batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    elif model_type == 'RNN':
        model = nn.RNN(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                         num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                         batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Define the optimizer
    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Unsupported optimizer: {optimizer_name}")

    # Define the loss function
    criterion = nn.CrossEntropyLoss()

    # Create DataLoader
    dataset = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Training loop
    num_epochs = 10
    total_loss = 0.0
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for inputs, labels in dataloader:

            if model_type in ['LSTM', 'GRU', 'RNN']:
                # Handle RNN inputs
                outputs = model(inputs)[0] if isinstance(model[0], (nn.LSTM, nn.GRU, nn.RNN)) else model(inputs)
            else:
                outputs = model(inputs)

            loss = criterion(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_epoch_loss = epoch_loss / len(dataloader)
        total_loss += avg_epoch_loss

    avg_loss = total_loss / num_epochs

    return float(avg_loss)

--------------------------------------------------------------------------------
[2025-05-20 12:05:46] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#1): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-05-20 12:05:52] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
from typing import Any
from ConfigSpace import Configuration
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): Configuration object specifying hyperparameters.
        dataset (Any): Dictionary containing 'X' (features) and 'y' (labels).

    Returns:
        float: Average training loss over 10 epochs.
    """

    X = dataset['X']
    y = dataset['y']

    model_type = cfg.get('model_type')
    learning_rate = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    optimizer_name = cfg.get('optimizer')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)
    y_tensor = torch.tensor(y, dtype=torch.long).to(device)

    input_size = X_tensor.shape[-1]  #infer input size
    num_classes = len(torch.unique(y_tensor))
    if len(X_tensor.shape) == 3:
        sequence_length = X_tensor.shape[1]
        input_size = X_tensor.shape[2]

    if model_type in ['LSTM', 'GRU', 'RNN'] and X_tensor.ndim != 3:
        raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")

    # Define the model
    if model_type == 'MLP':
        model = nn.Sequential(
            nn.Linear(input_size, cfg.get('hidden_size')),
            nn.ReLU(),
            nn.Dropout(cfg.get('dropout')),
            nn.Linear(cfg.get('hidden_size'), num_classes)
        ).to(device)
    elif model_type == 'LSTM':
        model = nn.LSTM(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                          num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                          batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    elif model_type == 'GRU':
        model = nn.GRU(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                         num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                         batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    elif model_type == 'RNN':
        model = nn.RNN(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                         num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                         batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Define the optimizer
    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Unsupported optimizer: {optimizer_name}")

    # Define the loss function
    criterion = nn.CrossEntropyLoss()

    # Create DataLoader
    dataset = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Training loop
    num_epochs = 10
    total_loss = 0.0
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for inputs, labels in dataloader:

            if model_type in ['LSTM', 'GRU', 'RNN']:
                outputs = model(inputs)[0] if isinstance(model[0], (nn.LSTM, nn.GRU, nn.RNN)) else model(inputs)
                if isinstance(model, nn.Sequential):
                    outputs = model[1](outputs[:, -1, :])
            else:
                outputs = model(inputs)

            loss = criterion(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_epoch_loss = epoch_loss / len(dataloader)
        total_loss += avg_epoch_loss

    avg_loss = total_loss / num_epochs

    return float(avg_loss)
```
--------------------------------------------------------------------------------
[2025-05-20 12:05:52] [Metadata: {'component': 'train_function', 'action': 'run'}] from typing import Any
from ConfigSpace import Configuration
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): Configuration object specifying hyperparameters.
        dataset (Any): Dictionary containing 'X' (features) and 'y' (labels).

    Returns:
        float: Average training loss over 10 epochs.
    """

    X = dataset['X']
    y = dataset['y']

    model_type = cfg.get('model_type')
    learning_rate = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    optimizer_name = cfg.get('optimizer')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)
    y_tensor = torch.tensor(y, dtype=torch.long).to(device)

    input_size = X_tensor.shape[-1]  #infer input size
    num_classes = len(torch.unique(y_tensor))
    if len(X_tensor.shape) == 3:
        sequence_length = X_tensor.shape[1]
        input_size = X_tensor.shape[2]

    if model_type in ['LSTM', 'GRU', 'RNN'] and X_tensor.ndim != 3:
        raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")

    # Define the model
    if model_type == 'MLP':
        model = nn.Sequential(
            nn.Linear(input_size, cfg.get('hidden_size')),
            nn.ReLU(),
            nn.Dropout(cfg.get('dropout')),
            nn.Linear(cfg.get('hidden_size'), num_classes)
        ).to(device)
    elif model_type == 'LSTM':
        model = nn.LSTM(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                          num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                          batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    elif model_type == 'GRU':
        model = nn.GRU(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                         num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                         batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    elif model_type == 'RNN':
        model = nn.RNN(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                         num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                         batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Define the optimizer
    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Unsupported optimizer: {optimizer_name}")

    # Define the loss function
    criterion = nn.CrossEntropyLoss()

    # Create DataLoader
    dataset = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Training loop
    num_epochs = 10
    total_loss = 0.0
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for inputs, labels in dataloader:

            if model_type in ['LSTM', 'GRU', 'RNN']:
                outputs = model(inputs)[0] if isinstance(model[0], (nn.LSTM, nn.GRU, nn.RNN)) else model(inputs)
                if isinstance(model, nn.Sequential):
                    outputs = model[1](outputs[:, -1, :])
            else:
                outputs = model(inputs)

            loss = criterion(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_epoch_loss = epoch_loss / len(dataloader)
        total_loss += avg_epoch_loss

    avg_loss = total_loss / num_epochs

    return float(avg_loss)

--------------------------------------------------------------------------------
[2025-05-20 12:05:52] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#2): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-05-20 12:05:59] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
from typing import Any
from ConfigSpace import Configuration
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): Configuration object specifying hyperparameters.
        dataset (Any): Dictionary containing 'X' (features) and 'y' (labels).

    Returns:
        float: Average training loss over 10 epochs.
    """

    X = dataset['X']
    y = dataset['y']

    model_type = cfg.get('model_type')
    learning_rate = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    optimizer_name = cfg.get('optimizer')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)
    y_tensor = torch.tensor(y, dtype=torch.long).to(device)

    input_size = X_tensor.shape[-1]  #infer input size
    num_classes = len(torch.unique(y_tensor))
    if len(X_tensor.shape) == 3:
        sequence_length = X_tensor.shape[1]
        input_size = X_tensor.shape[2]

    if model_type in ['LSTM', 'GRU', 'RNN'] and X_tensor.ndim != 3:
        raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")

    # Define the model
    if model_type == 'MLP':
        model = nn.Sequential(
            nn.Linear(input_size, cfg.get('hidden_size')),
            nn.ReLU(),
            nn.Dropout(cfg.get('dropout')),
            nn.Linear(cfg.get('hidden_size'), num_classes)
        ).to(device)
    elif model_type == 'LSTM':
        model = nn.LSTM(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                          num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                          batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    elif model_type == 'GRU':
        model = nn.GRU(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                         num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                         batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    elif model_type == 'RNN':
        model = nn.RNN(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                         num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                         batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Define the optimizer
    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Unsupported optimizer: {optimizer_name}")

    # Define the loss function
    criterion = nn.CrossEntropyLoss()

    # Create DataLoader
    dataset = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Training loop
    num_epochs = 10
    total_loss = 0.0
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for inputs, labels in dataloader:
            if model_type in ['LSTM', 'GRU', 'RNN']:
                outputs, _ = model[0](inputs) if isinstance(model[0], (nn.LSTM, nn.GRU, nn.RNN)) else (model[0](inputs), None)
                outputs = model[1](outputs[:, -1, :])
            else:
                outputs = model(inputs)
            loss = criterion(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_epoch_loss = epoch_loss / len(dataloader)
        total_loss += avg_epoch_loss

    avg_loss = total_loss / num_epochs

    return float(avg_loss)
```
--------------------------------------------------------------------------------
[2025-05-20 12:05:59] [Metadata: {'component': 'train_function', 'action': 'run'}] from typing import Any
from ConfigSpace import Configuration
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): Configuration object specifying hyperparameters.
        dataset (Any): Dictionary containing 'X' (features) and 'y' (labels).

    Returns:
        float: Average training loss over 10 epochs.
    """

    X = dataset['X']
    y = dataset['y']

    model_type = cfg.get('model_type')
    learning_rate = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    optimizer_name = cfg.get('optimizer')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)
    y_tensor = torch.tensor(y, dtype=torch.long).to(device)

    input_size = X_tensor.shape[-1]  #infer input size
    num_classes = len(torch.unique(y_tensor))
    if len(X_tensor.shape) == 3:
        sequence_length = X_tensor.shape[1]
        input_size = X_tensor.shape[2]

    if model_type in ['LSTM', 'GRU', 'RNN'] and X_tensor.ndim != 3:
        raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")

    # Define the model
    if model_type == 'MLP':
        model = nn.Sequential(
            nn.Linear(input_size, cfg.get('hidden_size')),
            nn.ReLU(),
            nn.Dropout(cfg.get('dropout')),
            nn.Linear(cfg.get('hidden_size'), num_classes)
        ).to(device)
    elif model_type == 'LSTM':
        model = nn.LSTM(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                          num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                          batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    elif model_type == 'GRU':
        model = nn.GRU(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                         num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                         batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    elif model_type == 'RNN':
        model = nn.RNN(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                         num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                         batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Define the optimizer
    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Unsupported optimizer: {optimizer_name}")

    # Define the loss function
    criterion = nn.CrossEntropyLoss()

    # Create DataLoader
    dataset = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Training loop
    num_epochs = 10
    total_loss = 0.0
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for inputs, labels in dataloader:
            if model_type in ['LSTM', 'GRU', 'RNN']:
                outputs, _ = model[0](inputs) if isinstance(model[0], (nn.LSTM, nn.GRU, nn.RNN)) else (model[0](inputs), None)
                outputs = model[1](outputs[:, -1, :])
            else:
                outputs = model(inputs)
            loss = criterion(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_epoch_loss = epoch_loss / len(dataloader)
        total_loss += avg_epoch_loss

    avg_loss = total_loss / num_epochs

    return float(avg_loss)

--------------------------------------------------------------------------------
[2025-05-20 12:05:59] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#3): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-05-20 12:06:05] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
from typing import Any
from ConfigSpace import Configuration
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): Configuration object specifying hyperparameters.
        dataset (Any): Dictionary containing 'X' (features) and 'y' (labels).

    Returns:
        float: Average training loss over 10 epochs.
    """

    X = dataset['X']
    y = dataset['y']

    model_type = cfg.get('model_type')
    learning_rate = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    optimizer_name = cfg.get('optimizer')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)
    y_tensor = torch.tensor(y, dtype=torch.long).to(device)

    input_size = X_tensor.shape[-1]  #infer input size
    num_classes = len(torch.unique(y_tensor))
    if len(X_tensor.shape) == 3:
        sequence_length = X_tensor.shape[1]
        input_size = X_tensor.shape[2]

    if model_type in ['LSTM', 'GRU', 'RNN'] and X_tensor.ndim != 3:
        raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")

    # Define the model
    if model_type == 'MLP':
        model = nn.Sequential(
            nn.Linear(input_size, cfg.get('hidden_size')),
            nn.ReLU(),
            nn.Dropout(cfg.get('dropout')),
            nn.Linear(cfg.get('hidden_size'), num_classes)
        ).to(device)
    elif model_type == 'LSTM':
        model = nn.LSTM(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                          num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                          batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    elif model_type == 'GRU':
        model = nn.GRU(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                         num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                         batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    elif model_type == 'RNN':
        model = nn.RNN(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                         num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                         batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Define the optimizer
    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Unsupported optimizer: {optimizer_name}")

    # Define the loss function
    criterion = nn.CrossEntropyLoss()

    # Create DataLoader
    dataset = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Training loop
    num_epochs = 10
    total_loss = 0.0
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for inputs, labels in dataloader:
            if model_type in ['LSTM', 'GRU', 'RNN']:
                outputs = model[0](inputs)[0] if isinstance(model[0], (nn.LSTM, nn.GRU, nn.RNN)) else model[0](inputs)
                outputs = model[1](outputs[:, -1, :])
            else:
                outputs = model(inputs)
            loss = criterion(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_epoch_loss = epoch_loss / len(dataloader)
        total_loss += avg_epoch_loss

    avg_loss = total_loss / num_epochs

    return float(avg_loss)
```
--------------------------------------------------------------------------------
[2025-05-20 12:06:05] [Metadata: {'component': 'train_function', 'action': 'run'}] from typing import Any
from ConfigSpace import Configuration
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): Configuration object specifying hyperparameters.
        dataset (Any): Dictionary containing 'X' (features) and 'y' (labels).

    Returns:
        float: Average training loss over 10 epochs.
    """

    X = dataset['X']
    y = dataset['y']

    model_type = cfg.get('model_type')
    learning_rate = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    optimizer_name = cfg.get('optimizer')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)
    y_tensor = torch.tensor(y, dtype=torch.long).to(device)

    input_size = X_tensor.shape[-1]  #infer input size
    num_classes = len(torch.unique(y_tensor))
    if len(X_tensor.shape) == 3:
        sequence_length = X_tensor.shape[1]
        input_size = X_tensor.shape[2]

    if model_type in ['LSTM', 'GRU', 'RNN'] and X_tensor.ndim != 3:
        raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")

    # Define the model
    if model_type == 'MLP':
        model = nn.Sequential(
            nn.Linear(input_size, cfg.get('hidden_size')),
            nn.ReLU(),
            nn.Dropout(cfg.get('dropout')),
            nn.Linear(cfg.get('hidden_size'), num_classes)
        ).to(device)
    elif model_type == 'LSTM':
        model = nn.LSTM(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                          num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                          batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    elif model_type == 'GRU':
        model = nn.GRU(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                         num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                         batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    elif model_type == 'RNN':
        model = nn.RNN(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                         num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                         batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Define the optimizer
    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Unsupported optimizer: {optimizer_name}")

    # Define the loss function
    criterion = nn.CrossEntropyLoss()

    # Create DataLoader
    dataset = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Training loop
    num_epochs = 10
    total_loss = 0.0
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for inputs, labels in dataloader:
            if model_type in ['LSTM', 'GRU', 'RNN']:
                outputs = model[0](inputs)[0] if isinstance(model[0], (nn.LSTM, nn.GRU, nn.RNN)) else model[0](inputs)
                outputs = model[1](outputs[:, -1, :])
            else:
                outputs = model(inputs)
            loss = criterion(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_epoch_loss = epoch_loss / len(dataloader)
        total_loss += avg_epoch_loss

    avg_loss = total_loss / num_epochs

    return float(avg_loss)

--------------------------------------------------------------------------------
[2025-05-20 12:06:05] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#4): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-05-20 12:06:12] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
from typing import Any
from ConfigSpace import Configuration
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): Configuration object specifying hyperparameters.
        dataset (Any): Dictionary containing 'X' (features) and 'y' (labels).

    Returns:
        float: Average training loss over 10 epochs.
    """

    X = dataset['X']
    y = dataset['y']

    model_type = cfg.get('model_type')
    learning_rate = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    optimizer_name = cfg.get('optimizer')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)
    y_tensor = torch.tensor(y, dtype=torch.long).to(device)

    input_size = X_tensor.shape[-1]  #infer input size
    num_classes = len(torch.unique(y_tensor))
    if len(X_tensor.shape) == 3:
        sequence_length = X_tensor.shape[1]
        input_size = X_tensor.shape[2]

    if model_type in ['LSTM', 'GRU', 'RNN'] and X_tensor.ndim != 3:
        raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")

    # Define the model
    if model_type == 'MLP':
        model = nn.Sequential(
            nn.Linear(input_size, cfg.get('hidden_size')),
            nn.ReLU(),
            nn.Dropout(cfg.get('dropout')),
            nn.Linear(cfg.get('hidden_size'), num_classes)
        ).to(device)
    elif model_type == 'LSTM':
        model = nn.LSTM(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                          num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                          batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    elif model_type == 'GRU':
        model = nn.GRU(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                         num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                         batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    elif model_type == 'RNN':
        model = nn.RNN(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                         num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                         batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Define the optimizer
    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Unsupported optimizer: {optimizer_name}")

    # Define the loss function
    criterion = nn.CrossEntropyLoss()

    # Create DataLoader
    dataset = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Training loop
    num_epochs = 10
    total_loss = 0.0
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for inputs, labels in dataloader:
            if model_type in ['LSTM', 'GRU', 'RNN']:
                outputs, _ = model[0](inputs) if isinstance(model[0], (nn.LSTM, nn.GRU, nn.RNN)) else (model[0](inputs), None)
                outputs = model[1](outputs[:, -1, :])
            else:
                outputs = model(inputs)
            loss = criterion(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_epoch_loss = epoch_loss / len(dataloader)
        total_loss += avg_epoch_loss

    avg_loss = total_loss / num_epochs

    return float(avg_loss)
```
--------------------------------------------------------------------------------
[2025-05-20 12:06:12] [Metadata: {'component': 'train_function', 'action': 'run'}] from typing import Any
from ConfigSpace import Configuration
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): Configuration object specifying hyperparameters.
        dataset (Any): Dictionary containing 'X' (features) and 'y' (labels).

    Returns:
        float: Average training loss over 10 epochs.
    """

    X = dataset['X']
    y = dataset['y']

    model_type = cfg.get('model_type')
    learning_rate = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    optimizer_name = cfg.get('optimizer')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)
    y_tensor = torch.tensor(y, dtype=torch.long).to(device)

    input_size = X_tensor.shape[-1]  #infer input size
    num_classes = len(torch.unique(y_tensor))
    if len(X_tensor.shape) == 3:
        sequence_length = X_tensor.shape[1]
        input_size = X_tensor.shape[2]

    if model_type in ['LSTM', 'GRU', 'RNN'] and X_tensor.ndim != 3:
        raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")

    # Define the model
    if model_type == 'MLP':
        model = nn.Sequential(
            nn.Linear(input_size, cfg.get('hidden_size')),
            nn.ReLU(),
            nn.Dropout(cfg.get('dropout')),
            nn.Linear(cfg.get('hidden_size'), num_classes)
        ).to(device)
    elif model_type == 'LSTM':
        model = nn.LSTM(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                          num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                          batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    elif model_type == 'GRU':
        model = nn.GRU(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                         num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                         batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    elif model_type == 'RNN':
        model = nn.RNN(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                         num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                         batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Define the optimizer
    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Unsupported optimizer: {optimizer_name}")

    # Define the loss function
    criterion = nn.CrossEntropyLoss()

    # Create DataLoader
    dataset = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Training loop
    num_epochs = 10
    total_loss = 0.0
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for inputs, labels in dataloader:
            if model_type in ['LSTM', 'GRU', 'RNN']:
                outputs, _ = model[0](inputs) if isinstance(model[0], (nn.LSTM, nn.GRU, nn.RNN)) else (model[0](inputs), None)
                outputs = model[1](outputs[:, -1, :])
            else:
                outputs = model(inputs)
            loss = criterion(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_epoch_loss = epoch_loss / len(dataloader)
        total_loss += avg_epoch_loss

    avg_loss = total_loss / num_epochs

    return float(avg_loss)

--------------------------------------------------------------------------------
[2025-05-20 12:06:12] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#5): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-05-20 12:06:21] [Metadata: {'component': 'train_function', 'action': 'run'}] from typing import Any

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from ConfigSpace import Configuration


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing the training data, with keys 'X' and 'y'.

    Returns:
        float: The average training loss over 10 epochs.
    """
    X = dataset['X']
    y = dataset['y']

    # Convert data to PyTorch tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.long)

    model_type = cfg.get('model_type')
    learning_rate = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    num_layers = cfg.get('num_layers')
    hidden_size = cfg.get('hidden_size')
    dropout = cfg.get('dropout')
    optimizer_name = cfg.get('optimizer')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        num_samples, seq_len, num_features = X_tensor.shape
    elif X_tensor.ndim == 2:
        num_samples, num_features = X_tensor.shape
    else:
        raise ValueError(f"Unexpected input dimension: {X_tensor.ndim}")
    num_classes = len(np.unique(y))

    # Define the model
    class LSTMModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(LSTMModel, self).__init__()
            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.lstm(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class GRUModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(GRUModel, self).__init__()
            self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.gru(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class RNNModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(RNNModel, self).__init__()
            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.rnn(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class MLPModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(MLPModel, self).__init__()
            self.layers = nn.ModuleList()
            self.layers.append(nn.Linear(input_size, hidden_size))
            self.layers.append(nn.ReLU())
            self.layers.append(nn.Dropout(dropout))
            for _ in range(num_layers - 1):
                self.layers.append(nn.Linear(hidden_size, hidden_size))
                self.layers.append(nn.ReLU())
                self.layers.append(nn.Dropout(dropout))
            self.layers.append(nn.Linear(hidden_size, num_classes))

        def forward(self, x):
            for layer in self.layers:
                x = layer(x)
            return x

    if model_type in ['LSTM', 'GRU', 'RNN']:
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")
        input_size = X_tensor.shape[2]
        if model_type == 'LSTM':
            model = LSTMModel(input_size, hidden_size, num_layers, num_classes, dropout)
        elif model_type == 'GRU':
            model = GRUModel(input_size, hidden_size, num_layers, num_classes, dropout)
        else:  # model_type == 'RNN'
            model = RNNModel(input_size, hidden_size, num_layers, num_classes, dropout)

    elif model_type == 'MLP':
        if X_tensor.ndim != 2:
            raise ValueError(f"Expected 2D input (batch, features) for MLP, got {X_tensor.shape}")
        input_size = X_tensor.shape[1]
        model = MLPModel(input_size, hidden_size, num_layers, num_classes, dropout)

    else:
        raise ValueError(f"Invalid model type: {model_type}")

    # Define the loss function and optimizer
    criterion = nn.CrossEntropyLoss()

    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Invalid optimizer: {optimizer_name}")

    # Training loop
    num_epochs = 10
    total_loss = 0.0

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        
        # Mini-batch training
        for i in range(0, len(X_tensor), batch_size):
            inputs = X_tensor[i:i+batch_size]
            labels = y_tensor[i:i+batch_size]

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        epoch_loss = running_loss / (len(X_tensor) // batch_size)
        total_loss += epoch_loss
    
    avg_loss = total_loss / num_epochs
    return float(avg_loss)

--------------------------------------------------------------------------------
[2025-05-20 12:06:21] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#1): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-05-20 12:06:28] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
from typing import Any

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from ConfigSpace import Configuration


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing the training data, with keys 'X' and 'y'.

    Returns:
        float: The average training loss over 10 epochs.
    """
    X = dataset['X']
    y = dataset['y']

    # Convert data to PyTorch tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.long)

    model_type = cfg.get('model_type')
    learning_rate = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    num_layers = cfg.get('num_layers')
    hidden_size = cfg.get('hidden_size')
    dropout = cfg.get('dropout')
    optimizer_name = cfg.get('optimizer')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        num_samples, seq_len, num_features = X_tensor.shape
    elif X_tensor.ndim == 2:
        num_samples, num_features = X_tensor.shape
    else:
        raise ValueError(f"Unexpected input dimension: {X_tensor.ndim}")
    num_classes = len(np.unique(y))

    # Define the model
    class LSTMModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(LSTMModel, self).__init__()
            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.lstm(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class GRUModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(GRUModel, self).__init__()
            self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.gru(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class RNNModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(RNNModel, self).__init__()
            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.rnn(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class MLPModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(MLPModel, self).__init__()
            self.layers = nn.ModuleList()
            self.layers.append(nn.Linear(input_size, hidden_size))
            self.layers.append(nn.ReLU())
            self.layers.append(nn.Dropout(dropout))
            for _ in range(num_layers - 1):
                self.layers.append(nn.Linear(hidden_size, hidden_size))
                self.layers.append(nn.ReLU())
                self.layers.append(nn.Dropout(dropout))
            self.layers.append(nn.Linear(hidden_size, num_classes))

        def forward(self, x):
            for layer in self.layers:
                x = layer(x)
            return x

    if model_type in ['LSTM', 'GRU', 'RNN']:
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")
        input_size = X_tensor.shape[2]
        if model_type == 'LSTM':
            model = LSTMModel(input_size, hidden_size, num_layers, num_classes, dropout)
        elif model_type == 'GRU':
            model = GRUModel(input_size, hidden_size, num_layers, num_classes, dropout)
        else:  # model_type == 'RNN'
            model = RNNModel(input_size, hidden_size, num_layers, num_classes, dropout)

    elif model_type == 'MLP':
        if X_tensor.ndim != 2:
            raise ValueError(f"Expected 2D input (batch, features) for MLP, got {X_tensor.shape}")
        input_size = X_tensor.shape[1]
        model = MLPModel(input_size, hidden_size, num_layers, num_classes, dropout)

    else:
        raise ValueError(f"Invalid model type: {model_type}")

    # Define the loss function and optimizer
    criterion = nn.CrossEntropyLoss()

    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Invalid optimizer: {optimizer_name}")

    # Training loop
    num_epochs = 10
    total_loss = 0.0

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        # Mini-batch training
        for i in range(0, len(X_tensor), batch_size):
            inputs = X_tensor[i:i+batch_size]
            labels = y_tensor[i:i+batch_size]

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        epoch_loss = running_loss / (len(X_tensor) / batch_size)
        total_loss += epoch_loss

    avg_loss = total_loss / num_epochs
    return float(avg_loss)
```
--------------------------------------------------------------------------------
[2025-05-20 12:06:28] [Metadata: {'component': 'train_function', 'action': 'run'}] from typing import Any

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from ConfigSpace import Configuration


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing the training data, with keys 'X' and 'y'.

    Returns:
        float: The average training loss over 10 epochs.
    """
    X = dataset['X']
    y = dataset['y']

    # Convert data to PyTorch tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.long)

    model_type = cfg.get('model_type')
    learning_rate = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    num_layers = cfg.get('num_layers')
    hidden_size = cfg.get('hidden_size')
    dropout = cfg.get('dropout')
    optimizer_name = cfg.get('optimizer')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        num_samples, seq_len, num_features = X_tensor.shape
    elif X_tensor.ndim == 2:
        num_samples, num_features = X_tensor.shape
    else:
        raise ValueError(f"Unexpected input dimension: {X_tensor.ndim}")
    num_classes = len(np.unique(y))

    # Define the model
    class LSTMModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(LSTMModel, self).__init__()
            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.lstm(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class GRUModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(GRUModel, self).__init__()
            self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.gru(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class RNNModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(RNNModel, self).__init__()
            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.rnn(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class MLPModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(MLPModel, self).__init__()
            self.layers = nn.ModuleList()
            self.layers.append(nn.Linear(input_size, hidden_size))
            self.layers.append(nn.ReLU())
            self.layers.append(nn.Dropout(dropout))
            for _ in range(num_layers - 1):
                self.layers.append(nn.Linear(hidden_size, hidden_size))
                self.layers.append(nn.ReLU())
                self.layers.append(nn.Dropout(dropout))
            self.layers.append(nn.Linear(hidden_size, num_classes))

        def forward(self, x):
            for layer in self.layers:
                x = layer(x)
            return x

    if model_type in ['LSTM', 'GRU', 'RNN']:
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")
        input_size = X_tensor.shape[2]
        if model_type == 'LSTM':
            model = LSTMModel(input_size, hidden_size, num_layers, num_classes, dropout)
        elif model_type == 'GRU':
            model = GRUModel(input_size, hidden_size, num_layers, num_classes, dropout)
        else:  # model_type == 'RNN'
            model = RNNModel(input_size, hidden_size, num_layers, num_classes, dropout)

    elif model_type == 'MLP':
        if X_tensor.ndim != 2:
            raise ValueError(f"Expected 2D input (batch, features) for MLP, got {X_tensor.shape}")
        input_size = X_tensor.shape[1]
        model = MLPModel(input_size, hidden_size, num_layers, num_classes, dropout)

    else:
        raise ValueError(f"Invalid model type: {model_type}")

    # Define the loss function and optimizer
    criterion = nn.CrossEntropyLoss()

    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Invalid optimizer: {optimizer_name}")

    # Training loop
    num_epochs = 10
    total_loss = 0.0

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        # Mini-batch training
        for i in range(0, len(X_tensor), batch_size):
            inputs = X_tensor[i:i+batch_size]
            labels = y_tensor[i:i+batch_size]

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        epoch_loss = running_loss / (len(X_tensor) / batch_size)
        total_loss += epoch_loss

    avg_loss = total_loss / num_epochs
    return float(avg_loss)

--------------------------------------------------------------------------------
[2025-05-20 12:06:28] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#2): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-05-20 12:06:36] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
from typing import Any

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from ConfigSpace import Configuration


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing the training data, with keys 'X' and 'y'.

    Returns:
        float: The average training loss over 10 epochs.
    """
    X = dataset['X']
    y = dataset['y']

    # Convert data to PyTorch tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.long)

    model_type = cfg.get('model_type')
    learning_rate = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    num_layers = cfg.get('num_layers')
    hidden_size = cfg.get('hidden_size')
    dropout = cfg.get('dropout')
    optimizer_name = cfg.get('optimizer')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        num_samples, seq_len, num_features = X_tensor.shape
    elif X_tensor.ndim == 2:
        num_samples, num_features = X_tensor.shape
    else:
        raise ValueError(f"Unexpected input dimension: {X_tensor.ndim}")
    num_classes = len(np.unique(y))

    # Define the model
    class LSTMModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(LSTMModel, self).__init__()
            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.lstm(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class GRUModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(GRUModel, self).__init__()
            self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.gru(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class RNNModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(RNNModel, self).__init__()
            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.rnn(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class MLPModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(MLPModel, self).__init__()
            self.layers = nn.ModuleList()
            self.layers.append(nn.Linear(input_size, hidden_size))
            self.layers.append(nn.ReLU())
            self.layers.append(nn.Dropout(dropout))
            for _ in range(num_layers - 1):
                self.layers.append(nn.Linear(hidden_size, hidden_size))
                self.layers.append(nn.ReLU())
                self.layers.append(nn.Dropout(dropout))
            self.layers.append(nn.Linear(hidden_size, num_classes))

        def forward(self, x):
            for layer in self.layers:
                x = layer(x)
            return x

    if model_type in ['LSTM', 'GRU', 'RNN']:
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")
        input_size = X_tensor.shape[2]
        if model_type == 'LSTM':
            model = LSTMModel(input_size, hidden_size, num_layers, num_classes, dropout)
        elif model_type == 'GRU':
            model = GRUModel(input_size, hidden_size, num_layers, num_classes, dropout)
        else:  # model_type == 'RNN'
            model = RNNModel(input_size, hidden_size, num_layers, num_classes, dropout)

    elif model_type == 'MLP':
        if X_tensor.ndim != 2:
            raise ValueError(f"Expected 2D input (batch, features) for MLP, got {X_tensor.shape}")
        input_size = X_tensor.shape[1]
        model = MLPModel(input_size, hidden_size, num_layers, num_classes, dropout)

    else:
        raise ValueError(f"Invalid model type: {model_type}")

    # Define the loss function and optimizer
    criterion = nn.CrossEntropyLoss()

    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Invalid optimizer: {optimizer_name}")

    # Training loop
    num_epochs = 10
    total_loss = 0.0

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        # Mini-batch training
        for i in range(0, len(X_tensor), batch_size):
            inputs = X_tensor[i:i+batch_size]
            labels = y_tensor[i:i+batch_size]

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        epoch_loss = running_loss / (len(X_tensor) / batch_size)
        total_loss += epoch_loss

    avg_loss = total_loss / num_epochs
    return float(avg_loss)
```
--------------------------------------------------------------------------------
[2025-05-20 12:06:36] [Metadata: {'component': 'train_function', 'action': 'run'}] from typing import Any

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from ConfigSpace import Configuration


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing the training data, with keys 'X' and 'y'.

    Returns:
        float: The average training loss over 10 epochs.
    """
    X = dataset['X']
    y = dataset['y']

    # Convert data to PyTorch tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.long)

    model_type = cfg.get('model_type')
    learning_rate = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    num_layers = cfg.get('num_layers')
    hidden_size = cfg.get('hidden_size')
    dropout = cfg.get('dropout')
    optimizer_name = cfg.get('optimizer')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        num_samples, seq_len, num_features = X_tensor.shape
    elif X_tensor.ndim == 2:
        num_samples, num_features = X_tensor.shape
    else:
        raise ValueError(f"Unexpected input dimension: {X_tensor.ndim}")
    num_classes = len(np.unique(y))

    # Define the model
    class LSTMModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(LSTMModel, self).__init__()
            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.lstm(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class GRUModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(GRUModel, self).__init__()
            self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.gru(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class RNNModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(RNNModel, self).__init__()
            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.rnn(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class MLPModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(MLPModel, self).__init__()
            self.layers = nn.ModuleList()
            self.layers.append(nn.Linear(input_size, hidden_size))
            self.layers.append(nn.ReLU())
            self.layers.append(nn.Dropout(dropout))
            for _ in range(num_layers - 1):
                self.layers.append(nn.Linear(hidden_size, hidden_size))
                self.layers.append(nn.ReLU())
                self.layers.append(nn.Dropout(dropout))
            self.layers.append(nn.Linear(hidden_size, num_classes))

        def forward(self, x):
            for layer in self.layers:
                x = layer(x)
            return x

    if model_type in ['LSTM', 'GRU', 'RNN']:
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")
        input_size = X_tensor.shape[2]
        if model_type == 'LSTM':
            model = LSTMModel(input_size, hidden_size, num_layers, num_classes, dropout)
        elif model_type == 'GRU':
            model = GRUModel(input_size, hidden_size, num_layers, num_classes, dropout)
        else:  # model_type == 'RNN'
            model = RNNModel(input_size, hidden_size, num_layers, num_classes, dropout)

    elif model_type == 'MLP':
        if X_tensor.ndim != 2:
            raise ValueError(f"Expected 2D input (batch, features) for MLP, got {X_tensor.shape}")
        input_size = X_tensor.shape[1]
        model = MLPModel(input_size, hidden_size, num_layers, num_classes, dropout)

    else:
        raise ValueError(f"Invalid model type: {model_type}")

    # Define the loss function and optimizer
    criterion = nn.CrossEntropyLoss()

    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Invalid optimizer: {optimizer_name}")

    # Training loop
    num_epochs = 10
    total_loss = 0.0

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        # Mini-batch training
        for i in range(0, len(X_tensor), batch_size):
            inputs = X_tensor[i:i+batch_size]
            labels = y_tensor[i:i+batch_size]

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        epoch_loss = running_loss / (len(X_tensor) / batch_size)
        total_loss += epoch_loss

    avg_loss = total_loss / num_epochs
    return float(avg_loss)

--------------------------------------------------------------------------------
[2025-05-20 12:06:36] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#3): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-05-20 12:06:44] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
from typing import Any

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from ConfigSpace import Configuration


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing the training data, with keys 'X' and 'y'.

    Returns:
        float: The average training loss over 10 epochs.
    """
    X = dataset['X']
    y = dataset['y']

    # Convert data to PyTorch tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.long)

    model_type = cfg.get('model_type')
    learning_rate = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    num_layers = cfg.get('num_layers')
    hidden_size = cfg.get('hidden_size')
    dropout = cfg.get('dropout')
    optimizer_name = cfg.get('optimizer')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        num_samples, seq_len, num_features = X_tensor.shape
    elif X_tensor.ndim == 2:
        num_samples, num_features = X_tensor.shape
    else:
        raise ValueError(f"Unexpected input dimension: {X_tensor.ndim}")
    num_classes = len(np.unique(y))

    # Define the model
    class LSTMModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(LSTMModel, self).__init__()
            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.lstm(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class GRUModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(GRUModel, self).__init__()
            self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.gru(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class RNNModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(RNNModel, self).__init__()
            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.rnn(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class MLPModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(MLPModel, self).__init__()
            self.layers = nn.ModuleList()
            self.layers.append(nn.Linear(input_size, hidden_size))
            self.layers.append(nn.ReLU())
            self.layers.append(nn.Dropout(dropout))
            for _ in range(num_layers - 1):
                self.layers.append(nn.Linear(hidden_size, hidden_size))
                self.layers.append(nn.ReLU())
                self.layers.append(nn.Dropout(dropout))
            self.layers.append(nn.Linear(hidden_size, num_classes))

        def forward(self, x):
            for layer in self.layers:
                x = layer(x)
            return x

    if model_type in ['LSTM', 'GRU', 'RNN']:
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")
        input_size = X_tensor.shape[2]
        if model_type == 'LSTM':
            model = LSTMModel(input_size, hidden_size, num_layers, num_classes, dropout)
        elif model_type == 'GRU':
            model = GRUModel(input_size, hidden_size, num_layers, num_classes, dropout)
        else:  # model_type == 'RNN'
            model = RNNModel(input_size, hidden_size, num_layers, num_classes, dropout)

    elif model_type == 'MLP':
        if X_tensor.ndim != 2:
            raise ValueError(f"Expected 2D input (batch, features) for MLP, got {X_tensor.shape}")
        input_size = X_tensor.shape[1]
        model = MLPModel(input_size, hidden_size, num_layers, num_classes, dropout)

    else:
        raise ValueError(f"Invalid model type: {model_type}")

    # Define the loss function and optimizer
    criterion = nn.CrossEntropyLoss()

    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Invalid optimizer: {optimizer_name}")

    # Training loop
    num_epochs = 10
    total_loss = 0.0

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        # Mini-batch training
        for i in range(0, len(X_tensor), batch_size):
            inputs = X_tensor[i:i+batch_size]
            labels = y_tensor[i:i+batch_size]

            if len(inputs) == 0:
                continue

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        epoch_loss = running_loss / (len(X_tensor) / batch_size)
        total_loss += epoch_loss

    avg_loss = total_loss / num_epochs
    return float(avg_loss)
```
--------------------------------------------------------------------------------
[2025-05-20 12:06:44] [Metadata: {'component': 'train_function', 'action': 'run'}] from typing import Any

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from ConfigSpace import Configuration


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing the training data, with keys 'X' and 'y'.

    Returns:
        float: The average training loss over 10 epochs.
    """
    X = dataset['X']
    y = dataset['y']

    # Convert data to PyTorch tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.long)

    model_type = cfg.get('model_type')
    learning_rate = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    num_layers = cfg.get('num_layers')
    hidden_size = cfg.get('hidden_size')
    dropout = cfg.get('dropout')
    optimizer_name = cfg.get('optimizer')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        num_samples, seq_len, num_features = X_tensor.shape
    elif X_tensor.ndim == 2:
        num_samples, num_features = X_tensor.shape
    else:
        raise ValueError(f"Unexpected input dimension: {X_tensor.ndim}")
    num_classes = len(np.unique(y))

    # Define the model
    class LSTMModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(LSTMModel, self).__init__()
            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.lstm(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class GRUModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(GRUModel, self).__init__()
            self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.gru(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class RNNModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(RNNModel, self).__init__()
            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.rnn(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class MLPModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(MLPModel, self).__init__()
            self.layers = nn.ModuleList()
            self.layers.append(nn.Linear(input_size, hidden_size))
            self.layers.append(nn.ReLU())
            self.layers.append(nn.Dropout(dropout))
            for _ in range(num_layers - 1):
                self.layers.append(nn.Linear(hidden_size, hidden_size))
                self.layers.append(nn.ReLU())
                self.layers.append(nn.Dropout(dropout))
            self.layers.append(nn.Linear(hidden_size, num_classes))

        def forward(self, x):
            for layer in self.layers:
                x = layer(x)
            return x

    if model_type in ['LSTM', 'GRU', 'RNN']:
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")
        input_size = X_tensor.shape[2]
        if model_type == 'LSTM':
            model = LSTMModel(input_size, hidden_size, num_layers, num_classes, dropout)
        elif model_type == 'GRU':
            model = GRUModel(input_size, hidden_size, num_layers, num_classes, dropout)
        else:  # model_type == 'RNN'
            model = RNNModel(input_size, hidden_size, num_layers, num_classes, dropout)

    elif model_type == 'MLP':
        if X_tensor.ndim != 2:
            raise ValueError(f"Expected 2D input (batch, features) for MLP, got {X_tensor.shape}")
        input_size = X_tensor.shape[1]
        model = MLPModel(input_size, hidden_size, num_layers, num_classes, dropout)

    else:
        raise ValueError(f"Invalid model type: {model_type}")

    # Define the loss function and optimizer
    criterion = nn.CrossEntropyLoss()

    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Invalid optimizer: {optimizer_name}")

    # Training loop
    num_epochs = 10
    total_loss = 0.0

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        # Mini-batch training
        for i in range(0, len(X_tensor), batch_size):
            inputs = X_tensor[i:i+batch_size]
            labels = y_tensor[i:i+batch_size]

            if len(inputs) == 0:
                continue

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        epoch_loss = running_loss / (len(X_tensor) / batch_size)
        total_loss += epoch_loss

    avg_loss = total_loss / num_epochs
    return float(avg_loss)

--------------------------------------------------------------------------------
[2025-05-20 12:06:44] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#4): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-05-20 12:06:51] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
from typing import Any

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from ConfigSpace import Configuration


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing the training data, with keys 'X' and 'y'.

    Returns:
        float: The average training loss over 10 epochs.
    """
    X = dataset['X']
    y = dataset['y']

    # Convert data to PyTorch tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.long)

    model_type = cfg.get('model_type')
    learning_rate = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    num_layers = cfg.get('num_layers')
    hidden_size = cfg.get('hidden_size')
    dropout = cfg.get('dropout')
    optimizer_name = cfg.get('optimizer')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        num_samples, seq_len, num_features = X_tensor.shape
    elif X_tensor.ndim == 2:
        num_samples, num_features = X_tensor.shape
    else:
        raise ValueError(f"Unexpected input dimension: {X_tensor.ndim}")
    num_classes = len(np.unique(y))

    # Define the model
    class LSTMModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(LSTMModel, self).__init__()
            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0.0)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.lstm(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class GRUModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(GRUModel, self).__init__()
            self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0.0)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.gru(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class RNNModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(RNNModel, self).__init__()
            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0.0)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.rnn(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class MLPModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(MLPModel, self).__init__()
            self.layers = nn.ModuleList()
            self.layers.append(nn.Linear(input_size, hidden_size))
            self.layers.append(nn.ReLU())
            self.layers.append(nn.Dropout(dropout))
            for _ in range(num_layers - 1):
                self.layers.append(nn.Linear(hidden_size, hidden_size))
                self.layers.append(nn.ReLU())
                self.layers.append(nn.Dropout(dropout))
            self.layers.append(nn.Linear(hidden_size, num_classes))

        def forward(self, x):
            for layer in self.layers:
                x = layer(x)
            return x

    if model_type in ['LSTM', 'GRU', 'RNN']:
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")
        input_size = X_tensor.shape[2]
        if model_type == 'LSTM':
            model = LSTMModel(input_size, hidden_size, num_layers, num_classes, dropout)
        elif model_type == 'GRU':
            model = GRUModel(input_size, hidden_size, num_layers, num_classes, dropout)
        else:  # model_type == 'RNN'
            model = RNNModel(input_size, hidden_size, num_layers, num_classes, dropout)

    elif model_type == 'MLP':
        if X_tensor.ndim != 2:
            raise ValueError(f"Expected 2D input (batch, features) for MLP, got {X_tensor.shape}")
        input_size = X_tensor.shape[1]
        model = MLPModel(input_size, hidden_size, num_layers, num_classes, dropout)

    else:
        raise ValueError(f"Invalid model type: {model_type}")

    # Define the loss function and optimizer
    criterion = nn.CrossEntropyLoss()

    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Invalid optimizer: {optimizer_name}")

    # Training loop
    num_epochs = 10
    total_loss = 0.0

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        # Mini-batch training
        for i in range(0, len(X_tensor), batch_size):
            inputs = X_tensor[i:i+batch_size]
            labels = y_tensor[i:i+batch_size]

            if len(inputs) == 0:
                continue

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        epoch_loss = running_loss / (len(X_tensor) / batch_size)
        total_loss += epoch_loss

    avg_loss = total_loss / num_epochs
    return float(avg_loss)
```
--------------------------------------------------------------------------------
[2025-05-20 12:06:51] [Metadata: {'component': 'train_function', 'action': 'run'}] from typing import Any

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from ConfigSpace import Configuration


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing the training data, with keys 'X' and 'y'.

    Returns:
        float: The average training loss over 10 epochs.
    """
    X = dataset['X']
    y = dataset['y']

    # Convert data to PyTorch tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.long)

    model_type = cfg.get('model_type')
    learning_rate = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    num_layers = cfg.get('num_layers')
    hidden_size = cfg.get('hidden_size')
    dropout = cfg.get('dropout')
    optimizer_name = cfg.get('optimizer')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        num_samples, seq_len, num_features = X_tensor.shape
    elif X_tensor.ndim == 2:
        num_samples, num_features = X_tensor.shape
    else:
        raise ValueError(f"Unexpected input dimension: {X_tensor.ndim}")
    num_classes = len(np.unique(y))

    # Define the model
    class LSTMModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(LSTMModel, self).__init__()
            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0.0)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.lstm(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class GRUModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(GRUModel, self).__init__()
            self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0.0)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.gru(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class RNNModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(RNNModel, self).__init__()
            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout if num_layers > 1 else 0.0)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.rnn(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class MLPModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(MLPModel, self).__init__()
            self.layers = nn.ModuleList()
            self.layers.append(nn.Linear(input_size, hidden_size))
            self.layers.append(nn.ReLU())
            self.layers.append(nn.Dropout(dropout))
            for _ in range(num_layers - 1):
                self.layers.append(nn.Linear(hidden_size, hidden_size))
                self.layers.append(nn.ReLU())
                self.layers.append(nn.Dropout(dropout))
            self.layers.append(nn.Linear(hidden_size, num_classes))

        def forward(self, x):
            for layer in self.layers:
                x = layer(x)
            return x

    if model_type in ['LSTM', 'GRU', 'RNN']:
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")
        input_size = X_tensor.shape[2]
        if model_type == 'LSTM':
            model = LSTMModel(input_size, hidden_size, num_layers, num_classes, dropout)
        elif model_type == 'GRU':
            model = GRUModel(input_size, hidden_size, num_layers, num_classes, dropout)
        else:  # model_type == 'RNN'
            model = RNNModel(input_size, hidden_size, num_layers, num_classes, dropout)

    elif model_type == 'MLP':
        if X_tensor.ndim != 2:
            raise ValueError(f"Expected 2D input (batch, features) for MLP, got {X_tensor.shape}")
        input_size = X_tensor.shape[1]
        model = MLPModel(input_size, hidden_size, num_layers, num_classes, dropout)

    else:
        raise ValueError(f"Invalid model type: {model_type}")

    # Define the loss function and optimizer
    criterion = nn.CrossEntropyLoss()

    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Invalid optimizer: {optimizer_name}")

    # Training loop
    num_epochs = 10
    total_loss = 0.0

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        # Mini-batch training
        for i in range(0, len(X_tensor), batch_size):
            inputs = X_tensor[i:i+batch_size]
            labels = y_tensor[i:i+batch_size]

            if len(inputs) == 0:
                continue

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        epoch_loss = running_loss / (len(X_tensor) / batch_size)
        total_loss += epoch_loss

    avg_loss = total_loss / num_epochs
    return float(avg_loss)

--------------------------------------------------------------------------------
[2025-05-20 12:06:51] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#5): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-05-20 12:06:58] [Metadata: {'component': 'train_function', 'action': 'run'}] from typing import Any
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from ConfigSpace import Configuration

def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing the training data ('X' and 'y').

    Returns:
        float: The average training loss over 10 epochs.
    """

    X = dataset['X']
    y = dataset['y']

    # Convert to numpy arrays if they aren't already
    if not isinstance(X, np.ndarray):
        X = np.array(X)
    if not isinstance(y, np.ndarray):
        y = np.array(y)

    # Infer input and output dimensions
    input_size = X.shape[1] if len(X.shape) > 1 else 1 # Handle 1D input
    num_classes = len(np.unique(y))

    # Hyperparameters from configuration
    model_type = cfg.get('model_type')
    learning_rate = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    optimizer_name = cfg.get('optimizer')

    # Convert data to PyTorch tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.long)

    # Create DataLoader
    train_dataset = TensorDataset(X_tensor, y_tensor)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    # Model definition
    if model_type == 'MLP':
        model = nn.Sequential(
            nn.Linear(input_size, cfg.get('hidden_size')),
            nn.ReLU(),
            nn.Linear(cfg.get('hidden_size'), num_classes)
        )
    elif model_type in ['LSTM', 'GRU', 'RNN']:
        # Time Series Handling

        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")

        sequence_length = X_tensor.shape[1]
        feature_dim = X_tensor.shape[2]

        hidden_size = cfg.get('hidden_size')
        num_layers = cfg.get('num_layers')
        dropout = cfg.get('dropout')

        if model_type == 'LSTM':
            model = nn.LSTM(feature_dim, hidden_size, num_layers, batch_first=True, dropout=dropout)
        elif model_type == 'GRU':
            model = nn.GRU(feature_dim, hidden_size, num_layers, batch_first=True, dropout=dropout)
        elif model_type == 'RNN':
            model = nn.RNN(feature_dim, hidden_size, num_layers, batch_first=True, dropout=dropout)

        # Add a final linear layer to map the hidden state to the output
        model = nn.Sequential(
            model,
            nn.Linear(hidden_size, num_classes) #Output Layer
        )

    else:
        raise ValueError(f"Unsupported model type: {model_type}")
    # Loss function and optimizer
    criterion = nn.CrossEntropyLoss()
    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Unsupported optimizer: {optimizer_name}")

    # Training loop
    num_epochs = 10
    total_loss = 0.0
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for X_batch, y_batch in train_loader:
            # Forward pass
            if model_type in ['LSTM', 'GRU', 'RNN']:
                out, _ = model[0](X_batch) #Pass through recurrent layer
                outputs = model[1](out[:, -1, :])# Pass only the last output to the fully connected layer
            else:
                outputs = model(X_batch)
            loss = criterion(outputs, y_batch)

            # Backward and optimize
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_epoch_loss = epoch_loss / len(train_loader)
        total_loss += avg_epoch_loss

    avg_loss = total_loss / num_epochs
    return float(avg_loss)

--------------------------------------------------------------------------------
[2025-05-20 12:06:58] [Metadata: {'component': 'train_function', 'status': 'success', 'loss': 31.761103395179468}] Training executed successfully, loss: 31.761103395179468
--------------------------------------------------------------------------------
