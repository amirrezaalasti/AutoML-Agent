[2025-05-20 12:05:03] [Metadata: {'component': 'config'}] **TASK**

Goal: Write a Python function called `get_configspace()` that returns a valid `ConfigurationSpace` for a classification task.

---

**STRICT OUTPUT RULES**

* Output only the `get_configspace()` function and necessary imports.
* Do not include any extra text, explanations, or comments.
* Code must be syntactically correct, executable, and compatible with SMAC.

---

**ALLOWED CLASSES**

**Core Classes**

* `ConfigurationSpace`
* `Categorical`
* `Float`
* `Integer`
* `Constant`

**Conditions**

* `EqualsCondition`
* `InCondition`
* `OrConjunction`

**Forbidden Clauses**

* `ForbiddenEqualsClause`
* `ForbiddenAndConjunction`

**Distributions (only if needed)**

* `Beta`
* `Normal`

**Serialization (only if needed)**

* `to_yaml()`
* `from_yaml()`

---

**ALLOWED OPTIONS**

* `default`
* `log`
* `distribution`
* `seed`

---

**CONSTRAINTS**

* Must include **at least one** `ForbiddenAndConjunction` to block invalid combinations.

---

**CONFIGURATION SPACE REQUIREMENTS**

* Initialize `ConfigurationSpace` with `seed=1234`.

---

**DATASET DESCRIPTION**

* The configuration space must be based on the following information
This is a time series dataset.
Number of samples: 289
Time index type: <class 'pandas.core.indexes.range.RangeIndex'>
Time range: 0 to 288
Features:
- 0

### Time Series Handling Requirements
- Assume `dataset['X']` is a 3D array or tensor with shape `(num_samples, sequence_length, num_features)`.
- If `dataset['X']` is 2D, raise a `ValueError` if the model is RNN-based (`LSTM`, `GRU`, `RNN`).
- Do **not** flatten the input when using RNN-based models.
- Use `batch_first=True` in all recurrent models to maintain `(batch, seq_len, features)` format.
- Dynamically infer sequence length as `X.shape[1]` and feature dimension as `X.shape[2]`.
- If `X.ndim != 3` and a sequential model is selected, raise a clear error with shape info.
- Example input validation check:
  ```python
  if model_type in ['LSTM', 'GRU', 'RNN'] and X_tensor.ndim != 3:
      raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")
  ```
- Time index or datetime values can be logged but should not be used in the model unless specified.
.
* Hyperparameters and model choices must reflect what is appropriate for that dataset type.

---

**IMPORTANT RULE**

* Do **not** use any classes, functions, methods, or modules outside of the **ALLOWED CLASSES**.

[EXAMPLES]

# Example 1: Basic ConfigurationSpace
```python
from ConfigSpace import ConfigurationSpace

cs = ConfigurationSpace(
    space={
        "C": (-1.0, 1.0),
        "max_iter": (10, 100),
    },
    seed=1234,
)
```
# Example 2: Adding Hyperparameters
```python
from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer

kernel_type = Categorical('kernel_type', ['linear', 'poly', 'rbf', 'sigmoid'])
degree = Integer('degree', bounds=(2, 4), default=2)
coef0 = Float('coef0', bounds=(0, 1), default=0.0)
gamma = Float('gamma', bounds=(1e-5, 1e2), default=1, log=True)

cs = ConfigurationSpace()
cs.add([kernel_type, degree, coef0, gamma])
```
# Example 3: Adding Conditions
```python
from ConfigSpace import EqualsCondition, InCondition, OrConjunction

cond_1 = EqualsCondition(degree, kernel_type, 'poly')
cond_2 = OrConjunction(
    EqualsCondition(coef0, kernel_type, 'poly'),
    EqualsCondition(coef0, kernel_type, 'sigmoid')
)
cond_3 = InCondition(gamma, kernel_type, ['rbf', 'poly', 'sigmoid'])
```
# Example 4: Adding Forbidden Clauses
```pyhon
from ConfigSpace import ForbiddenEqualsClause, ForbiddenAndConjunction

penalty_and_loss = ForbiddenAndConjunction(
    ForbiddenEqualsClause(penalty, "l1"),
    ForbiddenEqualsClause(loss, "hinge")
)
constant_penalty_and_loss = ForbiddenAndConjunction(
    ForbiddenEqualsClause(dual, "False"),
    ForbiddenEqualsClause(penalty, "l2"),
    ForbiddenEqualsClause(loss, "hinge")
)
penalty_and_dual = ForbiddenAndConjunction(
    ForbiddenEqualsClause(dual, "False"),
    ForbiddenEqualsClause(penalty, "l1")
)
```
Example 5: Serialization
```python
from pathlib import Path
from ConfigSpace import ConfigurationSpace

path = Path("configspace.yaml")
cs = ConfigurationSpace(
    space={
        "C": (-1.0, 1.0),
        "max_iter": (10, 100),
    },
    seed=1234,
)
cs.to_yaml(path)
loaded_cs = ConfigurationSpace.from_yaml(path)
```
# Example 6: Priors
```python
import numpy as np
from ConfigSpace import ConfigurationSpace, Float, Categorical, Beta, Normal

cs = ConfigurationSpace(
    space={
        "lr": Float(
            'lr',
            bounds=(1e-5, 1e-1),
            default=1e-3,
            log=True,
            distribution=Normal(1e-3, 1e-1)
        ),
        "dropout": Float(
            'dropout',
            bounds=(0, 0.99),
            default=0.25,
            distribution=Beta(alpha=2, beta=4)
        ),
        "activation": Categorical(
            'activation',
            items=['tanh', 'relu'],
            weights=[0.2, 0.8]
        ),
    },
    seed=1234,
)
```

--------------------------------------------------------------------------------
[2025-05-20 12:05:03] [Metadata: {'component': 'config', 'action': 'run'}] Running config code:
--------------------------------------------------------------------------------
[2025-05-20 12:05:03] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#1): 'UniformIntegerHyperparameter' object has no attribute 'default'
--------------------------------------------------------------------------------
[2025-05-20 12:05:03] [Metadata: {'component': 'config', 'action': 'fix'}] The following python code is generated by an LLM which now has run into the following errors:
'UniformIntegerHyperparameter' object has no attribute 'default'

Your task is to fix the issues in the code. The requirements below MUST be respected:

1. Do not change the function name or its purpose.
2. Fix all syntax and runtime errors.
3. Use only valid parameters.
4. Do not change the logic unless necessary for fixing errors.
5. Return ONLY the corrected function code — no explanations, comments, or extra output.
6. The function must be syntactically and semantically correct, and executable with `exec`.

Buggy code:
```python
from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause

def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical('model_type', ['MLP', 'LSTM', 'GRU', 'RNN', 'TCN'], default='MLP')
    learning_rate = Float('learning_rate', (1e-5, 1e-2), default=1e-3, log=True)
    batch_size = Categorical('batch_size', [16, 32, 64, 128], default=32)
    optimizer = Categorical('optimizer', ['Adam', 'SGD', 'RMSprop'], default='Adam')
    num_layers = Integer('num_layers', (1, 3), default=2)
    dropout_rate = Float('dropout_rate', (0.0, 0.9), default=0.5)

    # MLP Specific
    mlp_hidden_dim = Integer('mlp_hidden_dim', (32, 256), default=128, log=True)

    # RNN Specific
    rnn_hidden_dim = Integer('rnn_hidden_dim', (32, 256), default=64, log=True)
    
    # TCN Specific
    tcn_num_filters = Integer('tcn_num_filters', (32, 256), default=64, log=True)
    tcn_kernel_size = Integer('tcn_kernel_size', (2, 8), default=3)

    # Add hyperparameters to the configuration space
    cs.add([model_type, learning_rate, batch_size, optimizer, num_layers, dropout_rate])

    # Add hyperparameters specific to MLP
    cs.add([mlp_hidden_dim])
    
    # Add hyperparameters specific to RNN
    cs.add([rnn_hidden_dim])
    
    # Add hyperparameters specific to TCN
    cs.add([tcn_num_filters, tcn_kernel_size])

    # Add Forbidden Clauses
    forbidden_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'LSTM'),
        ForbiddenEqualsClause(mlp_hidden_dim, mlp_hidden_dim.default)
    )

    forbidden_lstm = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'MLP'),
        ForbiddenEqualsClause(rnn_hidden_dim, rnn_hidden_dim.default)
    )
    
    forbidden_tcn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'MLP'),
        ForbiddenEqualsClause(tcn_num_filters, tcn_num_filters.default)
    )

    cs.add_forbidden_clause(forbidden_mlp)
    cs.add_forbidden_clause(forbidden_lstm)
    cs.add_forbidden_clause(forbidden_tcn)

    return cs
```

--------------------------------------------------------------------------------
[2025-05-20 12:05:06] [Metadata: {'component': 'config', 'action': 'run'}] Running config code:
--------------------------------------------------------------------------------
[2025-05-20 12:05:06] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#2): Float() takes 2 positional arguments but 3 positional arguments (and 2 keyword-only arguments) were given
--------------------------------------------------------------------------------
[2025-05-20 12:05:06] [Metadata: {'component': 'config', 'action': 'fix'}] The following python code is generated by an LLM which now has run into the following errors:
'UniformIntegerHyperparameter' object has no attribute 'default'
Float() takes 2 positional arguments but 3 positional arguments (and 2 keyword-only arguments) were given

Your task is to fix the issues in the code. The requirements below MUST be respected:

1. Do not change the function name or its purpose.
2. Fix all syntax and runtime errors.
3. Use only valid parameters.
4. Do not change the logic unless necessary for fixing errors.
5. Return ONLY the corrected function code — no explanations, comments, or extra output.
6. The function must be syntactically and semantically correct, and executable with `exec`.

Buggy code:
```python
from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause

def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical('model_type', ['MLP', 'LSTM', 'GRU', 'RNN', 'TCN'], default='MLP')
    learning_rate = Float('learning_rate', 1e-5, 1e-2, default=1e-3, log=True)
    batch_size = Categorical('batch_size', [16, 32, 64, 128], default=32)
    optimizer = Categorical('optimizer', ['Adam', 'SGD', 'RMSprop'], default='Adam')
    num_layers = Integer('num_layers', 1, 3, default=2)
    dropout_rate = Float('dropout_rate', 0.0, 0.9, default=0.5)

    # MLP Specific
    mlp_hidden_dim = Integer('mlp_hidden_dim', 32, 256, default=128, log=True)

    # RNN Specific
    rnn_hidden_dim = Integer('rnn_hidden_dim', 32, 256, default=64, log=True)
    
    # TCN Specific
    tcn_num_filters = Integer('tcn_num_filters', 32, 256, default=64, log=True)
    tcn_kernel_size = Integer('tcn_kernel_size', 2, 8, default=3)

    # Add hyperparameters to the configuration space
    cs.add_hyperparameters([model_type, learning_rate, batch_size, optimizer, num_layers, dropout_rate])

    # Add hyperparameters specific to MLP
    cs.add_hyperparameters([mlp_hidden_dim])
    
    # Add hyperparameters specific to RNN
    cs.add_hyperparameters([rnn_hidden_dim])
    
    # Add hyperparameters specific to TCN
    cs.add_hyperparameters([tcn_num_filters, tcn_kernel_size])

    # Add Forbidden Clauses
    forbidden_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'LSTM'),
        ForbiddenEqualsClause(mlp_hidden_dim, 128)
    )

    forbidden_lstm = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'MLP'),
        ForbiddenEqualsClause(rnn_hidden_dim, 64)
    )
    
    forbidden_tcn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'MLP'),
        ForbiddenEqualsClause(tcn_num_filters, 64)
    )

    cs.add_forbidden_clause(forbidden_mlp)
    cs.add_forbidden_clause(forbidden_lstm)
    cs.add_forbidden_clause(forbidden_tcn)

    return cs
```

--------------------------------------------------------------------------------
[2025-05-20 12:05:10] [Metadata: {'component': 'config', 'action': 'run'}] Running config code:
--------------------------------------------------------------------------------
[2025-05-20 12:05:10] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#3): Categorical() got an unexpected keyword argument 'default_value'
--------------------------------------------------------------------------------
[2025-05-20 12:05:10] [Metadata: {'component': 'config', 'action': 'fix'}] The following python code is generated by an LLM which now has run into the following errors:
'UniformIntegerHyperparameter' object has no attribute 'default'
Float() takes 2 positional arguments but 3 positional arguments (and 2 keyword-only arguments) were given
Categorical() got an unexpected keyword argument 'default_value'

Your task is to fix the issues in the code. The requirements below MUST be respected:

1. Do not change the function name or its purpose.
2. Fix all syntax and runtime errors.
3. Use only valid parameters.
4. Do not change the logic unless necessary for fixing errors.
5. Return ONLY the corrected function code — no explanations, comments, or extra output.
6. The function must be syntactically and semantically correct, and executable with `exec`.

Buggy code:
```python
from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause

def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical('model_type', ['MLP', 'LSTM', 'GRU', 'RNN', 'TCN'], default_value='MLP')
    learning_rate = Float('learning_rate', lower=1e-5, upper=1e-2, default_value=1e-3, log=True)
    batch_size = Categorical('batch_size', [16, 32, 64, 128], default_value=32)
    optimizer = Categorical('optimizer', ['Adam', 'SGD', 'RMSprop'], default_value='Adam')
    num_layers = Integer('num_layers', lower=1, upper=3, default_value=2)
    dropout_rate = Float('dropout_rate', lower=0.0, upper=0.9, default_value=0.5)

    # MLP Specific
    mlp_hidden_dim = Integer('mlp_hidden_dim', lower=32, upper=256, default_value=128, log=True)

    # RNN Specific
    rnn_hidden_dim = Integer('rnn_hidden_dim', lower=32, upper=256, default_value=64, log=True)
    
    # TCN Specific
    tcn_num_filters = Integer('tcn_num_filters', lower=32, upper=256, default_value=64, log=True)
    tcn_kernel_size = Integer('tcn_kernel_size', lower=2, upper=8, default_value=3)

    # Add hyperparameters to the configuration space
    cs.add_hyperparameters([model_type, learning_rate, batch_size, optimizer, num_layers, dropout_rate])

    # Add hyperparameters specific to MLP
    cs.add_hyperparameters([mlp_hidden_dim])
    
    # Add hyperparameters specific to RNN
    cs.add_hyperparameters([rnn_hidden_dim])
    
    # Add hyperparameters specific to TCN
    cs.add_hyperparameters([tcn_num_filters, tcn_kernel_size])

    # Add Forbidden Clauses
    forbidden_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'LSTM'),
        ForbiddenEqualsClause(mlp_hidden_dim, 128)
    )

    forbidden_lstm = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'MLP'),
        ForbiddenEqualsClause(rnn_hidden_dim, 64)
    )
    
    forbidden_tcn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'MLP'),
        ForbiddenEqualsClause(tcn_num_filters, 64)
    )

    cs.add_forbidden_clause(forbidden_mlp)
    cs.add_forbidden_clause(forbidden_lstm)
    cs.add_forbidden_clause(forbidden_tcn)

    return cs
```

--------------------------------------------------------------------------------
[2025-05-20 12:05:13] [Metadata: {'component': 'config', 'action': 'run'}] Running config code:
--------------------------------------------------------------------------------
[2025-05-20 12:05:13] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#4): Float() got an unexpected keyword argument 'lower'
--------------------------------------------------------------------------------
[2025-05-20 12:05:13] [Metadata: {'component': 'config', 'action': 'fix'}] The following python code is generated by an LLM which now has run into the following errors:
'UniformIntegerHyperparameter' object has no attribute 'default'
Float() takes 2 positional arguments but 3 positional arguments (and 2 keyword-only arguments) were given
Categorical() got an unexpected keyword argument 'default_value'
Float() got an unexpected keyword argument 'lower'

Your task is to fix the issues in the code. The requirements below MUST be respected:

1. Do not change the function name or its purpose.
2. Fix all syntax and runtime errors.
3. Use only valid parameters.
4. Do not change the logic unless necessary for fixing errors.
5. Return ONLY the corrected function code — no explanations, comments, or extra output.
6. The function must be syntactically and semantically correct, and executable with `exec`.

Buggy code:
```python
from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause

def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical('model_type', ['MLP', 'LSTM', 'GRU', 'RNN', 'TCN'])
    learning_rate = Float('learning_rate', lower=1e-5, upper=1e-2, log=True)
    batch_size = Categorical('batch_size', [16, 32, 64, 128])
    optimizer = Categorical('optimizer', ['Adam', 'SGD', 'RMSprop'])
    num_layers = Integer('num_layers', lower=1, upper=3)
    dropout_rate = Float('dropout_rate', lower=0.0, upper=0.9)

    # MLP Specific
    mlp_hidden_dim = Integer('mlp_hidden_dim', lower=32, upper=256, log=True)

    # RNN Specific
    rnn_hidden_dim = Integer('rnn_hidden_dim', lower=32, upper=256, log=True)
    
    # TCN Specific
    tcn_num_filters = Integer('tcn_num_filters', lower=32, upper=256, log=True)
    tcn_kernel_size = Integer('tcn_kernel_size', lower=2, upper=8)

    # Add hyperparameters to the configuration space
    cs.add_hyperparameters([model_type, learning_rate, batch_size, optimizer, num_layers, dropout_rate])

    # Add hyperparameters specific to MLP
    cs.add_hyperparameters([mlp_hidden_dim])
    
    # Add hyperparameters specific to RNN
    cs.add_hyperparameters([rnn_hidden_dim])
    
    # Add hyperparameters specific to TCN
    cs.add_hyperparameters([tcn_num_filters, tcn_kernel_size])

    # Add Forbidden Clauses
    forbidden_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'LSTM'),
        ForbiddenEqualsClause(mlp_hidden_dim, 128)
    )

    forbidden_lstm = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'MLP'),
        ForbiddenEqualsClause(rnn_hidden_dim, 64)
    )
    
    forbidden_tcn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, 'MLP'),
        ForbiddenEqualsClause(tcn_num_filters, 64)
    )

    cs.add_forbidden_clause(forbidden_mlp)
    cs.add_forbidden_clause(forbidden_lstm)
    cs.add_forbidden_clause(forbidden_tcn)

    return cs
```

--------------------------------------------------------------------------------
[2025-05-20 12:05:17] [Metadata: {'component': 'config', 'action': 'run'}] Running config code:
--------------------------------------------------------------------------------
[2025-05-20 12:05:17] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#5): Categorical() got an unexpected keyword argument 'choices'
--------------------------------------------------------------------------------
[2025-05-20 12:05:17] [Metadata: {'component': 'config', 'action': 'retry'}] Retry limit reached for config. Fetching fresh code from LLM.
--------------------------------------------------------------------------------
[2025-05-20 12:05:21] [Metadata: {'component': 'config', 'action': 'run'}] Running config code:
--------------------------------------------------------------------------------
[2025-05-20 12:05:21] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#1): Given vector violates forbidden clause: (Forbidden: model_type == 'MLP' && Forbidden: rnn_hidden_size == 64)
--------------------------------------------------------------------------------
[2025-05-20 12:05:21] [Metadata: {'component': 'config', 'action': 'fix'}] The following python code is generated by an LLM which now has run into the following errors:
Float() takes 2 positional arguments but 3 positional arguments (and 2 keyword-only arguments) were given
Categorical() got an unexpected keyword argument 'default_value'
Float() got an unexpected keyword argument 'lower'
Categorical() got an unexpected keyword argument 'choices'
Given vector violates forbidden clause: (Forbidden: model_type == 'MLP' && Forbidden: rnn_hidden_size == 64)

Your task is to fix the issues in the code. The requirements below MUST be respected:

1. Do not change the function name or its purpose.
2. Fix all syntax and runtime errors.
3. Use only valid parameters.
4. Do not change the logic unless necessary for fixing errors.
5. Return ONLY the corrected function code — no explanations, comments, or extra output.
6. The function must be syntactically and semantically correct, and executable with `exec`.

Buggy code:
```python
from ConfigSpace import (
    ConfigurationSpace,
    Categorical,
    Float,
    Integer,
    ForbiddenAndConjunction,
    ForbiddenEqualsClause,
)


def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical("model_type", ["LSTM", "GRU", "MLP", "Linear"], default="MLP")
    learning_rate = Float("learning_rate", (1e-5, 1e-2), default=1e-3, log=True)
    batch_size = Categorical("batch_size", [16, 32, 64], default=32)
    epochs = Integer("epochs", (10, 100), default=50)

    # MLP specific hyperparameters
    num_layers = Integer("num_layers", (1, 3), default=2)
    hidden_size = Integer("hidden_size", (32, 256), default=128, log=True)
    dropout = Float("dropout", (0.0, 0.99), default=0.5)

    # RNN specific hyperparameters
    rnn_hidden_size = Integer("rnn_hidden_size", (32, 256), default=64, log=True)
    rnn_num_layers = Integer("rnn_num_layers", (1, 3), default=1)

    # Add hyperparameters to the ConfigurationSpace
    cs.add_hyperparameters([model_type, learning_rate, batch_size, epochs])
    cs.add_hyperparameters([num_layers, hidden_size, dropout])  # MLP
    cs.add_hyperparameters([rnn_hidden_size, rnn_num_layers])  # RNN

    # Add forbidden clauses to avoid invalid configurations
    # e.g., MLP specific params when using RNN
    forbidden_mlp_rnn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "LSTM"),
        ForbiddenEqualsClause(model_type, "GRU"),
        ForbiddenEqualsClause(num_layers, 2)
    )

    cs.add_forbidden_clause(forbidden_mlp_rnn)

    forbidden_rnn_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "MLP"),
        ForbiddenEqualsClause(rnn_hidden_size, 64)
    )

    cs.add_forbidden_clause(forbidden_rnn_mlp)

    forbidden_linear_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "Linear"),
        ForbiddenEqualsClause(num_layers, 2)
    )

    cs.add_forbidden_clause(forbidden_linear_mlp)
    
    forbidden_linear_rnn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "Linear"),
        ForbiddenEqualsClause(rnn_hidden_size, 64)
    )

    cs.add_forbidden_clause(forbidden_linear_rnn)

    return cs
```

--------------------------------------------------------------------------------
[2025-05-20 12:05:25] [Metadata: {'component': 'config', 'action': 'run'}] Running config code:
--------------------------------------------------------------------------------
[2025-05-20 12:05:25] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#2): Categorical() got an unexpected keyword argument 'choices'
--------------------------------------------------------------------------------
[2025-05-20 12:05:25] [Metadata: {'component': 'config', 'action': 'fix'}] The following python code is generated by an LLM which now has run into the following errors:
Categorical() got an unexpected keyword argument 'default_value'
Float() got an unexpected keyword argument 'lower'
Categorical() got an unexpected keyword argument 'choices'
Given vector violates forbidden clause: (Forbidden: model_type == 'MLP' && Forbidden: rnn_hidden_size == 64)
Categorical() got an unexpected keyword argument 'choices'

Your task is to fix the issues in the code. The requirements below MUST be respected:

1. Do not change the function name or its purpose.
2. Fix all syntax and runtime errors.
3. Use only valid parameters.
4. Do not change the logic unless necessary for fixing errors.
5. Return ONLY the corrected function code — no explanations, comments, or extra output.
6. The function must be syntactically and semantically correct, and executable with `exec`.

Buggy code:
```python
from ConfigSpace import (
    ConfigurationSpace,
    Categorical,
    Float,
    Integer,
    ForbiddenAndConjunction,
    ForbiddenEqualsClause,
)


def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical("model_type", choices=["LSTM", "GRU", "MLP", "Linear"], default="MLP")
    learning_rate = Float("learning_rate", low=1e-5, high=1e-2, default=1e-3, log=True)
    batch_size = Categorical("batch_size", choices=[16, 32, 64], default=32)
    epochs = Integer("epochs", lower=10, upper=100, default=50)

    # MLP specific hyperparameters
    num_layers = Integer("num_layers", lower=1, upper=3, default=2)
    hidden_size = Integer("hidden_size", lower=32, upper=256, default=128, log=True)
    dropout = Float("dropout", low=0.0, high=0.99, default=0.5)

    # RNN specific hyperparameters
    rnn_hidden_size = Integer("rnn_hidden_size", lower=32, upper=256, default=64, log=True)
    rnn_num_layers = Integer("rnn_num_layers", lower=1, upper=3, default=1)

    # Add hyperparameters to the ConfigurationSpace
    cs.add_hyperparameters([model_type, learning_rate, batch_size, epochs])
    cs.add_hyperparameters([num_layers, hidden_size, dropout])  # MLP
    cs.add_hyperparameters([rnn_hidden_size, rnn_num_layers])  # RNN

    # Add forbidden clauses to avoid invalid configurations
    # e.g., MLP specific params when using RNN
    forbidden_mlp_rnn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "LSTM"),
        ForbiddenEqualsClause(model_type, "GRU"),
        ForbiddenEqualsClause(model_type, "Linear")
    )

    cs.add_forbidden_clause(forbidden_mlp_rnn)

    forbidden_rnn_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "MLP"),
        ForbiddenEqualsClause(rnn_hidden_size, 64)
    )

    cs.add_forbidden_clause(forbidden_rnn_mlp)
    
    return cs
```

--------------------------------------------------------------------------------
[2025-05-20 12:05:28] [Metadata: {'component': 'config', 'action': 'run'}] Running config code:
--------------------------------------------------------------------------------
[2025-05-20 12:05:28] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#3): Categorical() got an unexpected keyword argument 'choices'
--------------------------------------------------------------------------------
[2025-05-20 12:05:28] [Metadata: {'component': 'config', 'action': 'fix'}] The following python code is generated by an LLM which now has run into the following errors:
Float() got an unexpected keyword argument 'lower'
Categorical() got an unexpected keyword argument 'choices'
Given vector violates forbidden clause: (Forbidden: model_type == 'MLP' && Forbidden: rnn_hidden_size == 64)
Categorical() got an unexpected keyword argument 'choices'
Categorical() got an unexpected keyword argument 'choices'

Your task is to fix the issues in the code. The requirements below MUST be respected:

1. Do not change the function name or its purpose.
2. Fix all syntax and runtime errors.
3. Use only valid parameters.
4. Do not change the logic unless necessary for fixing errors.
5. Return ONLY the corrected function code — no explanations, comments, or extra output.
6. The function must be syntactically and semantically correct, and executable with `exec`.

Buggy code:
```python
from ConfigSpace import (
    ConfigurationSpace,
    Categorical,
    Float,
    Integer,
    ForbiddenAndConjunction,
    ForbiddenEqualsClause,
)


def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical("model_type", choices=["LSTM", "GRU", "MLP", "Linear"])
    learning_rate = Float("learning_rate", lower=1e-5, upper=1e-2, log=True)
    batch_size = Categorical("batch_size", choices=[16, 32, 64])
    epochs = Integer("epochs", lower=10, upper=100)

    # MLP specific hyperparameters
    num_layers = Integer("num_layers", lower=1, upper=3)
    hidden_size = Integer("hidden_size", lower=32, upper=256, log=True)
    dropout = Float("dropout", lower=0.0, upper=0.99)

    # RNN specific hyperparameters
    rnn_hidden_size = Integer("rnn_hidden_size", lower=32, upper=256, log=True)
    rnn_num_layers = Integer("rnn_num_layers", lower=1, upper=3)

    # Add hyperparameters to the ConfigurationSpace
    cs.add_hyperparameters([model_type, learning_rate, batch_size, epochs])
    cs.add_hyperparameters([num_layers, hidden_size, dropout])  # MLP
    cs.add_hyperparameters([rnn_hidden_size, rnn_num_layers])  # RNN

    # Add forbidden clauses to avoid invalid configurations
    # e.g., MLP specific params when using RNN
    forbidden_mlp_rnn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "LSTM"),
        ForbiddenEqualsClause(model_type, "GRU"),
        ForbiddenEqualsClause(model_type, "Linear")
    )

    cs.add_forbidden_clause(forbidden_mlp_rnn)

    forbidden_rnn_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "MLP"),
        ForbiddenEqualsClause(rnn_hidden_size, 64)
    )

    cs.add_forbidden_clause(forbidden_rnn_mlp)
    
    return cs
```

--------------------------------------------------------------------------------
[2025-05-20 12:05:31] [Metadata: {'component': 'config', 'action': 'run'}] Running config code:
--------------------------------------------------------------------------------
[2025-05-20 12:05:31] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#4): Categorical() got an unexpected keyword argument 'choices'
--------------------------------------------------------------------------------
[2025-05-20 12:05:31] [Metadata: {'component': 'config', 'action': 'fix'}] The following python code is generated by an LLM which now has run into the following errors:
Categorical() got an unexpected keyword argument 'choices'
Given vector violates forbidden clause: (Forbidden: model_type == 'MLP' && Forbidden: rnn_hidden_size == 64)
Categorical() got an unexpected keyword argument 'choices'
Categorical() got an unexpected keyword argument 'choices'
Categorical() got an unexpected keyword argument 'choices'

Your task is to fix the issues in the code. The requirements below MUST be respected:

1. Do not change the function name or its purpose.
2. Fix all syntax and runtime errors.
3. Use only valid parameters.
4. Do not change the logic unless necessary for fixing errors.
5. Return ONLY the corrected function code — no explanations, comments, or extra output.
6. The function must be syntactically and semantically correct, and executable with `exec`.

Buggy code:
```python
from ConfigSpace import (
    ConfigurationSpace,
    Categorical,
    Float,
    Integer,
    ForbiddenAndConjunction,
    ForbiddenEqualsClause,
)


def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical("model_type", choices=["LSTM", "GRU", "MLP", "Linear"])
    learning_rate = Float("learning_rate", lower=1e-5, upper=1e-2, log=True)
    batch_size = Categorical("batch_size", choices=[16, 32, 64])
    epochs = Integer("epochs", lower=10, upper=100)

    # MLP specific hyperparameters
    num_layers = Integer("num_layers", lower=1, upper=3)
    hidden_size = Integer("hidden_size", lower=32, upper=256, log=True)
    dropout = Float("dropout", lower=0.0, upper=0.99)

    # RNN specific hyperparameters
    rnn_hidden_size = Integer("rnn_hidden_size", lower=32, upper=256, log=True)
    rnn_num_layers = Integer("rnn_num_layers", lower=1, upper=3)

    # Add hyperparameters to the ConfigurationSpace
    cs.add_hyperparameters([model_type, learning_rate, batch_size, epochs])
    cs.add_hyperparameters([num_layers, hidden_size, dropout])  # MLP
    cs.add_hyperparameters([rnn_hidden_size, rnn_num_layers])  # RNN

    # Add forbidden clauses to avoid invalid configurations
    # e.g., MLP specific params when using RNN
    forbidden_mlp_rnn = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "LSTM"),
        ForbiddenEqualsClause(model_type, "GRU"),
        ForbiddenEqualsClause(model_type, "Linear"),
        
    )

    cs.add_forbidden_clause(forbidden_mlp_rnn)

    forbidden_rnn_mlp = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "MLP"),
    )

    cs.add_forbidden_clause(forbidden_rnn_mlp)
    
    return cs
```

--------------------------------------------------------------------------------
[2025-05-20 12:05:34] [Metadata: {'component': 'config', 'action': 'run'}] Running config code:
--------------------------------------------------------------------------------
[2025-05-20 12:05:34] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#5): Categorical() got an unexpected keyword argument 'choices'
--------------------------------------------------------------------------------
[2025-05-20 12:05:34] [Metadata: {'component': 'config', 'action': 'retry'}] Retry limit reached for config. Fetching fresh code from LLM.
--------------------------------------------------------------------------------
[2025-05-20 12:05:36] [Metadata: {'component': 'config', 'action': 'run'}] Running config code:
--------------------------------------------------------------------------------
[2025-05-20 12:05:37] [Metadata: {'component': 'scenario'}] ---

**Objective:**
Generate a **Python function** named `generate_scenario(cs)` that returns a valid `Scenario` object configured for SMAC (v2.0+), strictly following the rules below.

---

**Output Format Rules (Strict):**

* Output **only** the function `generate_scenario(cs)` and the **necessary import statements**.
* Use **Python 3.10 syntax** but **do not** include type annotations for the function or parameters.
* The code must be **fully executable** with the latest **SMAC v2.0+** version.
* Output **only valid Python code** – **no comments**, **no explanations**, **no extra text**, and **no example usage**.
* The function must be **self-contained**.

---

**Functional Requirements:**

* The input `cs` is a `ConfigurationSpace` object.
* Return a `Scenario` configured with the following:
  * `output_directory`: `"./automl_results"`
  * `deterministic`: `False` (enable variability)
  * `n_workers`: greater than 1 (to enable parallel optimization)
  * `min_budget` and `max_budget`: set appropriately for multi-fidelity tuning (e.g., training epochs)
---

**Reminder:** The output must be limited to:

* Valid `import` statements
* A single `generate_scenario(cs)` function that returns a properly configured `Scenario` object
* Do not use any parameters other than the ones explicitly listed in this prompt.

---

**Example (Correct Output Format):**

```python
from smac import Scenario
from ConfigSpace import Configuration

def generate_scenario(cs: Configuration):
    scenario = Scenario(
        configspace=cs,
        objectives="validation_loss",
        output_directory="./automl_results",
        deterministic=False,
        min_budget=1,
        max_budget=100,
        n_workers=4
    )
    return scenario
```

--------------------------------------------------------------------------------
[2025-05-20 12:05:37] [Metadata: {'component': 'scenario', 'action': 'run'}] Running scenario code:
--------------------------------------------------------------------------------
[2025-05-20 12:05:45] [Metadata: {'component': 'train_function'}] **Generate production-grade Python code for a machine learning training function with the following STRICT requirements:**

---

### **Function signature** must be:

```python
from ConfigSpace import Configuration
def train(cfg: Configuration, dataset: Any) -> float:
```

---

### **Function Behavior Requirements:**

* The function **must accept** a `dataset` dictionary with:

  * `dataset['X']`: feature matrix or input tensor
  * `dataset['y']`: label vector or label tensor

* Assume `cfg` is a sampled configuration object:

  * Access primitive values using `cfg.get('key')` (only `int`, `float`, `str`, etc.).
  * **Do not access or manipulate non-primitive hyperparameter objects**.

* The function must return the **average training loss** over 10 epochs.

* You must check whether dataset['X'] is already image-shaped (e.g., len(X.shape) == 4). If not, and CNN is used, reshape carefully and raise a ValueError if the input size is not a perfect square.

* Do not assume dataset['X'] has a specific shape. Always verify input dimensions before reshaping.

* If using a CNN model, you must validate that reshaping is safe and explain your assumption.

```python
return loss  # float
```

* Lower `loss` means a better model.

---

### **Frameworks**

You may choose **PyTorch**, **TensorFlow**, or **scikit-learn**, depending on the dataset and supporting code provided.

---

### **Model Requirements**

* Infer input and output dimensions dynamically from the dataset:

  ```python
  input_size = dataset['X'].shape[1]
  num_classes = len(np.unique(dataset['y']))
  ```

---

### **Optimizer Logic**

If `learning_rate` is specified in `cfg`, use:

* `'constant'`:

  * Use SGD with `lr=eta0` (supported in all frameworks)
* `'invscaling'`:

  * Use SGD with `lr=eta0` and `momentum=power_t` (if supported, otherwise fall back gracefully)
* `'adaptive'`:

  * Use Adam or equivalent with `lr=eta0`

- Only use valid parameters for each optimizer. Do **not** use unsupported arguments (e.g., `eta0` in PyTorch ASGD or `AdaptiveASGD`).

---

### **Supporting Code Provided:**

* ConfigSpace definition: `from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause


def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    model_type = Categorical('model_type', ['LSTM', 'GRU', 'RNN', 'MLP'], default='LSTM')
    learning_rate = Float('learning_rate', (1e-5, 1e-2), default=1e-3, log=True)
    batch_size = Integer('batch_size', (8, 64), default=32, log=True)
    num_layers = Integer('num_layers', (1, 3), default=2)
    hidden_size = Integer('hidden_size', (32, 256), default=128, log=True)
    dropout = Float('dropout', (0.0, 0.5), default=0.2)
    optimizer = Categorical('optimizer', ['Adam', 'SGD'], default='Adam')

    cs.add([model_type, learning_rate, batch_size, num_layers, hidden_size, dropout, optimizer])

    # Add forbidden clauses
    # Example: if optimizer is SGD, dropout cannot be greater than 0.3
    forbidden_optimizer_dropout = ForbiddenAndConjunction(
        ForbiddenEqualsClause(optimizer, 'SGD'),
        ForbiddenEqualsClause(dropout, 0.5)
    )
    cs.add_forbidden_clause(forbidden_optimizer_dropout)

    return cs
`
* SMAC scenario: `from smac import Scenario
from ConfigSpace import ConfigurationSpace

def generate_scenario(cs):
    scenario = Scenario(
        configspace=cs,
        output_directory="./automl_results",
        deterministic=False,
        min_budget=1,
        max_budget=100,
        n_workers=4
    )
    return scenario
`
* Dataset description: `This is a time series dataset.
Number of samples: 289
Time index type: <class 'pandas.core.indexes.range.RangeIndex'>
Time range: 0 to 288
Features:
- 0

### Time Series Handling Requirements
- Assume `dataset['X']` is a 3D array or tensor with shape `(num_samples, sequence_length, num_features)`.
- If `dataset['X']` is 2D, raise a `ValueError` if the model is RNN-based (`LSTM`, `GRU`, `RNN`).
- Do **not** flatten the input when using RNN-based models.
- Use `batch_first=True` in all recurrent models to maintain `(batch, seq_len, features)` format.
- Dynamically infer sequence length as `X.shape[1]` and feature dimension as `X.shape[2]`.
- If `X.ndim != 3` and a sequential model is selected, raise a clear error with shape info.
- Example input validation check:
  ```python
  if model_type in ['LSTM', 'GRU', 'RNN'] and X_tensor.ndim != 3:
      raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")
  ```
- Time index or datetime values can be logged but should not be used in the model unless specified.
`

---

### **Additional Instructions**

* The code must not hardcode dataset dimensions like `784` or class count `10`.
* The function must be runnable and not assume unavailable classes or modules.
* You must only output the `def train(...)` function and nothing else.

--------------------------------------------------------------------------------
[2025-05-20 12:05:45] [Metadata: {'component': 'train_function', 'action': 'run'}] Running train_function code:
--------------------------------------------------------------------------------
[2025-05-20 12:05:46] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#1): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-05-20 12:05:46] [Metadata: {'component': 'train_function', 'action': 'fix'}] The following python code is generated by an LLM which now has run into the following errors:
expected sequence of length 289 at dim 0 (got 1)

Your task is to fix the issues in the code. The requirements below MUST be respected:

1. Do not change the function name or its purpose.
2. Fix all syntax and runtime errors.
3. Use only valid parameters.
4. Do not change the logic unless necessary for fixing errors.
5. Return ONLY the corrected function code — no explanations, comments, or extra output.
6. The function must be syntactically and semantically correct, and executable with `exec`.

Buggy code:
```python
from typing import Any
from ConfigSpace import Configuration
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): Configuration object specifying hyperparameters.
        dataset (Any): Dictionary containing 'X' (features) and 'y' (labels).

    Returns:
        float: Average training loss over 10 epochs.
    """

    X = dataset['X']
    y = dataset['y']

    model_type = cfg.get('model_type')
    learning_rate = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    optimizer_name = cfg.get('optimizer')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)
    y_tensor = torch.tensor(y, dtype=torch.long).to(device)

    input_size = X_tensor.shape[-1]  #infer input size
    num_classes = len(torch.unique(y_tensor))
    if len(X_tensor.shape) == 3:
        sequence_length = X_tensor.shape[1]
        input_size = X_tensor.shape[2]

    if model_type in ['LSTM', 'GRU', 'RNN'] and X_tensor.ndim != 3:
        raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")

    # Define the model
    if model_type == 'MLP':
        model = nn.Sequential(
            nn.Linear(input_size, cfg.get('hidden_size')),
            nn.ReLU(),
            nn.Dropout(cfg.get('dropout')),
            nn.Linear(cfg.get('hidden_size'), num_classes)
        ).to(device)
    elif model_type == 'LSTM':
        model = nn.LSTM(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                          num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                          batch_first=True).to(device)
        # Modify output layer to handle LSTM output
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    elif model_type == 'GRU':
        model = nn.GRU(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                         num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                         batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    elif model_type == 'RNN':
        model = nn.RNN(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                         num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                         batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Define the optimizer
    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Unsupported optimizer: {optimizer_name}")

    # Define the loss function
    criterion = nn.CrossEntropyLoss()

    # Create DataLoader
    dataset = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Training loop
    num_epochs = 10
    total_loss = 0.0
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for inputs, labels in dataloader:

            if model_type in ['LSTM', 'GRU', 'RNN']:
                # Handle RNN inputs
                outputs = model(inputs)[0] if isinstance(model[0], (nn.LSTM, nn.GRU, nn.RNN)) else model(inputs)
            else:
                outputs = model(inputs)

            loss = criterion(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_epoch_loss = epoch_loss / len(dataloader)
        total_loss += avg_epoch_loss

    avg_loss = total_loss / num_epochs

    return float(avg_loss)
```

--------------------------------------------------------------------------------
[2025-05-20 12:05:52] [Metadata: {'component': 'train_function', 'action': 'run'}] Running train_function code:
--------------------------------------------------------------------------------
[2025-05-20 12:05:52] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#2): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-05-20 12:05:52] [Metadata: {'component': 'train_function', 'action': 'fix'}] The following python code is generated by an LLM which now has run into the following errors:
expected sequence of length 289 at dim 0 (got 1)
expected sequence of length 289 at dim 0 (got 1)

Your task is to fix the issues in the code. The requirements below MUST be respected:

1. Do not change the function name or its purpose.
2. Fix all syntax and runtime errors.
3. Use only valid parameters.
4. Do not change the logic unless necessary for fixing errors.
5. Return ONLY the corrected function code — no explanations, comments, or extra output.
6. The function must be syntactically and semantically correct, and executable with `exec`.

Buggy code:
```python
from typing import Any
from ConfigSpace import Configuration
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): Configuration object specifying hyperparameters.
        dataset (Any): Dictionary containing 'X' (features) and 'y' (labels).

    Returns:
        float: Average training loss over 10 epochs.
    """

    X = dataset['X']
    y = dataset['y']

    model_type = cfg.get('model_type')
    learning_rate = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    optimizer_name = cfg.get('optimizer')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)
    y_tensor = torch.tensor(y, dtype=torch.long).to(device)

    input_size = X_tensor.shape[-1]  #infer input size
    num_classes = len(torch.unique(y_tensor))
    if len(X_tensor.shape) == 3:
        sequence_length = X_tensor.shape[1]
        input_size = X_tensor.shape[2]

    if model_type in ['LSTM', 'GRU', 'RNN'] and X_tensor.ndim != 3:
        raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")

    # Define the model
    if model_type == 'MLP':
        model = nn.Sequential(
            nn.Linear(input_size, cfg.get('hidden_size')),
            nn.ReLU(),
            nn.Dropout(cfg.get('dropout')),
            nn.Linear(cfg.get('hidden_size'), num_classes)
        ).to(device)
    elif model_type == 'LSTM':
        model = nn.LSTM(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                          num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                          batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    elif model_type == 'GRU':
        model = nn.GRU(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                         num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                         batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    elif model_type == 'RNN':
        model = nn.RNN(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                         num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                         batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Define the optimizer
    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Unsupported optimizer: {optimizer_name}")

    # Define the loss function
    criterion = nn.CrossEntropyLoss()

    # Create DataLoader
    dataset = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Training loop
    num_epochs = 10
    total_loss = 0.0
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for inputs, labels in dataloader:

            if model_type in ['LSTM', 'GRU', 'RNN']:
                outputs = model(inputs)[0] if isinstance(model[0], (nn.LSTM, nn.GRU, nn.RNN)) else model(inputs)
                if isinstance(model, nn.Sequential):
                    outputs = model[1](outputs[:, -1, :])
            else:
                outputs = model(inputs)

            loss = criterion(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_epoch_loss = epoch_loss / len(dataloader)
        total_loss += avg_epoch_loss

    avg_loss = total_loss / num_epochs

    return float(avg_loss)
```

--------------------------------------------------------------------------------
[2025-05-20 12:05:59] [Metadata: {'component': 'train_function', 'action': 'run'}] Running train_function code:
--------------------------------------------------------------------------------
[2025-05-20 12:05:59] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#3): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-05-20 12:05:59] [Metadata: {'component': 'train_function', 'action': 'fix'}] The following python code is generated by an LLM which now has run into the following errors:
expected sequence of length 289 at dim 0 (got 1)
expected sequence of length 289 at dim 0 (got 1)
expected sequence of length 289 at dim 0 (got 1)

Your task is to fix the issues in the code. The requirements below MUST be respected:

1. Do not change the function name or its purpose.
2. Fix all syntax and runtime errors.
3. Use only valid parameters.
4. Do not change the logic unless necessary for fixing errors.
5. Return ONLY the corrected function code — no explanations, comments, or extra output.
6. The function must be syntactically and semantically correct, and executable with `exec`.

Buggy code:
```python
from typing import Any
from ConfigSpace import Configuration
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): Configuration object specifying hyperparameters.
        dataset (Any): Dictionary containing 'X' (features) and 'y' (labels).

    Returns:
        float: Average training loss over 10 epochs.
    """

    X = dataset['X']
    y = dataset['y']

    model_type = cfg.get('model_type')
    learning_rate = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    optimizer_name = cfg.get('optimizer')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)
    y_tensor = torch.tensor(y, dtype=torch.long).to(device)

    input_size = X_tensor.shape[-1]  #infer input size
    num_classes = len(torch.unique(y_tensor))
    if len(X_tensor.shape) == 3:
        sequence_length = X_tensor.shape[1]
        input_size = X_tensor.shape[2]

    if model_type in ['LSTM', 'GRU', 'RNN'] and X_tensor.ndim != 3:
        raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")

    # Define the model
    if model_type == 'MLP':
        model = nn.Sequential(
            nn.Linear(input_size, cfg.get('hidden_size')),
            nn.ReLU(),
            nn.Dropout(cfg.get('dropout')),
            nn.Linear(cfg.get('hidden_size'), num_classes)
        ).to(device)
    elif model_type == 'LSTM':
        model = nn.LSTM(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                          num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                          batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    elif model_type == 'GRU':
        model = nn.GRU(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                         num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                         batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    elif model_type == 'RNN':
        model = nn.RNN(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                         num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                         batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Define the optimizer
    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Unsupported optimizer: {optimizer_name}")

    # Define the loss function
    criterion = nn.CrossEntropyLoss()

    # Create DataLoader
    dataset = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Training loop
    num_epochs = 10
    total_loss = 0.0
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for inputs, labels in dataloader:
            if model_type in ['LSTM', 'GRU', 'RNN']:
                outputs, _ = model[0](inputs) if isinstance(model[0], (nn.LSTM, nn.GRU, nn.RNN)) else (model[0](inputs), None)
                outputs = model[1](outputs[:, -1, :])
            else:
                outputs = model(inputs)
            loss = criterion(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_epoch_loss = epoch_loss / len(dataloader)
        total_loss += avg_epoch_loss

    avg_loss = total_loss / num_epochs

    return float(avg_loss)
```

--------------------------------------------------------------------------------
[2025-05-20 12:06:05] [Metadata: {'component': 'train_function', 'action': 'run'}] Running train_function code:
--------------------------------------------------------------------------------
[2025-05-20 12:06:05] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#4): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-05-20 12:06:05] [Metadata: {'component': 'train_function', 'action': 'fix'}] The following python code is generated by an LLM which now has run into the following errors:
expected sequence of length 289 at dim 0 (got 1)
expected sequence of length 289 at dim 0 (got 1)
expected sequence of length 289 at dim 0 (got 1)
expected sequence of length 289 at dim 0 (got 1)

Your task is to fix the issues in the code. The requirements below MUST be respected:

1. Do not change the function name or its purpose.
2. Fix all syntax and runtime errors.
3. Use only valid parameters.
4. Do not change the logic unless necessary for fixing errors.
5. Return ONLY the corrected function code — no explanations, comments, or extra output.
6. The function must be syntactically and semantically correct, and executable with `exec`.

Buggy code:
```python
from typing import Any
from ConfigSpace import Configuration
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): Configuration object specifying hyperparameters.
        dataset (Any): Dictionary containing 'X' (features) and 'y' (labels).

    Returns:
        float: Average training loss over 10 epochs.
    """

    X = dataset['X']
    y = dataset['y']

    model_type = cfg.get('model_type')
    learning_rate = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    optimizer_name = cfg.get('optimizer')
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)
    y_tensor = torch.tensor(y, dtype=torch.long).to(device)

    input_size = X_tensor.shape[-1]  #infer input size
    num_classes = len(torch.unique(y_tensor))
    if len(X_tensor.shape) == 3:
        sequence_length = X_tensor.shape[1]
        input_size = X_tensor.shape[2]

    if model_type in ['LSTM', 'GRU', 'RNN'] and X_tensor.ndim != 3:
        raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")

    # Define the model
    if model_type == 'MLP':
        model = nn.Sequential(
            nn.Linear(input_size, cfg.get('hidden_size')),
            nn.ReLU(),
            nn.Dropout(cfg.get('dropout')),
            nn.Linear(cfg.get('hidden_size'), num_classes)
        ).to(device)
    elif model_type == 'LSTM':
        model = nn.LSTM(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                          num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                          batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    elif model_type == 'GRU':
        model = nn.GRU(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                         num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                         batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    elif model_type == 'RNN':
        model = nn.RNN(input_size=input_size, hidden_size=cfg.get('hidden_size'),
                         num_layers=cfg.get('num_layers'), dropout=cfg.get('dropout'),
                         batch_first=True).to(device)
        model = nn.Sequential(model, nn.Linear(cfg.get('hidden_size'), num_classes)).to(device)
    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Define the optimizer
    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Unsupported optimizer: {optimizer_name}")

    # Define the loss function
    criterion = nn.CrossEntropyLoss()

    # Create DataLoader
    dataset = TensorDataset(X_tensor, y_tensor)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Training loop
    num_epochs = 10
    total_loss = 0.0
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for inputs, labels in dataloader:
            if model_type in ['LSTM', 'GRU', 'RNN']:
                outputs = model[0](inputs)[0] if isinstance(model[0], (nn.LSTM, nn.GRU, nn.RNN)) else model[0](inputs)
                outputs = model[1](outputs[:, -1, :])
            else:
                outputs = model(inputs)
            loss = criterion(outputs, labels)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_epoch_loss = epoch_loss / len(dataloader)
        total_loss += avg_epoch_loss

    avg_loss = total_loss / num_epochs

    return float(avg_loss)
```

--------------------------------------------------------------------------------
[2025-05-20 12:06:12] [Metadata: {'component': 'train_function', 'action': 'run'}] Running train_function code:
--------------------------------------------------------------------------------
[2025-05-20 12:06:12] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#5): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-05-20 12:06:12] [Metadata: {'component': 'train_function', 'action': 'retry'}] Retry limit reached for train_function. Fetching fresh code from LLM.
--------------------------------------------------------------------------------
[2025-05-20 12:06:21] [Metadata: {'component': 'train_function', 'action': 'run'}] Running train_function code:
--------------------------------------------------------------------------------
[2025-05-20 12:06:21] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#1): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-05-20 12:06:21] [Metadata: {'component': 'train_function', 'action': 'fix'}] The following python code is generated by an LLM which now has run into the following errors:
expected sequence of length 289 at dim 0 (got 1)
expected sequence of length 289 at dim 0 (got 1)
expected sequence of length 289 at dim 0 (got 1)
expected sequence of length 289 at dim 0 (got 1)
expected sequence of length 289 at dim 0 (got 1)

Your task is to fix the issues in the code. The requirements below MUST be respected:

1. Do not change the function name or its purpose.
2. Fix all syntax and runtime errors.
3. Use only valid parameters.
4. Do not change the logic unless necessary for fixing errors.
5. Return ONLY the corrected function code — no explanations, comments, or extra output.
6. The function must be syntactically and semantically correct, and executable with `exec`.

Buggy code:
```python
from typing import Any

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from ConfigSpace import Configuration


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing the training data, with keys 'X' and 'y'.

    Returns:
        float: The average training loss over 10 epochs.
    """
    X = dataset['X']
    y = dataset['y']

    # Convert data to PyTorch tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.long)

    model_type = cfg.get('model_type')
    learning_rate = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    num_layers = cfg.get('num_layers')
    hidden_size = cfg.get('hidden_size')
    dropout = cfg.get('dropout')
    optimizer_name = cfg.get('optimizer')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        num_samples, seq_len, num_features = X_tensor.shape
    elif X_tensor.ndim == 2:
        num_samples, num_features = X_tensor.shape
    else:
        raise ValueError(f"Unexpected input dimension: {X_tensor.ndim}")
    num_classes = len(np.unique(y))

    # Define the model
    class LSTMModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(LSTMModel, self).__init__()
            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.lstm(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class GRUModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(GRUModel, self).__init__()
            self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.gru(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class RNNModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(RNNModel, self).__init__()
            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.rnn(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class MLPModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(MLPModel, self).__init__()
            self.layers = nn.ModuleList()
            self.layers.append(nn.Linear(input_size, hidden_size))
            self.layers.append(nn.ReLU())
            self.layers.append(nn.Dropout(dropout))
            for _ in range(num_layers - 1):
                self.layers.append(nn.Linear(hidden_size, hidden_size))
                self.layers.append(nn.ReLU())
                self.layers.append(nn.Dropout(dropout))
            self.layers.append(nn.Linear(hidden_size, num_classes))

        def forward(self, x):
            for layer in self.layers:
                x = layer(x)
            return x

    if model_type in ['LSTM', 'GRU', 'RNN']:
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")
        input_size = X_tensor.shape[2]
        if model_type == 'LSTM':
            model = LSTMModel(input_size, hidden_size, num_layers, num_classes, dropout)
        elif model_type == 'GRU':
            model = GRUModel(input_size, hidden_size, num_layers, num_classes, dropout)
        else:  # model_type == 'RNN'
            model = RNNModel(input_size, hidden_size, num_layers, num_classes, dropout)

    elif model_type == 'MLP':
        if X_tensor.ndim != 2:
            raise ValueError(f"Expected 2D input (batch, features) for MLP, got {X_tensor.shape}")
        input_size = X_tensor.shape[1]
        model = MLPModel(input_size, hidden_size, num_layers, num_classes, dropout)

    else:
        raise ValueError(f"Invalid model type: {model_type}")

    # Define the loss function and optimizer
    criterion = nn.CrossEntropyLoss()

    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Invalid optimizer: {optimizer_name}")

    # Training loop
    num_epochs = 10
    total_loss = 0.0

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0
        
        # Mini-batch training
        for i in range(0, len(X_tensor), batch_size):
            inputs = X_tensor[i:i+batch_size]
            labels = y_tensor[i:i+batch_size]

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        epoch_loss = running_loss / (len(X_tensor) // batch_size)
        total_loss += epoch_loss
    
    avg_loss = total_loss / num_epochs
    return float(avg_loss)
```

--------------------------------------------------------------------------------
[2025-05-20 12:06:28] [Metadata: {'component': 'train_function', 'action': 'run'}] Running train_function code:
--------------------------------------------------------------------------------
[2025-05-20 12:06:28] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#2): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-05-20 12:06:28] [Metadata: {'component': 'train_function', 'action': 'fix'}] The following python code is generated by an LLM which now has run into the following errors:
expected sequence of length 289 at dim 0 (got 1)
expected sequence of length 289 at dim 0 (got 1)
expected sequence of length 289 at dim 0 (got 1)
expected sequence of length 289 at dim 0 (got 1)
expected sequence of length 289 at dim 0 (got 1)

Your task is to fix the issues in the code. The requirements below MUST be respected:

1. Do not change the function name or its purpose.
2. Fix all syntax and runtime errors.
3. Use only valid parameters.
4. Do not change the logic unless necessary for fixing errors.
5. Return ONLY the corrected function code — no explanations, comments, or extra output.
6. The function must be syntactically and semantically correct, and executable with `exec`.

Buggy code:
```python
from typing import Any

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from ConfigSpace import Configuration


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing the training data, with keys 'X' and 'y'.

    Returns:
        float: The average training loss over 10 epochs.
    """
    X = dataset['X']
    y = dataset['y']

    # Convert data to PyTorch tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.long)

    model_type = cfg.get('model_type')
    learning_rate = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    num_layers = cfg.get('num_layers')
    hidden_size = cfg.get('hidden_size')
    dropout = cfg.get('dropout')
    optimizer_name = cfg.get('optimizer')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        num_samples, seq_len, num_features = X_tensor.shape
    elif X_tensor.ndim == 2:
        num_samples, num_features = X_tensor.shape
    else:
        raise ValueError(f"Unexpected input dimension: {X_tensor.ndim}")
    num_classes = len(np.unique(y))

    # Define the model
    class LSTMModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(LSTMModel, self).__init__()
            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.lstm(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class GRUModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(GRUModel, self).__init__()
            self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.gru(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class RNNModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(RNNModel, self).__init__()
            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.rnn(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class MLPModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(MLPModel, self).__init__()
            self.layers = nn.ModuleList()
            self.layers.append(nn.Linear(input_size, hidden_size))
            self.layers.append(nn.ReLU())
            self.layers.append(nn.Dropout(dropout))
            for _ in range(num_layers - 1):
                self.layers.append(nn.Linear(hidden_size, hidden_size))
                self.layers.append(nn.ReLU())
                self.layers.append(nn.Dropout(dropout))
            self.layers.append(nn.Linear(hidden_size, num_classes))

        def forward(self, x):
            for layer in self.layers:
                x = layer(x)
            return x

    if model_type in ['LSTM', 'GRU', 'RNN']:
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")
        input_size = X_tensor.shape[2]
        if model_type == 'LSTM':
            model = LSTMModel(input_size, hidden_size, num_layers, num_classes, dropout)
        elif model_type == 'GRU':
            model = GRUModel(input_size, hidden_size, num_layers, num_classes, dropout)
        else:  # model_type == 'RNN'
            model = RNNModel(input_size, hidden_size, num_layers, num_classes, dropout)

    elif model_type == 'MLP':
        if X_tensor.ndim != 2:
            raise ValueError(f"Expected 2D input (batch, features) for MLP, got {X_tensor.shape}")
        input_size = X_tensor.shape[1]
        model = MLPModel(input_size, hidden_size, num_layers, num_classes, dropout)

    else:
        raise ValueError(f"Invalid model type: {model_type}")

    # Define the loss function and optimizer
    criterion = nn.CrossEntropyLoss()

    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Invalid optimizer: {optimizer_name}")

    # Training loop
    num_epochs = 10
    total_loss = 0.0

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        # Mini-batch training
        for i in range(0, len(X_tensor), batch_size):
            inputs = X_tensor[i:i+batch_size]
            labels = y_tensor[i:i+batch_size]

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        epoch_loss = running_loss / (len(X_tensor) / batch_size)
        total_loss += epoch_loss

    avg_loss = total_loss / num_epochs
    return float(avg_loss)
```

--------------------------------------------------------------------------------
[2025-05-20 12:06:36] [Metadata: {'component': 'train_function', 'action': 'run'}] Running train_function code:
--------------------------------------------------------------------------------
[2025-05-20 12:06:36] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#3): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-05-20 12:06:36] [Metadata: {'component': 'train_function', 'action': 'fix'}] The following python code is generated by an LLM which now has run into the following errors:
expected sequence of length 289 at dim 0 (got 1)
expected sequence of length 289 at dim 0 (got 1)
expected sequence of length 289 at dim 0 (got 1)
expected sequence of length 289 at dim 0 (got 1)
expected sequence of length 289 at dim 0 (got 1)

Your task is to fix the issues in the code. The requirements below MUST be respected:

1. Do not change the function name or its purpose.
2. Fix all syntax and runtime errors.
3. Use only valid parameters.
4. Do not change the logic unless necessary for fixing errors.
5. Return ONLY the corrected function code — no explanations, comments, or extra output.
6. The function must be syntactically and semantically correct, and executable with `exec`.

Buggy code:
```python
from typing import Any

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from ConfigSpace import Configuration


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing the training data, with keys 'X' and 'y'.

    Returns:
        float: The average training loss over 10 epochs.
    """
    X = dataset['X']
    y = dataset['y']

    # Convert data to PyTorch tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.long)

    model_type = cfg.get('model_type')
    learning_rate = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    num_layers = cfg.get('num_layers')
    hidden_size = cfg.get('hidden_size')
    dropout = cfg.get('dropout')
    optimizer_name = cfg.get('optimizer')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        num_samples, seq_len, num_features = X_tensor.shape
    elif X_tensor.ndim == 2:
        num_samples, num_features = X_tensor.shape
    else:
        raise ValueError(f"Unexpected input dimension: {X_tensor.ndim}")
    num_classes = len(np.unique(y))

    # Define the model
    class LSTMModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(LSTMModel, self).__init__()
            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.lstm(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class GRUModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(GRUModel, self).__init__()
            self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.gru(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class RNNModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(RNNModel, self).__init__()
            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.rnn(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class MLPModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(MLPModel, self).__init__()
            self.layers = nn.ModuleList()
            self.layers.append(nn.Linear(input_size, hidden_size))
            self.layers.append(nn.ReLU())
            self.layers.append(nn.Dropout(dropout))
            for _ in range(num_layers - 1):
                self.layers.append(nn.Linear(hidden_size, hidden_size))
                self.layers.append(nn.ReLU())
                self.layers.append(nn.Dropout(dropout))
            self.layers.append(nn.Linear(hidden_size, num_classes))

        def forward(self, x):
            for layer in self.layers:
                x = layer(x)
            return x

    if model_type in ['LSTM', 'GRU', 'RNN']:
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")
        input_size = X_tensor.shape[2]
        if model_type == 'LSTM':
            model = LSTMModel(input_size, hidden_size, num_layers, num_classes, dropout)
        elif model_type == 'GRU':
            model = GRUModel(input_size, hidden_size, num_layers, num_classes, dropout)
        else:  # model_type == 'RNN'
            model = RNNModel(input_size, hidden_size, num_layers, num_classes, dropout)

    elif model_type == 'MLP':
        if X_tensor.ndim != 2:
            raise ValueError(f"Expected 2D input (batch, features) for MLP, got {X_tensor.shape}")
        input_size = X_tensor.shape[1]
        model = MLPModel(input_size, hidden_size, num_layers, num_classes, dropout)

    else:
        raise ValueError(f"Invalid model type: {model_type}")

    # Define the loss function and optimizer
    criterion = nn.CrossEntropyLoss()

    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Invalid optimizer: {optimizer_name}")

    # Training loop
    num_epochs = 10
    total_loss = 0.0

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        # Mini-batch training
        for i in range(0, len(X_tensor), batch_size):
            inputs = X_tensor[i:i+batch_size]
            labels = y_tensor[i:i+batch_size]

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        epoch_loss = running_loss / (len(X_tensor) / batch_size)
        total_loss += epoch_loss

    avg_loss = total_loss / num_epochs
    return float(avg_loss)
```

--------------------------------------------------------------------------------
[2025-05-20 12:06:44] [Metadata: {'component': 'train_function', 'action': 'run'}] Running train_function code:
--------------------------------------------------------------------------------
[2025-05-20 12:06:44] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#4): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-05-20 12:06:44] [Metadata: {'component': 'train_function', 'action': 'fix'}] The following python code is generated by an LLM which now has run into the following errors:
expected sequence of length 289 at dim 0 (got 1)
expected sequence of length 289 at dim 0 (got 1)
expected sequence of length 289 at dim 0 (got 1)
expected sequence of length 289 at dim 0 (got 1)
expected sequence of length 289 at dim 0 (got 1)

Your task is to fix the issues in the code. The requirements below MUST be respected:

1. Do not change the function name or its purpose.
2. Fix all syntax and runtime errors.
3. Use only valid parameters.
4. Do not change the logic unless necessary for fixing errors.
5. Return ONLY the corrected function code — no explanations, comments, or extra output.
6. The function must be syntactically and semantically correct, and executable with `exec`.

Buggy code:
```python
from typing import Any

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from ConfigSpace import Configuration


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A configuration object containing hyperparameters.
        dataset (Any): A dictionary containing the training data, with keys 'X' and 'y'.

    Returns:
        float: The average training loss over 10 epochs.
    """
    X = dataset['X']
    y = dataset['y']

    # Convert data to PyTorch tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.long)

    model_type = cfg.get('model_type')
    learning_rate = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    num_layers = cfg.get('num_layers')
    hidden_size = cfg.get('hidden_size')
    dropout = cfg.get('dropout')
    optimizer_name = cfg.get('optimizer')

    # Infer input and output dimensions dynamically
    if X_tensor.ndim == 3:
        num_samples, seq_len, num_features = X_tensor.shape
    elif X_tensor.ndim == 2:
        num_samples, num_features = X_tensor.shape
    else:
        raise ValueError(f"Unexpected input dimension: {X_tensor.ndim}")
    num_classes = len(np.unique(y))

    # Define the model
    class LSTMModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(LSTMModel, self).__init__()
            self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.lstm(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class GRUModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(GRUModel, self).__init__()
            self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.gru(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class RNNModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(RNNModel, self).__init__()
            self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)
            self.fc = nn.Linear(hidden_size, num_classes)

        def forward(self, x):
            out, _ = self.rnn(x)
            out = self.fc(out[:, -1, :])  # Only take the last time step's output
            return out

    class MLPModel(nn.Module):
        def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout):
            super(MLPModel, self).__init__()
            self.layers = nn.ModuleList()
            self.layers.append(nn.Linear(input_size, hidden_size))
            self.layers.append(nn.ReLU())
            self.layers.append(nn.Dropout(dropout))
            for _ in range(num_layers - 1):
                self.layers.append(nn.Linear(hidden_size, hidden_size))
                self.layers.append(nn.ReLU())
                self.layers.append(nn.Dropout(dropout))
            self.layers.append(nn.Linear(hidden_size, num_classes))

        def forward(self, x):
            for layer in self.layers:
                x = layer(x)
            return x

    if model_type in ['LSTM', 'GRU', 'RNN']:
        if X_tensor.ndim != 3:
            raise ValueError(f"Expected 3D input (batch, seq_len, features) for {model_type}, got {X_tensor.shape}")
        input_size = X_tensor.shape[2]
        if model_type == 'LSTM':
            model = LSTMModel(input_size, hidden_size, num_layers, num_classes, dropout)
        elif model_type == 'GRU':
            model = GRUModel(input_size, hidden_size, num_layers, num_classes, dropout)
        else:  # model_type == 'RNN'
            model = RNNModel(input_size, hidden_size, num_layers, num_classes, dropout)

    elif model_type == 'MLP':
        if X_tensor.ndim != 2:
            raise ValueError(f"Expected 2D input (batch, features) for MLP, got {X_tensor.shape}")
        input_size = X_tensor.shape[1]
        model = MLPModel(input_size, hidden_size, num_layers, num_classes, dropout)

    else:
        raise ValueError(f"Invalid model type: {model_type}")

    # Define the loss function and optimizer
    criterion = nn.CrossEntropyLoss()

    if optimizer_name == 'Adam':
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == 'SGD':
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Invalid optimizer: {optimizer_name}")

    # Training loop
    num_epochs = 10
    total_loss = 0.0

    for epoch in range(num_epochs):
        model.train()
        running_loss = 0.0

        # Mini-batch training
        for i in range(0, len(X_tensor), batch_size):
            inputs = X_tensor[i:i+batch_size]
            labels = y_tensor[i:i+batch_size]

            if len(inputs) == 0:
                continue

            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

        epoch_loss = running_loss / (len(X_tensor) / batch_size)
        total_loss += epoch_loss

    avg_loss = total_loss / num_epochs
    return float(avg_loss)
```

--------------------------------------------------------------------------------
[2025-05-20 12:06:51] [Metadata: {'component': 'train_function', 'action': 'run'}] Running train_function code:
--------------------------------------------------------------------------------
[2025-05-20 12:06:51] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#5): expected sequence of length 289 at dim 0 (got 1)
--------------------------------------------------------------------------------
[2025-05-20 12:06:51] [Metadata: {'component': 'train_function', 'action': 'retry'}] Retry limit reached for train_function. Fetching fresh code from LLM.
--------------------------------------------------------------------------------
[2025-05-20 12:06:58] [Metadata: {'component': 'train_function', 'action': 'run'}] Running train_function code:
--------------------------------------------------------------------------------
