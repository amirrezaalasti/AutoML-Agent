[2025-06-15 16:34:23] [Metadata: {'component': 'config'}] ```python
from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, EqualsCondition

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace for hyperparameter optimization of a CNN
    for image classification.
    """
    cs = ConfigurationSpace()

    # --- Learning Rate ---
    learning_rate = UniformFloatHyperparameter(
        "learning_rate",
        lower=1e-5,
        upper=1e-2,
        default_value=1e-3,
        log=True,
        q=None,
        doc="Initial learning rate for the optimizer"
    )
    cs.add_hyperparameter(learning_rate)

    # --- Optimizer ---
    optimizer = CategoricalHyperparameter(
        "optimizer",
        choices=["Adam", "SGD", "RMSprop"],
        default_value="Adam",
        doc="The optimization algorithm to use"
    )
    cs.add_hyperparameter(optimizer)

    # --- Batch Size ---
    batch_size = UniformIntegerHyperparameter(
        "batch_size",
        lower=32,
        upper=256,
        default_value=64,
        log=True,
        q=None,
        doc="Number of samples per batch"
    )
    cs.add_hyperparameter(batch_size)

    # --- Number of Epochs ---
    epochs = UniformIntegerHyperparameter(
        "epochs",
        lower=5,
        upper=50,
        default_value=10,
        log=False,
        q=None,
        doc="Number of training epochs"
    )
    cs.add_hyperparameter(epochs)

    # --- Momentum (only for SGD) ---
    momentum = UniformFloatHyperparameter(
        "momentum",
        lower=0.0,
        upper=0.99,
        default_value=0.9,
        q=None,
        doc="Momentum for SGD optimizer",
    )
    cs.add_hyperparameter(momentum)
    condition_momentum = EqualsCondition(momentum, optimizer, "SGD")
    cs.add_condition(condition_momentum)

    # --- Weight Decay (L2 Regularization) ---
    weight_decay = UniformFloatHyperparameter(
        "weight_decay",
        lower=1e-6,
        upper=1e-3,
        default_value=1e-4,
        log=True,
        q=None,
        doc="L2 regularization strength"
    )
    cs.add_hyperparameter(weight_decay)

    # --- Number of Convolutional Layers ---
    num_conv_layers = UniformIntegerHyperparameter(
        "num_conv_layers",
        lower=1,
        upper=5,
        default_value=3,
        log=False,
        q=None,
        doc="Number of convolutional layers"
    )
    cs.add_hyperparameter(num_conv_layers)

    # --- Number of Filters (Conv Layers) ---
    num_filters_l1 = UniformIntegerHyperparameter(
        "num_filters_l1",
        lower=32,
        upper=128,
        default_value=64,
        log=True,
        q=None,
        doc="Number of filters in the first convolutional layer"
    )
    cs.add_hyperparameter(num_filters_l1)

    num_filters_mult = UniformFloatHyperparameter(
        "num_filters_mult",
        lower=1.0,
        upper=2.0,
        default_value=1.5,
        log=False,
        q=None,
        doc="Multiplier for the number of filters in subsequent convolutional layers"
    )
    cs.add_hyperparameter(num_filters_mult)


    # --- Dropout Rate ---
    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate",
        lower=0.0,
        upper=0.7,
        default_value=0.25,
        q=None,
        doc="Dropout rate for regularization"
    )
    cs.add_hyperparameter(dropout_rate)

    # --- Number of Dense Layers ---
    num_dense_layers = UniformIntegerHyperparameter(
        "num_dense_layers",
        lower=1,
        upper=3,
        default_value=2,
        log=False,
        q=None,
        doc="Number of dense (fully connected) layers"
    )
    cs.add_hyperparameter(num_dense_layers)

    # --- Number of Units (Dense Layers) ---
    num_units_l1 = UniformIntegerHyperparameter(
        "num_units_l1",
        lower=64,
        upper=512,
        default_value=256,
        log=True,
        q=None,
        doc="Number of units in the first dense layer"
    )
    cs.add_hyperparameter(num_units_l1)

    num_units_mult = UniformFloatHyperparameter(
        "num_units_mult",
        lower=0.25,
        upper=1.0,
        default_value=0.5,
        log=False,
        q=None,
        doc="Multiplier for the number of units in subsequent dense layers"
    )
    cs.add_hyperparameter(num_units_mult)
    return cs
```
--------------------------------------------------------------------------------
[2025-06-15 16:34:23] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, EqualsCondition

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace for hyperparameter optimization of a CNN
    for image classification.
    """
    cs = ConfigurationSpace()

    # --- Learning Rate ---
    learning_rate = UniformFloatHyperparameter(
        "learning_rate",
        lower=1e-5,
        upper=1e-2,
        default_value=1e-3,
        log=True,
        q=None,
        doc="Initial learning rate for the optimizer"
    )
    cs.add_hyperparameter(learning_rate)

    # --- Optimizer ---
    optimizer = CategoricalHyperparameter(
        "optimizer",
        choices=["Adam", "SGD", "RMSprop"],
        default_value="Adam",
        doc="The optimization algorithm to use"
    )
    cs.add_hyperparameter(optimizer)

    # --- Batch Size ---
    batch_size = UniformIntegerHyperparameter(
        "batch_size",
        lower=32,
        upper=256,
        default_value=64,
        log=True,
        q=None,
        doc="Number of samples per batch"
    )
    cs.add_hyperparameter(batch_size)

    # --- Number of Epochs ---
    epochs = UniformIntegerHyperparameter(
        "epochs",
        lower=5,
        upper=50,
        default_value=10,
        log=False,
        q=None,
        doc="Number of training epochs"
    )
    cs.add_hyperparameter(epochs)

    # --- Momentum (only for SGD) ---
    momentum = UniformFloatHyperparameter(
        "momentum",
        lower=0.0,
        upper=0.99,
        default_value=0.9,
        q=None,
        doc="Momentum for SGD optimizer",
    )
    cs.add_hyperparameter(momentum)
    condition_momentum = EqualsCondition(momentum, optimizer, "SGD")
    cs.add_condition(condition_momentum)

    # --- Weight Decay (L2 Regularization) ---
    weight_decay = UniformFloatHyperparameter(
        "weight_decay",
        lower=1e-6,
        upper=1e-3,
        default_value=1e-4,
        log=True,
        q=None,
        doc="L2 regularization strength"
    )
    cs.add_hyperparameter(weight_decay)

    # --- Number of Convolutional Layers ---
    num_conv_layers = UniformIntegerHyperparameter(
        "num_conv_layers",
        lower=1,
        upper=5,
        default_value=3,
        log=False,
        q=None,
        doc="Number of convolutional layers"
    )
    cs.add_hyperparameter(num_conv_layers)

    # --- Number of Filters (Conv Layers) ---
    num_filters_l1 = UniformIntegerHyperparameter(
        "num_filters_l1",
        lower=32,
        upper=128,
        default_value=64,
        log=True,
        q=None,
        doc="Number of filters in the first convolutional layer"
    )
    cs.add_hyperparameter(num_filters_l1)

    num_filters_mult = UniformFloatHyperparameter(
        "num_filters_mult",
        lower=1.0,
        upper=2.0,
        default_value=1.5,
        log=False,
        q=None,
        doc="Multiplier for the number of filters in subsequent convolutional layers"
    )
    cs.add_hyperparameter(num_filters_mult)


    # --- Dropout Rate ---
    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate",
        lower=0.0,
        upper=0.7,
        default_value=0.25,
        q=None,
        doc="Dropout rate for regularization"
    )
    cs.add_hyperparameter(dropout_rate)

    # --- Number of Dense Layers ---
    num_dense_layers = UniformIntegerHyperparameter(
        "num_dense_layers",
        lower=1,
        upper=3,
        default_value=2,
        log=False,
        q=None,
        doc="Number of dense (fully connected) layers"
    )
    cs.add_hyperparameter(num_dense_layers)

    # --- Number of Units (Dense Layers) ---
    num_units_l1 = UniformIntegerHyperparameter(
        "num_units_l1",
        lower=64,
        upper=512,
        default_value=256,
        log=True,
        q=None,
        doc="Number of units in the first dense layer"
    )
    cs.add_hyperparameter(num_units_l1)

    num_units_mult = UniformFloatHyperparameter(
        "num_units_mult",
        lower=0.25,
        upper=1.0,
        default_value=0.5,
        log=False,
        q=None,
        doc="Multiplier for the number of units in subsequent dense layers"
    )
    cs.add_hyperparameter(num_units_mult)
    return cs

--------------------------------------------------------------------------------
[2025-06-15 16:34:23] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#1): UniformFloatHyperparameter.__init__() got an unexpected keyword argument 'q'
--------------------------------------------------------------------------------
[2025-06-15 16:34:29] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, EqualsCondition

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace for hyperparameter optimization of a CNN
    for image classification.
    """
    cs = ConfigurationSpace()

    # --- Learning Rate ---
    learning_rate = UniformFloatHyperparameter(
        "learning_rate",
        lower=1e-5,
        upper=1e-2,
        default_value=1e-3,
        log=True,
        doc="Initial learning rate for the optimizer"
    )
    cs.add_hyperparameter(learning_rate)

    # --- Optimizer ---
    optimizer = CategoricalHyperparameter(
        "optimizer",
        choices=["Adam", "SGD", "RMSprop"],
        default_value="Adam",
        doc="The optimization algorithm to use"
    )
    cs.add_hyperparameter(optimizer)

    # --- Batch Size ---
    batch_size = UniformIntegerHyperparameter(
        "batch_size",
        lower=32,
        upper=256,
        default_value=64,
        log=True,
        doc="Number of samples per batch"
    )
    cs.add_hyperparameter(batch_size)

    # --- Number of Epochs ---
    epochs = UniformIntegerHyperparameter(
        "epochs",
        lower=5,
        upper=50,
        default_value=10,
        log=False,
        doc="Number of training epochs"
    )
    cs.add_hyperparameter(epochs)

    # --- Momentum (only for SGD) ---
    momentum = UniformFloatHyperparameter(
        "momentum",
        lower=0.0,
        upper=0.99,
        default_value=0.9,
        doc="Momentum for SGD optimizer",
    )
    cs.add_hyperparameter(momentum)
    condition_momentum = EqualsCondition(momentum, optimizer, "SGD")
    cs.add_condition(condition_momentum)

    # --- Weight Decay (L2 Regularization) ---
    weight_decay = UniformFloatHyperparameter(
        "weight_decay",
        lower=1e-6,
        upper=1e-3,
        default_value=1e-4,
        log=True,
        doc="L2 regularization strength"
    )
    cs.add_hyperparameter(weight_decay)

    # --- Number of Convolutional Layers ---
    num_conv_layers = UniformIntegerHyperparameter(
        "num_conv_layers",
        lower=1,
        upper=5,
        default_value=3,
        log=False,
        doc="Number of convolutional layers"
    )
    cs.add_hyperparameter(num_conv_layers)

    # --- Number of Filters (Conv Layers) ---
    num_filters_l1 = UniformIntegerHyperparameter(
        "num_filters_l1",
        lower=32,
        upper=128,
        default_value=64,
        log=True,
        doc="Number of filters in the first convolutional layer"
    )
    cs.add_hyperparameter(num_filters_l1)

    num_filters_mult = UniformFloatHyperparameter(
        "num_filters_mult",
        lower=1.0,
        upper=2.0,
        default_value=1.5,
        log=False,
        doc="Multiplier for the number of filters in subsequent convolutional layers"
    )
    cs.add_hyperparameter(num_filters_mult)


    # --- Dropout Rate ---
    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate",
        lower=0.0,
        upper=0.7,
        default_value=0.25,
        doc="Dropout rate for regularization"
    )
    cs.add_hyperparameter(dropout_rate)

    # --- Number of Dense Layers ---
    num_dense_layers = UniformIntegerHyperparameter(
        "num_dense_layers",
        lower=1,
        upper=3,
        default_value=2,
        log=False,
        doc="Number of dense (fully connected) layers"
    )
    cs.add_hyperparameter(num_dense_layers)

    # --- Number of Units (Dense Layers) ---
    num_units_l1 = UniformIntegerHyperparameter(
        "num_units_l1",
        lower=64,
        upper=512,
        default_value=256,
        log=True,
        doc="Number of units in the first dense layer"
    )
    cs.add_hyperparameter(num_units_l1)

    num_units_mult = UniformFloatHyperparameter(
        "num_units_mult",
        lower=0.25,
        upper=1.0,
        default_value=0.5,
        log=False,
        doc="Multiplier for the number of units in subsequent dense layers"
    )
    cs.add_hyperparameter(num_units_mult)
    return cs
```
--------------------------------------------------------------------------------
[2025-06-15 16:34:29] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, EqualsCondition

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace for hyperparameter optimization of a CNN
    for image classification.
    """
    cs = ConfigurationSpace()

    # --- Learning Rate ---
    learning_rate = UniformFloatHyperparameter(
        "learning_rate",
        lower=1e-5,
        upper=1e-2,
        default_value=1e-3,
        log=True,
        doc="Initial learning rate for the optimizer"
    )
    cs.add_hyperparameter(learning_rate)

    # --- Optimizer ---
    optimizer = CategoricalHyperparameter(
        "optimizer",
        choices=["Adam", "SGD", "RMSprop"],
        default_value="Adam",
        doc="The optimization algorithm to use"
    )
    cs.add_hyperparameter(optimizer)

    # --- Batch Size ---
    batch_size = UniformIntegerHyperparameter(
        "batch_size",
        lower=32,
        upper=256,
        default_value=64,
        log=True,
        doc="Number of samples per batch"
    )
    cs.add_hyperparameter(batch_size)

    # --- Number of Epochs ---
    epochs = UniformIntegerHyperparameter(
        "epochs",
        lower=5,
        upper=50,
        default_value=10,
        log=False,
        doc="Number of training epochs"
    )
    cs.add_hyperparameter(epochs)

    # --- Momentum (only for SGD) ---
    momentum = UniformFloatHyperparameter(
        "momentum",
        lower=0.0,
        upper=0.99,
        default_value=0.9,
        doc="Momentum for SGD optimizer",
    )
    cs.add_hyperparameter(momentum)
    condition_momentum = EqualsCondition(momentum, optimizer, "SGD")
    cs.add_condition(condition_momentum)

    # --- Weight Decay (L2 Regularization) ---
    weight_decay = UniformFloatHyperparameter(
        "weight_decay",
        lower=1e-6,
        upper=1e-3,
        default_value=1e-4,
        log=True,
        doc="L2 regularization strength"
    )
    cs.add_hyperparameter(weight_decay)

    # --- Number of Convolutional Layers ---
    num_conv_layers = UniformIntegerHyperparameter(
        "num_conv_layers",
        lower=1,
        upper=5,
        default_value=3,
        log=False,
        doc="Number of convolutional layers"
    )
    cs.add_hyperparameter(num_conv_layers)

    # --- Number of Filters (Conv Layers) ---
    num_filters_l1 = UniformIntegerHyperparameter(
        "num_filters_l1",
        lower=32,
        upper=128,
        default_value=64,
        log=True,
        doc="Number of filters in the first convolutional layer"
    )
    cs.add_hyperparameter(num_filters_l1)

    num_filters_mult = UniformFloatHyperparameter(
        "num_filters_mult",
        lower=1.0,
        upper=2.0,
        default_value=1.5,
        log=False,
        doc="Multiplier for the number of filters in subsequent convolutional layers"
    )
    cs.add_hyperparameter(num_filters_mult)


    # --- Dropout Rate ---
    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate",
        lower=0.0,
        upper=0.7,
        default_value=0.25,
        doc="Dropout rate for regularization"
    )
    cs.add_hyperparameter(dropout_rate)

    # --- Number of Dense Layers ---
    num_dense_layers = UniformIntegerHyperparameter(
        "num_dense_layers",
        lower=1,
        upper=3,
        default_value=2,
        log=False,
        doc="Number of dense (fully connected) layers"
    )
    cs.add_hyperparameter(num_dense_layers)

    # --- Number of Units (Dense Layers) ---
    num_units_l1 = UniformIntegerHyperparameter(
        "num_units_l1",
        lower=64,
        upper=512,
        default_value=256,
        log=True,
        doc="Number of units in the first dense layer"
    )
    cs.add_hyperparameter(num_units_l1)

    num_units_mult = UniformFloatHyperparameter(
        "num_units_mult",
        lower=0.25,
        upper=1.0,
        default_value=0.5,
        log=False,
        doc="Multiplier for the number of units in subsequent dense layers"
    )
    cs.add_hyperparameter(num_units_mult)
    return cs

--------------------------------------------------------------------------------
[2025-06-15 16:34:29] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#2): UniformFloatHyperparameter.__init__() got an unexpected keyword argument 'doc'
--------------------------------------------------------------------------------
[2025-06-15 16:34:34] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, EqualsCondition

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace for hyperparameter optimization of a CNN
    for image classification.
    """
    cs = ConfigurationSpace()

    # --- Learning Rate ---
    learning_rate = UniformFloatHyperparameter(
        "learning_rate",
        lower=1e-5,
        upper=1e-2,
        default_value=1e-3,
        log=True,
    )
    cs.add_hyperparameter(learning_rate)

    # --- Optimizer ---
    optimizer = CategoricalHyperparameter(
        "optimizer",
        choices=["Adam", "SGD", "RMSprop"],
        default_value="Adam",
    )
    cs.add_hyperparameter(optimizer)

    # --- Batch Size ---
    batch_size = UniformIntegerHyperparameter(
        "batch_size",
        lower=32,
        upper=256,
        default_value=64,
        log=True,
    )
    cs.add_hyperparameter(batch_size)

    # --- Number of Epochs ---
    epochs = UniformIntegerHyperparameter(
        "epochs",
        lower=5,
        upper=50,
        default_value=10,
        log=False,
    )
    cs.add_hyperparameter(epochs)

    # --- Momentum (only for SGD) ---
    momentum = UniformFloatHyperparameter(
        "momentum",
        lower=0.0,
        upper=0.99,
        default_value=0.9,
    )
    cs.add_hyperparameter(momentum)
    condition_momentum = EqualsCondition(momentum, optimizer, "SGD")
    cs.add_condition(condition_momentum)

    # --- Weight Decay (L2 Regularization) ---
    weight_decay = UniformFloatHyperparameter(
        "weight_decay",
        lower=1e-6,
        upper=1e-3,
        default_value=1e-4,
        log=True,
    )
    cs.add_hyperparameter(weight_decay)

    # --- Number of Convolutional Layers ---
    num_conv_layers = UniformIntegerHyperparameter(
        "num_conv_layers",
        lower=1,
        upper=5,
        default_value=3,
        log=False,
    )
    cs.add_hyperparameter(num_conv_layers)

    # --- Number of Filters (Conv Layers) ---
    num_filters_l1 = UniformIntegerHyperparameter(
        "num_filters_l1",
        lower=32,
        upper=128,
        default_value=64,
        log=True,
    )
    cs.add_hyperparameter(num_filters_l1)

    num_filters_mult = UniformFloatHyperparameter(
        "num_filters_mult",
        lower=1.0,
        upper=2.0,
        default_value=1.5,
        log=False,
    )
    cs.add_hyperparameter(num_filters_mult)


    # --- Dropout Rate ---
    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate",
        lower=0.0,
        upper=0.7,
        default_value=0.25,
    )
    cs.add_hyperparameter(dropout_rate)

    # --- Number of Dense Layers ---
    num_dense_layers = UniformIntegerHyperparameter(
        "num_dense_layers",
        lower=1,
        upper=3,
        default_value=2,
        log=False,
    )
    cs.add_hyperparameter(num_dense_layers)

    # --- Number of Units (Dense Layers) ---
    num_units_l1 = UniformIntegerHyperparameter(
        "num_units_l1",
        lower=64,
        upper=512,
        default_value=256,
        log=True,
    )
    cs.add_hyperparameter(num_units_l1)

    num_units_mult = UniformFloatHyperparameter(
        "num_units_mult",
        lower=0.25,
        upper=1.0,
        default_value=0.5,
        log=False,
    )
    cs.add_hyperparameter(num_units_mult)
    return cs
```
--------------------------------------------------------------------------------
[2025-06-15 16:34:34] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, EqualsCondition

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace for hyperparameter optimization of a CNN
    for image classification.
    """
    cs = ConfigurationSpace()

    # --- Learning Rate ---
    learning_rate = UniformFloatHyperparameter(
        "learning_rate",
        lower=1e-5,
        upper=1e-2,
        default_value=1e-3,
        log=True,
    )
    cs.add_hyperparameter(learning_rate)

    # --- Optimizer ---
    optimizer = CategoricalHyperparameter(
        "optimizer",
        choices=["Adam", "SGD", "RMSprop"],
        default_value="Adam",
    )
    cs.add_hyperparameter(optimizer)

    # --- Batch Size ---
    batch_size = UniformIntegerHyperparameter(
        "batch_size",
        lower=32,
        upper=256,
        default_value=64,
        log=True,
    )
    cs.add_hyperparameter(batch_size)

    # --- Number of Epochs ---
    epochs = UniformIntegerHyperparameter(
        "epochs",
        lower=5,
        upper=50,
        default_value=10,
        log=False,
    )
    cs.add_hyperparameter(epochs)

    # --- Momentum (only for SGD) ---
    momentum = UniformFloatHyperparameter(
        "momentum",
        lower=0.0,
        upper=0.99,
        default_value=0.9,
    )
    cs.add_hyperparameter(momentum)
    condition_momentum = EqualsCondition(momentum, optimizer, "SGD")
    cs.add_condition(condition_momentum)

    # --- Weight Decay (L2 Regularization) ---
    weight_decay = UniformFloatHyperparameter(
        "weight_decay",
        lower=1e-6,
        upper=1e-3,
        default_value=1e-4,
        log=True,
    )
    cs.add_hyperparameter(weight_decay)

    # --- Number of Convolutional Layers ---
    num_conv_layers = UniformIntegerHyperparameter(
        "num_conv_layers",
        lower=1,
        upper=5,
        default_value=3,
        log=False,
    )
    cs.add_hyperparameter(num_conv_layers)

    # --- Number of Filters (Conv Layers) ---
    num_filters_l1 = UniformIntegerHyperparameter(
        "num_filters_l1",
        lower=32,
        upper=128,
        default_value=64,
        log=True,
    )
    cs.add_hyperparameter(num_filters_l1)

    num_filters_mult = UniformFloatHyperparameter(
        "num_filters_mult",
        lower=1.0,
        upper=2.0,
        default_value=1.5,
        log=False,
    )
    cs.add_hyperparameter(num_filters_mult)


    # --- Dropout Rate ---
    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate",
        lower=0.0,
        upper=0.7,
        default_value=0.25,
    )
    cs.add_hyperparameter(dropout_rate)

    # --- Number of Dense Layers ---
    num_dense_layers = UniformIntegerHyperparameter(
        "num_dense_layers",
        lower=1,
        upper=3,
        default_value=2,
        log=False,
    )
    cs.add_hyperparameter(num_dense_layers)

    # --- Number of Units (Dense Layers) ---
    num_units_l1 = UniformIntegerHyperparameter(
        "num_units_l1",
        lower=64,
        upper=512,
        default_value=256,
        log=True,
    )
    cs.add_hyperparameter(num_units_l1)

    num_units_mult = UniformFloatHyperparameter(
        "num_units_mult",
        lower=0.25,
        upper=1.0,
        default_value=0.5,
        log=False,
    )
    cs.add_hyperparameter(num_units_mult)
    return cs

--------------------------------------------------------------------------------
[2025-06-15 16:34:34] [Metadata: {'component': 'config', 'status': 'success'}] Configuration space generated successfully
--------------------------------------------------------------------------------
[2025-06-15 16:34:36] [Metadata: {'component': 'scenario'}] ```python
from smac import Scenario
from ConfigSpace import ConfigurationSpace
from pathlib import Path

def generate_scenario(cs: ConfigurationSpace) -> Scenario:
    """
    Generates a SMAC scenario configuration for hyperparameter optimization.

    Args:
        cs (ConfigurationSpace): The configuration space from which to sample configurations.

    Returns:
        Scenario: A configured SMAC Scenario object.
    """

    scenario = Scenario(
        configspace=cs,
        name="image_classification_optimization",
        output_directory=Path("smac3_output"),
        deterministic=False,
        walltime_limit=7200,  # 2 hours
        cputime_limit=7200,  # 2 hours CPU time
        trial_walltime_limit=600,  # 10 minutes per trial
        trial_memory_limit=8192,  # 8 GB per trial
        n_trials=100,
        n_workers=4,  # Adjust based on available resources
        min_budget=1,
        max_budget=5,
        seed=42,
    )
    return scenario
```
--------------------------------------------------------------------------------
[2025-06-15 16:34:36] [Metadata: {'component': 'scenario', 'action': 'run'}] from smac import Scenario
from ConfigSpace import ConfigurationSpace
from pathlib import Path

def generate_scenario(cs: ConfigurationSpace) -> Scenario:
    """
    Generates a SMAC scenario configuration for hyperparameter optimization.

    Args:
        cs (ConfigurationSpace): The configuration space from which to sample configurations.

    Returns:
        Scenario: A configured SMAC Scenario object.
    """

    scenario = Scenario(
        configspace=cs,
        name="image_classification_optimization",
        output_directory=Path("smac3_output"),
        deterministic=False,
        walltime_limit=7200,  # 2 hours
        cputime_limit=7200,  # 2 hours CPU time
        trial_walltime_limit=600,  # 10 minutes per trial
        trial_memory_limit=8192,  # 8 GB per trial
        n_trials=100,
        n_workers=4,  # Adjust based on available resources
        min_budget=1,
        max_budget=5,
        seed=42,
    )
    return scenario

--------------------------------------------------------------------------------
[2025-06-15 16:34:36] [Metadata: {'component': 'scenario', 'status': 'success'}] Scenario generated successfully
--------------------------------------------------------------------------------
[2025-06-15 16:34:44] [Metadata: {'component': 'train_function'}] ```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from ConfigSpace import Configuration
from typing import Any
import numpy as np
from math import sqrt

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """Trains a CNN model on the given dataset using the provided configuration."""
    torch.manual_seed(seed)

    X, y = dataset['X'], dataset['y']

    # Data Preprocessing
    if X.ndim == 2:
        n_features = X.shape[1]
        height = width = int(sqrt(n_features))
        if height * width != n_features:
            raise ValueError("Input features are not a square image.")
        X = X.reshape(-1, 1, height, width)  # (N, 1, H, W)
    elif X.ndim == 3:
        X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # (N, 1, H, W)
    elif X.ndim == 4:
        pass  # Assume (N, C, H, W) or (N, H, W, C) - PyTorch default is NCHW
    else:
        raise ValueError("Invalid input data dimension.  Must be 2, 3, or 4.")

    X = X.astype(np.float32) / 255.0  # Normalize to [0, 1]
    y = y.astype(np.int64)

    X = torch.tensor(X)
    y = torch.tensor(y)

    # Hyperparameter Extraction
    lr = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    epochs = cfg.get('epochs')
    weight_decay = cfg.get('weight_decay')
    num_conv_layers = cfg.get('num_conv_layers')
    num_filters_l1 = cfg.get('num_filters_l1')
    num_filters_mult = cfg.get('num_filters_mult')
    dropout_rate = cfg.get('dropout_rate')
    num_dense_layers = cfg.get('num_dense_layers')
    num_units_l1 = cfg.get('num_units_l1')
    num_units_mult = cfg.get('num_units_mult')
    optimizer_name = cfg.get('optimizer')
    
    momentum = cfg.get('momentum') if 'momentum' in cfg else 0.9 # Handle conditional hyperparameter

    # Model Definition
    class CNN(nn.Module):
        def __init__(self, num_classes, num_conv_layers, num_filters_l1, num_filters_mult, dropout_rate, num_dense_layers, num_units_l1, num_units_mult):
            super().__init__()
            self.features = nn.ModuleList()
            in_channels = 1  # Assuming grayscale images
            num_filters = num_filters_l1
            for i in range(num_conv_layers):
                self.features.append(nn.Conv2d(in_channels, num_filters, kernel_size=3, padding=1))
                self.features.append(nn.ReLU(inplace=True))
                self.features.append(nn.BatchNorm2d(num_filters))
                self.features.append(nn.MaxPool2d(kernel_size=2, stride=2))
                in_channels = num_filters
                num_filters = int(num_filters * num_filters_mult)
                
            self.features = nn.Sequential(*self.features)
            
            self.avgpool = nn.AdaptiveAvgPool2d((7, 7))  # Ensure consistent feature map size
            
            self.classifier = nn.ModuleList()
            in_features = 512 * 7 * 7 if num_conv_layers > 0 else X.shape[1]  #Dynamically infer input size
            num_units = num_units_l1
            for i in range(num_dense_layers):
                self.classifier.append(nn.Linear(in_features, num_units))
                self.classifier.append(nn.ReLU(inplace=True))
                self.classifier.append(nn.Dropout(p=dropout_rate))
                in_features = num_units
                num_units = int(num_units * num_units_mult)

            self.classifier.append(nn.Linear(in_features, num_classes))
            self.classifier = nn.Sequential(*self.classifier)

        def forward(self, x):
            x = self.features(x)
            x = self.avgpool(x)
            x = torch.flatten(x, 1)
            x = self.classifier(x)
            return x

    model = CNN(num_classes=10,
                  num_conv_layers=num_conv_layers,
                  num_filters_l1=num_filters_l1,
                  num_filters_mult=num_filters_mult,
                  dropout_rate=dropout_rate,
                  num_dense_layers=num_dense_layers,
                  num_units_l1=num_units_l1,
                  num_units_mult=num_units_mult)

    # Optimizer Selection
    if optimizer_name == "Adam":
        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    elif optimizer_name == "SGD":
        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)
    elif optimizer_name == "RMSprop":
        optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)
    else:
        raise ValueError(f"Unsupported optimizer: {optimizer_name}")

    # Loss Function
    criterion = nn.CrossEntropyLoss()

    # Data Loader
    dataset = TensorDataset(X, y)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)

    # Training Loop
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in dataloader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

    # Return final loss
    return running_loss / len(dataloader)
```
--------------------------------------------------------------------------------
[2025-06-15 16:34:44] [Metadata: {'component': 'train_function', 'action': 'run'}] import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from ConfigSpace import Configuration
from typing import Any
import numpy as np
from math import sqrt

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """Trains a CNN model on the given dataset using the provided configuration."""
    torch.manual_seed(seed)

    X, y = dataset['X'], dataset['y']

    # Data Preprocessing
    if X.ndim == 2:
        n_features = X.shape[1]
        height = width = int(sqrt(n_features))
        if height * width != n_features:
            raise ValueError("Input features are not a square image.")
        X = X.reshape(-1, 1, height, width)  # (N, 1, H, W)
    elif X.ndim == 3:
        X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # (N, 1, H, W)
    elif X.ndim == 4:
        pass  # Assume (N, C, H, W) or (N, H, W, C) - PyTorch default is NCHW
    else:
        raise ValueError("Invalid input data dimension.  Must be 2, 3, or 4.")

    X = X.astype(np.float32) / 255.0  # Normalize to [0, 1]
    y = y.astype(np.int64)

    X = torch.tensor(X)
    y = torch.tensor(y)

    # Hyperparameter Extraction
    lr = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    epochs = cfg.get('epochs')
    weight_decay = cfg.get('weight_decay')
    num_conv_layers = cfg.get('num_conv_layers')
    num_filters_l1 = cfg.get('num_filters_l1')
    num_filters_mult = cfg.get('num_filters_mult')
    dropout_rate = cfg.get('dropout_rate')
    num_dense_layers = cfg.get('num_dense_layers')
    num_units_l1 = cfg.get('num_units_l1')
    num_units_mult = cfg.get('num_units_mult')
    optimizer_name = cfg.get('optimizer')
    
    momentum = cfg.get('momentum') if 'momentum' in cfg else 0.9 # Handle conditional hyperparameter

    # Model Definition
    class CNN(nn.Module):
        def __init__(self, num_classes, num_conv_layers, num_filters_l1, num_filters_mult, dropout_rate, num_dense_layers, num_units_l1, num_units_mult):
            super().__init__()
            self.features = nn.ModuleList()
            in_channels = 1  # Assuming grayscale images
            num_filters = num_filters_l1
            for i in range(num_conv_layers):
                self.features.append(nn.Conv2d(in_channels, num_filters, kernel_size=3, padding=1))
                self.features.append(nn.ReLU(inplace=True))
                self.features.append(nn.BatchNorm2d(num_filters))
                self.features.append(nn.MaxPool2d(kernel_size=2, stride=2))
                in_channels = num_filters
                num_filters = int(num_filters * num_filters_mult)
                
            self.features = nn.Sequential(*self.features)
            
            self.avgpool = nn.AdaptiveAvgPool2d((7, 7))  # Ensure consistent feature map size
            
            self.classifier = nn.ModuleList()
            in_features = 512 * 7 * 7 if num_conv_layers > 0 else X.shape[1]  #Dynamically infer input size
            num_units = num_units_l1
            for i in range(num_dense_layers):
                self.classifier.append(nn.Linear(in_features, num_units))
                self.classifier.append(nn.ReLU(inplace=True))
                self.classifier.append(nn.Dropout(p=dropout_rate))
                in_features = num_units
                num_units = int(num_units * num_units_mult)

            self.classifier.append(nn.Linear(in_features, num_classes))
            self.classifier = nn.Sequential(*self.classifier)

        def forward(self, x):
            x = self.features(x)
            x = self.avgpool(x)
            x = torch.flatten(x, 1)
            x = self.classifier(x)
            return x

    model = CNN(num_classes=10,
                  num_conv_layers=num_conv_layers,
                  num_filters_l1=num_filters_l1,
                  num_filters_mult=num_filters_mult,
                  dropout_rate=dropout_rate,
                  num_dense_layers=num_dense_layers,
                  num_units_l1=num_units_l1,
                  num_units_mult=num_units_mult)

    # Optimizer Selection
    if optimizer_name == "Adam":
        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    elif optimizer_name == "SGD":
        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)
    elif optimizer_name == "RMSprop":
        optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)
    else:
        raise ValueError(f"Unsupported optimizer: {optimizer_name}")

    # Loss Function
    criterion = nn.CrossEntropyLoss()

    # Data Loader
    dataset = TensorDataset(X, y)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)

    # Training Loop
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in dataloader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

    # Return final loss
    return running_loss / len(dataloader)

--------------------------------------------------------------------------------
[2025-06-15 16:34:44] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#1): 'DataFrame' object has no attribute 'reshape'
--------------------------------------------------------------------------------
[2025-06-15 16:34:51] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from ConfigSpace import Configuration
from typing import Any
import numpy as np
from math import sqrt
import pandas as pd

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """Trains a CNN model on the given dataset using the provided configuration."""
    torch.manual_seed(seed)

    X, y = dataset['X'], dataset['y']

    # Data Preprocessing
    if isinstance(X, pd.DataFrame):
        X = X.values
    if isinstance(y, pd.Series):
        y = y.values
        
    if X.ndim == 2:
        n_features = X.shape[1]
        height = width = int(sqrt(n_features))
        if height * width != n_features:
            raise ValueError("Input features are not a square image.")
        X = X.reshape(-1, 1, height, width)  # (N, 1, H, W)
    elif X.ndim == 3:
        X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # (N, 1, H, W)
    elif X.ndim == 4:
        pass  # Assume (N, C, H, W) or (N, H, W, C) - PyTorch default is NCHW
    else:
        raise ValueError("Invalid input data dimension.  Must be 2, 3, or 4.")

    X = X.astype(np.float32) / 255.0  # Normalize to [0, 1]
    y = y.astype(np.int64)

    X = torch.tensor(X)
    y = torch.tensor(y)

    # Hyperparameter Extraction
    lr = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    epochs = cfg.get('epochs')
    weight_decay = cfg.get('weight_decay')
    num_conv_layers = cfg.get('num_conv_layers')
    num_filters_l1 = cfg.get('num_filters_l1')
    num_filters_mult = cfg.get('num_filters_mult')
    dropout_rate = cfg.get('dropout_rate')
    num_dense_layers = cfg.get('num_dense_layers')
    num_units_l1 = cfg.get('num_units_l1')
    num_units_mult = cfg.get('num_units_mult')
    optimizer_name = cfg.get('optimizer')
    
    momentum = cfg.get('momentum') if 'momentum' in cfg else 0.9 # Handle conditional hyperparameter

    # Model Definition
    class CNN(nn.Module):
        def __init__(self, num_classes, num_conv_layers, num_filters_l1, num_filters_mult, dropout_rate, num_dense_layers, num_units_l1, num_units_mult):
            super().__init__()
            self.features = nn.ModuleList()
            in_channels = 1  # Assuming grayscale images
            num_filters = num_filters_l1
            for i in range(num_conv_layers):
                self.features.append(nn.Conv2d(in_channels, num_filters, kernel_size=3, padding=1))
                self.features.append(nn.ReLU(inplace=True))
                self.features.append(nn.BatchNorm2d(num_filters))
                self.features.append(nn.MaxPool2d(kernel_size=2, stride=2))
                in_channels = num_filters
                num_filters = int(num_filters * num_filters_mult)
                
            self.features = nn.Sequential(*self.features)
            
            self.avgpool = nn.AdaptiveAvgPool2d((7, 7))  # Ensure consistent feature map size
            
            self.classifier = nn.ModuleList()
            
            #Dynamically infer input size after passing through convolutional layers
            if num_conv_layers > 0:
                in_features = int(num_filters_l1 * (num_filters_mult ** (num_conv_layers -1)) * 7 * 7) if num_conv_layers > 1 else num_filters_l1 * 7 * 7
            else:
                in_features = X.shape[1] * X.shape[2] * X.shape[3] # Flattened input size

            num_units = num_units_l1
            for i in range(num_dense_layers):
                self.classifier.append(nn.Linear(in_features, num_units))
                self.classifier.append(nn.ReLU(inplace=True))
                self.classifier.append(nn.Dropout(p=dropout_rate))
                in_features = num_units
                num_units = int(num_units * num_units_mult)

            self.classifier.append(nn.Linear(in_features, num_classes))
            self.classifier = nn.Sequential(*self.classifier)

        def forward(self, x):
            x = self.features(x)
            x = self.avgpool(x)
            x = torch.flatten(x, 1)
            x = self.classifier(x)
            return x

    model = CNN(num_classes=10,
                  num_conv_layers=num_conv_layers,
                  num_filters_l1=num_filters_l1,
                  num_filters_mult=num_filters_mult,
                  dropout_rate=dropout_rate,
                  num_dense_layers=num_dense_layers,
                  num_units_l1=num_units_l1,
                  num_units_mult=num_units_mult)

    # Optimizer Selection
    if optimizer_name == "Adam":
        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    elif optimizer_name == "SGD":
        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)
    elif optimizer_name == "RMSprop":
        optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)
    else:
        raise ValueError(f"Unsupported optimizer: {{optimizer_name}}")

    # Loss Function
    criterion = nn.CrossEntropyLoss()

    # Data Loader
    dataset = TensorDataset(X, y)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)

    # Training Loop
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in dataloader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

    # Return final loss
    return running_loss / len(dataloader)
```
--------------------------------------------------------------------------------
[2025-06-15 16:34:51] [Metadata: {'component': 'train_function', 'action': 'run'}] import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from ConfigSpace import Configuration
from typing import Any
import numpy as np
from math import sqrt
import pandas as pd

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """Trains a CNN model on the given dataset using the provided configuration."""
    torch.manual_seed(seed)

    X, y = dataset['X'], dataset['y']

    # Data Preprocessing
    if isinstance(X, pd.DataFrame):
        X = X.values
    if isinstance(y, pd.Series):
        y = y.values
        
    if X.ndim == 2:
        n_features = X.shape[1]
        height = width = int(sqrt(n_features))
        if height * width != n_features:
            raise ValueError("Input features are not a square image.")
        X = X.reshape(-1, 1, height, width)  # (N, 1, H, W)
    elif X.ndim == 3:
        X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # (N, 1, H, W)
    elif X.ndim == 4:
        pass  # Assume (N, C, H, W) or (N, H, W, C) - PyTorch default is NCHW
    else:
        raise ValueError("Invalid input data dimension.  Must be 2, 3, or 4.")

    X = X.astype(np.float32) / 255.0  # Normalize to [0, 1]
    y = y.astype(np.int64)

    X = torch.tensor(X)
    y = torch.tensor(y)

    # Hyperparameter Extraction
    lr = cfg.get('learning_rate')
    batch_size = cfg.get('batch_size')
    epochs = cfg.get('epochs')
    weight_decay = cfg.get('weight_decay')
    num_conv_layers = cfg.get('num_conv_layers')
    num_filters_l1 = cfg.get('num_filters_l1')
    num_filters_mult = cfg.get('num_filters_mult')
    dropout_rate = cfg.get('dropout_rate')
    num_dense_layers = cfg.get('num_dense_layers')
    num_units_l1 = cfg.get('num_units_l1')
    num_units_mult = cfg.get('num_units_mult')
    optimizer_name = cfg.get('optimizer')
    
    momentum = cfg.get('momentum') if 'momentum' in cfg else 0.9 # Handle conditional hyperparameter

    # Model Definition
    class CNN(nn.Module):
        def __init__(self, num_classes, num_conv_layers, num_filters_l1, num_filters_mult, dropout_rate, num_dense_layers, num_units_l1, num_units_mult):
            super().__init__()
            self.features = nn.ModuleList()
            in_channels = 1  # Assuming grayscale images
            num_filters = num_filters_l1
            for i in range(num_conv_layers):
                self.features.append(nn.Conv2d(in_channels, num_filters, kernel_size=3, padding=1))
                self.features.append(nn.ReLU(inplace=True))
                self.features.append(nn.BatchNorm2d(num_filters))
                self.features.append(nn.MaxPool2d(kernel_size=2, stride=2))
                in_channels = num_filters
                num_filters = int(num_filters * num_filters_mult)
                
            self.features = nn.Sequential(*self.features)
            
            self.avgpool = nn.AdaptiveAvgPool2d((7, 7))  # Ensure consistent feature map size
            
            self.classifier = nn.ModuleList()
            
            #Dynamically infer input size after passing through convolutional layers
            if num_conv_layers > 0:
                in_features = int(num_filters_l1 * (num_filters_mult ** (num_conv_layers -1)) * 7 * 7) if num_conv_layers > 1 else num_filters_l1 * 7 * 7
            else:
                in_features = X.shape[1] * X.shape[2] * X.shape[3] # Flattened input size

            num_units = num_units_l1
            for i in range(num_dense_layers):
                self.classifier.append(nn.Linear(in_features, num_units))
                self.classifier.append(nn.ReLU(inplace=True))
                self.classifier.append(nn.Dropout(p=dropout_rate))
                in_features = num_units
                num_units = int(num_units * num_units_mult)

            self.classifier.append(nn.Linear(in_features, num_classes))
            self.classifier = nn.Sequential(*self.classifier)

        def forward(self, x):
            x = self.features(x)
            x = self.avgpool(x)
            x = torch.flatten(x, 1)
            x = self.classifier(x)
            return x

    model = CNN(num_classes=10,
                  num_conv_layers=num_conv_layers,
                  num_filters_l1=num_filters_l1,
                  num_filters_mult=num_filters_mult,
                  dropout_rate=dropout_rate,
                  num_dense_layers=num_dense_layers,
                  num_units_l1=num_units_l1,
                  num_units_mult=num_units_mult)

    # Optimizer Selection
    if optimizer_name == "Adam":
        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    elif optimizer_name == "SGD":
        optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)
    elif optimizer_name == "RMSprop":
        optimizer = optim.RMSprop(model.parameters(), lr=lr, weight_decay=weight_decay)
    else:
        raise ValueError(f"Unsupported optimizer: {{optimizer_name}}")

    # Loss Function
    criterion = nn.CrossEntropyLoss()

    # Data Loader
    dataset = TensorDataset(X, y)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=0)

    # Training Loop
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in dataloader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

    # Return final loss
    return running_loss / len(dataloader)

--------------------------------------------------------------------------------
[2025-06-15 16:39:22] [Metadata: {'component': 'train_function', 'status': 'success', 'loss': 0.008218348727954435}] Training executed successfully, loss: 0.008218348727954435
--------------------------------------------------------------------------------
