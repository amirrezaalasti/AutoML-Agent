[2025-05-20 12:16:27] [Metadata: {'component': 'config'}] **TASK**

Goal: Write a Python function called `get_configspace()` that returns a valid `ConfigurationSpace` for a classification task.

---

**STRICT OUTPUT RULES**

* Output only the `get_configspace()` function and necessary imports.
* Do not include any extra text, explanations, or comments.
* Code must be syntactically correct, executable, and compatible with SMAC.

---

**ALLOWED CLASSES**

**Core Classes**

* `ConfigurationSpace`
* `Categorical`
* `Float`
* `Integer`
* `Constant`

**Conditions**

* `EqualsCondition`
* `InCondition`
* `OrConjunction`

**Forbidden Clauses**

* `ForbiddenEqualsClause`
* `ForbiddenAndConjunction`

**Distributions (only if needed)**

* `Beta`
* `Normal`

**Serialization (only if needed)**

* `to_yaml()`
* `from_yaml()`

---

**ALLOWED OPTIONS**

* `default`
* `log`
* `distribution`
* `seed`

---

**CONSTRAINTS**

* Must include **at least one** `ForbiddenAndConjunction` to block invalid combinations.

---

**CONFIGURATION SPACE REQUIREMENTS**

* Initialize `ConfigurationSpace` with `seed=1234`.

---

**DATASET DESCRIPTION**

* The configuration space must be based on the following information
This is an image dataset.

Number of classes: 10
Class distribution:
1    6742
7    6265
3    6131
2    5958
9    5949
0    5923
6    5918
8    5851
4    5842
5    5421
Name: count, dtype: int64.
* Hyperparameters and model choices must reflect what is appropriate for that dataset type.

---

**IMPORTANT RULE**

* Do **not** use any classes, functions, methods, or modules outside of the **ALLOWED CLASSES**.

[EXAMPLES]

# Example 1: Basic ConfigurationSpace
```python
from ConfigSpace import ConfigurationSpace

cs = ConfigurationSpace(
    space={
        "C": (-1.0, 1.0),
        "max_iter": (10, 100),
    },
    seed=1234,
)
```
# Example 2: Adding Hyperparameters
```python
from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer

kernel_type = Categorical('kernel_type', ['linear', 'poly', 'rbf', 'sigmoid'])
degree = Integer('degree', bounds=(2, 4), default=2)
coef0 = Float('coef0', bounds=(0, 1), default=0.0)
gamma = Float('gamma', bounds=(1e-5, 1e2), default=1, log=True)

cs = ConfigurationSpace()
cs.add([kernel_type, degree, coef0, gamma])
```
# Example 3: Adding Conditions
```python
from ConfigSpace import EqualsCondition, InCondition, OrConjunction

cond_1 = EqualsCondition(degree, kernel_type, 'poly')
cond_2 = OrConjunction(
    EqualsCondition(coef0, kernel_type, 'poly'),
    EqualsCondition(coef0, kernel_type, 'sigmoid')
)
cond_3 = InCondition(gamma, kernel_type, ['rbf', 'poly', 'sigmoid'])
```
# Example 4: Adding Forbidden Clauses
```pyhon
from ConfigSpace import ForbiddenEqualsClause, ForbiddenAndConjunction

penalty_and_loss = ForbiddenAndConjunction(
    ForbiddenEqualsClause(penalty, "l1"),
    ForbiddenEqualsClause(loss, "hinge")
)
constant_penalty_and_loss = ForbiddenAndConjunction(
    ForbiddenEqualsClause(dual, "False"),
    ForbiddenEqualsClause(penalty, "l2"),
    ForbiddenEqualsClause(loss, "hinge")
)
penalty_and_dual = ForbiddenAndConjunction(
    ForbiddenEqualsClause(dual, "False"),
    ForbiddenEqualsClause(penalty, "l1")
)
```
Example 5: Serialization
```python
from pathlib import Path
from ConfigSpace import ConfigurationSpace

path = Path("configspace.yaml")
cs = ConfigurationSpace(
    space={
        "C": (-1.0, 1.0),
        "max_iter": (10, 100),
    },
    seed=1234,
)
cs.to_yaml(path)
loaded_cs = ConfigurationSpace.from_yaml(path)
```
# Example 6: Priors
```python
import numpy as np
from ConfigSpace import ConfigurationSpace, Float, Categorical, Beta, Normal

cs = ConfigurationSpace(
    space={
        "lr": Float(
            'lr',
            bounds=(1e-5, 1e-1),
            default=1e-3,
            log=True,
            distribution=Normal(1e-3, 1e-1)
        ),
        "dropout": Float(
            'dropout',
            bounds=(0, 0.99),
            default=0.25,
            distribution=Beta(alpha=2, beta=4)
        ),
        "activation": Categorical(
            'activation',
            items=['tanh', 'relu'],
            weights=[0.2, 0.8]
        ),
    },
    seed=1234,
)
```

--------------------------------------------------------------------------------
[2025-05-20 12:16:27] [Metadata: {'component': 'config', 'action': 'run'}] Running config code:
--------------------------------------------------------------------------------
[2025-05-20 12:16:27] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#1): Given vector violates forbidden clause: (Forbidden: momentum == 0.9 && Forbidden: optimizer == 'Adam')
--------------------------------------------------------------------------------
[2025-05-20 12:16:27] [Metadata: {'component': 'config', 'action': 'fix'}] The following python code is generated by an LLM which now has run into the following errors:
Given vector violates forbidden clause: (Forbidden: momentum == 0.9 && Forbidden: optimizer == 'Adam')

Your task is to fix the issues in the code. The requirements below MUST be respected:

1. Do not change the function name or its purpose.
2. Fix all syntax and runtime errors.
3. Use only valid parameters.
4. Do not change the logic unless necessary for fixing errors.
5. Return ONLY the corrected function code — no explanations, comments, or extra output.
6. The function must be syntactically and semantically correct, and executable with `exec`.

Buggy code:
```python
from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause


def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    optimizer = Categorical("optimizer", ["Adam", "SGD", "RMSprop"], default="Adam")
    learning_rate = Float("learning_rate", (1e-5, 1e-2), default=1e-3, log=True)
    batch_size = Integer("batch_size", (32, 256), default=64, log=True)
    momentum = Float("momentum", (0.0, 0.99), default=0.9, log=False)
    beta1 = Float("beta1", (0.0, 0.999), default=0.9, log=False)
    beta2 = Float("beta2", (0.0, 0.999), default=0.999, log=False)

    # Add hyperparameters to the configuration space
    cs.add_hyperparameters([optimizer, learning_rate, batch_size, momentum, beta1, beta2])

    # Define forbidden clauses
    forbidden_momentum = ForbiddenAndConjunction(
        ForbiddenEqualsClause(optimizer, "Adam"),
        ForbiddenEqualsClause(momentum, 0.9)
    )

    forbidden_beta1 = ForbiddenAndConjunction(
        ForbiddenEqualsClause(optimizer, "SGD"),
        ForbiddenEqualsClause(beta1, 0.9)
    )

    forbidden_beta2 = ForbiddenAndConjunction(
        ForbiddenEqualsClause(optimizer, "SGD"),
        ForbiddenEqualsClause(beta2, 0.999)
    )

    # Add forbidden clauses to the configuration space
    cs.add_forbidden_clauses([forbidden_momentum, forbidden_beta1, forbidden_beta2])

    return cs
```

--------------------------------------------------------------------------------
[2025-05-20 12:16:30] [Metadata: {'component': 'config', 'action': 'run'}] Running config code:
--------------------------------------------------------------------------------
[2025-05-20 12:16:31] [Metadata: {'component': 'scenario'}] ---

**Objective:**
Generate a **Python function** named `generate_scenario(cs)` that returns a valid `Scenario` object configured for SMAC (v2.0+), strictly following the rules below.

---

**Output Format Rules (Strict):**

* Output **only** the function `generate_scenario(cs)` and the **necessary import statements**.
* Use **Python 3.10 syntax** but **do not** include type annotations for the function or parameters.
* The code must be **fully executable** with the latest **SMAC v2.0+** version.
* Output **only valid Python code** – **no comments**, **no explanations**, **no extra text**, and **no example usage**.
* The function must be **self-contained**.

---

**Functional Requirements:**

* The input `cs` is a `ConfigurationSpace` object.
* Return a `Scenario` configured with the following:
  * `output_directory`: `"./automl_results"`
  * `deterministic`: `False` (enable variability)
  * `n_workers`: greater than 1 (to enable parallel optimization)
  * `min_budget` and `max_budget`: set appropriately for multi-fidelity tuning (e.g., training epochs)
---

**Reminder:** The output must be limited to:

* Valid `import` statements
* A single `generate_scenario(cs)` function that returns a properly configured `Scenario` object
* Do not use any parameters other than the ones explicitly listed in this prompt.

---

**Example (Correct Output Format):**

```python
from smac import Scenario
from ConfigSpace import Configuration

def generate_scenario(cs: Configuration):
    scenario = Scenario(
        configspace=cs,
        objectives="validation_loss",
        output_directory="./automl_results",
        deterministic=False,
        min_budget=1,
        max_budget=100,
        n_workers=4
    )
    return scenario
```

--------------------------------------------------------------------------------
[2025-05-20 12:16:31] [Metadata: {'component': 'scenario', 'action': 'run'}] Running scenario code:
--------------------------------------------------------------------------------
[2025-05-20 12:16:38] [Metadata: {'component': 'train_function'}] **Generate production-grade Python code for a machine learning training function with the following STRICT requirements:**

---

### **Function signature** must be:

```python
from ConfigSpace import Configuration
def train(cfg: Configuration, dataset: Any) -> float:
```

---

### **Function Behavior Requirements:**

* The function **must accept** a `dataset` dictionary with:

  * `dataset['X']`: feature matrix or input tensor
  * `dataset['y']`: label vector or label tensor

* Assume `cfg` is a sampled configuration object:

  * Access primitive values using `cfg.get('key')` (only `int`, `float`, `str`, etc.).
  * **Do not access or manipulate non-primitive hyperparameter objects**.

* The function must return the **average training loss** over 10 epochs.

### **Image Data Handling Requirements:**

For image datasets:

1. Check input dimensions:
   ```python
   X = dataset['X']
   if len(X.shape) == 4:  # Already in (N, C, H, W) or (N, H, W, C)
       # Use as is, just ensure correct channel format
       pass
   elif len(X.shape) == 3:  # (N, H, W) - Single channel
       X = X.reshape(X.shape[0], 1, X.shape[1], X.shape[2])
   elif len(X.shape) == 2:  # Flattened images
       # Calculate height/width for square images
       n_samples, n_features = X.shape
       height = int(np.sqrt(n_features))
       if height * height != n_features:
           raise ValueError(f"Flattened image dimensions {n_features} is not a perfect square")
       X = X.reshape(n_samples, 1, height, height)
   ```

2. Always verify and handle channel order:
   * For PyTorch: (N, C, H, W)
   * For TensorFlow: (N, H, W, C)

3. For non-CNN models:
   * Flatten the input if using dense/linear layers
   * Preserve original shape if using CNN

```python
return loss  # float
```

* Lower `loss` means a better model.

---

### **Frameworks**

You may choose **PyTorch**, **TensorFlow**, or **scikit-learn**, depending on the dataset and supporting code provided.

---

### **Model Requirements**

* Infer input and output dimensions dynamically from the dataset:

  ```python
  # For CNN models:
  if len(dataset['X'].shape) == 4:  # (N, C, H, W) or (N, H, W, C)
      n_channels = dataset['X'].shape[1] if using_pytorch else dataset['X'].shape[-1]
      height = width = dataset['X'].shape[2] if using_pytorch else dataset['X'].shape[1]
  else:  # For non-CNN models
      input_size = dataset['X'].shape[1]  # Flattened dimension
  
  num_classes = len(np.unique(dataset['y']))
  ```

---

### **Supporting Code Provided:**

* ConfigSpace definition: `from ConfigSpace import ConfigurationSpace, Categorical, Float, Integer, ForbiddenAndConjunction, ForbiddenEqualsClause


def get_configspace():
    cs = ConfigurationSpace(seed=1234)

    # Define hyperparameters
    optimizer = Categorical("optimizer", ["Adam", "SGD", "RMSprop"], default="Adam")
    learning_rate = Float("learning_rate", (1e-5, 1e-2), default=1e-3, log=True)
    batch_size = Integer("batch_size", (32, 256), default=64, log=True)
    momentum = Float("momentum", (0.0, 0.99), default=0.9, log=False)
    beta1 = Float("beta1", (0.0, 0.999), default=0.9, log=False)
    beta2 = Float("beta2", (0.0, 0.999), default=0.999, log=False)

    # Add hyperparameters to the configuration space
    cs.add_hyperparameters([optimizer, learning_rate, batch_size, momentum, beta1, beta2])

    # Define forbidden clauses
    forbidden_momentum = ForbiddenAndConjunction(
        ForbiddenEqualsClause(optimizer, "Adam"),
        ForbiddenEqualsClause(momentum, 0.0)
    )

    forbidden_beta1 = ForbiddenAndConjunction(
        ForbiddenEqualsClause(optimizer, "SGD"),
        ForbiddenEqualsClause(beta1, 0.0)
    )

    forbidden_beta2 = ForbiddenAndConjunction(
        ForbiddenEqualsClause(optimizer, "SGD"),
        ForbiddenEqualsClause(beta2, 0.0)
    )

    # Add forbidden clauses to the configuration space
    cs.add_forbidden_clauses([forbidden_momentum, forbidden_beta1, forbidden_beta2])

    return cs
`
* SMAC scenario: `from smac import Scenario
from ConfigSpace import ConfigurationSpace

def generate_scenario(cs):
    scenario = Scenario(
        configspace=cs,
        output_directory="./automl_results",
        deterministic=False,
        min_budget=1,
        max_budget=100,
        n_workers=4
    )
    return scenario
`
* Dataset description: `This is an image dataset.

Number of classes: 10
Class distribution:
1    6742
7    6265
3    6131
2    5958
9    5949
0    5923
6    5918
8    5851
4    5842
5    5421
Name: count, dtype: int64`

---

### **Additional Instructions**

* The code must not hardcode dataset dimensions like `784` or class count `10`.
* The function must be runnable and not assume unavailable classes or modules.
* You must only output the `def train(...)` function and nothing else.
* Always check dataset description for format hints before processing.

--------------------------------------------------------------------------------
[2025-05-20 12:16:38] [Metadata: {'component': 'train_function', 'action': 'run'}] Running train_function code:
--------------------------------------------------------------------------------
[2025-05-20 12:16:38] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#1): 'DataFrame' object has no attribute 'reshape'
--------------------------------------------------------------------------------
[2025-05-20 12:16:38] [Metadata: {'component': 'train_function', 'action': 'fix'}] The following python code is generated by an LLM which now has run into the following errors:
'DataFrame' object has no attribute 'reshape'

Your task is to fix the issues in the code. The requirements below MUST be respected:

1. Do not change the function name or its purpose.
2. Fix all syntax and runtime errors.
3. Use only valid parameters.
4. Do not change the logic unless necessary for fixing errors.
5. Return ONLY the corrected function code — no explanations, comments, or extra output.
6. The function must be syntactically and semantically correct, and executable with `exec`.

Buggy code:
```python
from typing import Any
from ConfigSpace import Configuration

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a PyTorch model on the given dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dictionary containing the dataset with keys 'X' and 'y'.

    Returns:
        float: Average training loss over 10 epochs.
    """

    X = dataset['X']
    y = dataset['y']

    # Image data handling
    if len(X.shape) == 4:  # Already in (N, C, H, W) or (N, H, W, C)
        pass  # Use as is, just ensure correct channel format
    elif len(X.shape) == 3:  # (N, H, W) - Single channel
        X = X.reshape(X.shape[0], 1, X.shape[1], X.shape[2])
    elif len(X.shape) == 2:  # Flattened images
        n_samples, n_features = X.shape
        height = int(np.sqrt(n_features))
        if height * height != n_features:
            raise ValueError(f"Flattened image dimensions {n_features} is not a perfect square")
        X = X.reshape(n_samples, 1, height, height)

    # Determine input dimensions and number of classes
    if len(X.shape) == 4:  # (N, C, H, W)
        n_channels = X.shape[1]
        height = width = X.shape[2]
    else:  # Flattened images
        input_size = X.shape[1]

    num_classes = len(np.unique(y))

    # Convert data to PyTorch tensors
    X = torch.tensor(X, dtype=torch.float32)
    y = torch.tensor(y, dtype=torch.long)

    # Create DataLoader
    batch_size = cfg.get('batch_size')
    train_dataset = TensorDataset(X, y)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    # Define the model
    if len(X.shape) == 4:
        # CNN model for image data
        class CNN(nn.Module):
            def __init__(self, n_channels, num_classes):
                super(CNN, self).__init__()
                self.conv1 = nn.Conv2d(n_channels, 32, kernel_size=3, padding=1)
                self.relu1 = nn.ReLU()
                self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)
                self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
                self.relu2 = nn.ReLU()
                self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)
                self.flatten = nn.Flatten()
                self.fc1 = nn.Linear(64 * (X.shape[2] // 4) * (X.shape[3] // 4), 128)
                self.relu3 = nn.ReLU()
                self.fc2 = nn.Linear(128, num_classes)

            def forward(self, x):
                x = self.maxpool1(self.relu1(self.conv1(x)))
                x = self.maxpool2(self.relu2(self.conv2(x)))
                x = self.flatten(x)
                x = self.relu3(self.fc1(x))
                x = self.fc2(x)
                return x

        model = CNN(n_channels, num_classes)
    else:
        # Simple linear model for flattened data
        class SimpleNN(nn.Module):
            def __init__(self, input_size, num_classes):
                super(SimpleNN, self).__init__()
                self.fc1 = nn.Linear(input_size, 128)
                self.relu = nn.ReLU()
                self.fc2 = nn.Linear(128, num_classes)

            def forward(self, x):
                x = self.relu(self.fc1(x))
                x = self.fc2(x)
                return x

        model = SimpleNN(input_size, num_classes)


    # Define optimizer
    optimizer_name = cfg.get('optimizer')
    learning_rate = cfg.get('learning_rate')
    if optimizer_name == 'Adam':
        beta1 = cfg.get('beta1')
        beta2 = cfg.get('beta2')
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2))
    elif optimizer_name == 'SGD':
        momentum = cfg.get('momentum')
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    elif optimizer_name == 'RMSprop':
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Unknown optimizer: {optimizer_name}")

    # Define loss function
    criterion = nn.CrossEntropyLoss()

    # Train the model
    num_epochs = 10
    total_loss = 0.0
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for inputs, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
        total_loss += epoch_loss / len(train_loader)  # Average loss per batch

    avg_loss = total_loss / num_epochs

    return float(avg_loss)
```

--------------------------------------------------------------------------------
[2025-05-20 12:16:44] [Metadata: {'component': 'train_function', 'action': 'run'}] Running train_function code:
--------------------------------------------------------------------------------
[2025-05-20 12:16:44] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#2): 'DataFrame' object has no attribute 'reshape'
--------------------------------------------------------------------------------
[2025-05-20 12:16:44] [Metadata: {'component': 'train_function', 'action': 'fix'}] The following python code is generated by an LLM which now has run into the following errors:
'DataFrame' object has no attribute 'reshape'
'DataFrame' object has no attribute 'reshape'

Your task is to fix the issues in the code. The requirements below MUST be respected:

1. Do not change the function name or its purpose.
2. Fix all syntax and runtime errors.
3. Use only valid parameters.
4. Do not change the logic unless necessary for fixing errors.
5. Return ONLY the corrected function code — no explanations, comments, or extra output.
6. The function must be syntactically and semantically correct, and executable with `exec`.

Buggy code:
```python
from typing import Any
from ConfigSpace import Configuration

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset


def train(cfg: Configuration, dataset: Any) -> float:
    """
    Trains a PyTorch model on the given dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dictionary containing the dataset with keys 'X' and 'y'.

    Returns:
        float: Average training loss over 10 epochs.
    """

    X = dataset['X']
    y = dataset['y']

    # Image data handling
    if len(X.shape) == 4:  # Already in (N, C, H, W) or (N, H, W, C)
        pass  # Use as is, just ensure correct channel format
    elif len(X.shape) == 3:  # (N, H, W) - Single channel
        X = X.reshape(X.shape[0], 1, X.shape[1], X.shape[2])
    elif len(X.shape) == 2:  # Flattened images
        n_samples, n_features = X.shape
        height = int(np.sqrt(n_features))
        if height * height != n_features:
            raise ValueError(f"Flattened image dimensions {{n_features}} is not a perfect square")
        X = X.reshape(n_samples, 1, height, height)

    # Determine input dimensions and number of classes
    if len(X.shape) == 4:  # (N, C, H, W)
        n_channels = X.shape[1]
        height = X.shape[2]
        width = X.shape[3]
    else:  # Flattened images
        input_size = X.shape[1]

    num_classes = len(np.unique(y))

    # Convert data to PyTorch tensors
    X = torch.tensor(X, dtype=torch.float32)
    y = torch.tensor(y, dtype=torch.long)

    # Create DataLoader
    batch_size = cfg.get('batch_size')
    train_dataset = TensorDataset(X, y)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    # Define the model
    if len(X.shape) == 4:
        # CNN model for image data
        class CNN(nn.Module):
            def __init__(self, n_channels, num_classes, img_height, img_width):
                super(CNN, self).__init__()
                self.conv1 = nn.Conv2d(n_channels, 32, kernel_size=3, padding=1)
                self.relu1 = nn.ReLU()
                self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)
                self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
                self.relu2 = nn.ReLU()
                self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)
                self.flatten = nn.Flatten()
                self.fc1 = nn.Linear(64 * (img_height // 4) * (img_width // 4), 128)
                self.relu3 = nn.ReLU()
                self.fc2 = nn.Linear(128, num_classes)

            def forward(self, x):
                x = self.maxpool1(self.relu1(self.conv1(x)))
                x = self.maxpool2(self.relu2(self.conv2(x)))
                x = self.flatten(x)
                x = self.relu3(self.fc1(x))
                x = self.fc2(x)
                return x

        model = CNN(n_channels, num_classes, height, width)
    else:
        # Simple linear model for flattened data
        class SimpleNN(nn.Module):
            def __init__(self, input_size, num_classes):
                super(SimpleNN, self).__init__()
                self.fc1 = nn.Linear(input_size, 128)
                self.relu = nn.ReLU()
                self.fc2 = nn.Linear(128, num_classes)

            def forward(self, x):
                x = self.relu(self.fc1(x))
                x = self.fc2(x)
                return x

        model = SimpleNN(input_size, num_classes)


    # Define optimizer
    optimizer_name = cfg.get('optimizer')
    learning_rate = cfg.get('learning_rate')
    if optimizer_name == 'Adam':
        beta1 = cfg.get('beta1')
        beta2 = cfg.get('beta2')
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2))
    elif optimizer_name == 'SGD':
        momentum = cfg.get('momentum')
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    elif optimizer_name == 'RMSprop':
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Unknown optimizer: {{optimizer_name}}")

    # Define loss function
    criterion = nn.CrossEntropyLoss()

    # Train the model
    num_epochs = 10
    total_loss = 0.0
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for inputs, labels in train_loader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            epoch_loss += loss.item()
        total_loss += epoch_loss / len(train_loader)  # Average loss per batch

    avg_loss = total_loss / num_epochs

    return float(avg_loss)
```

--------------------------------------------------------------------------------
[2025-05-20 12:16:51] [Metadata: {'component': 'train_function', 'action': 'run'}] Running train_function code:
--------------------------------------------------------------------------------
