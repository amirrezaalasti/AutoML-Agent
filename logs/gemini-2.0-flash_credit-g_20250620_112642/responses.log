[2025-06-20 11:26:57] [Metadata: {'component': 'config'}] ```python
from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, InCondition

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization.
    """
    cs = ConfigurationSpace()

    # --- Model Choice ---
    model_type = CategoricalHyperparameter(
        "model_type", choices=["logistic_regression", "random_forest", "svm"], default_value="random_forest",
        docstring="The type of model to use."
    )
    cs.add_hyperparameter(model_type)

    # --- Logistic Regression Hyperparameters ---
    lr_C = UniformFloatHyperparameter(
        "logistic_regression:C", lower=1e-5, upper=10, default_value=1.0, log=True,
        docstring="Inverse of regularization strength. Smaller values specify stronger regularization."
    )
    lr_penalty = CategoricalHyperparameter(
        "logistic_regression:penalty", choices=["l1", "l2"], default_value="l2",
        docstring="The norm used in the penalization."
    )

    cs.add_hyperparameters([lr_C, lr_penalty])

    condition_lr = InCondition(child=lr_C, parent=model_type, values=["logistic_regression"])
    condition_penalty = InCondition(child=lr_penalty, parent=model_type, values=["logistic_regression"])

    cs.add_conditions([condition_lr, condition_penalty])


    # --- Random Forest Hyperparameters ---
    rf_n_estimators = UniformIntegerHyperparameter(
        "random_forest:n_estimators", lower=10, upper=200, default_value=100,
        docstring="The number of trees in the forest."
    )
    rf_max_depth = UniformIntegerHyperparameter(
        "random_forest:max_depth", lower=2, upper=10, default_value=5,
        docstring="The maximum depth of the tree."
    )
    rf_min_samples_split = UniformIntegerHyperparameter(
        "random_forest:min_samples_split", lower=2, upper=10, default_value=2,
        docstring="The minimum number of samples required to split an internal node."
    )
    rf_min_samples_leaf = UniformIntegerHyperparameter(
        "random_forest:min_samples_leaf", lower=1, upper=10, default_value=1,
        docstring="The minimum number of samples required to be at a leaf node."
    )

    cs.add_hyperparameters([rf_n_estimators, rf_max_depth, rf_min_samples_split, rf_min_samples_leaf])

    condition_rf = InCondition(child=rf_n_estimators, parent=model_type, values=["random_forest"])
    condition_max_depth = InCondition(child=rf_max_depth, parent=model_type, values=["random_forest"])
    condition_min_samples_split = InCondition(child=rf_min_samples_split, parent=model_type, values=["random_forest"])
    condition_min_samples_leaf = InCondition(child=rf_min_samples_leaf, parent=model_type, values=["random_forest"])

    cs.add_conditions([condition_rf, condition_max_depth, condition_min_samples_split, condition_min_samples_leaf])



    # --- SVM Hyperparameters ---
    svm_C = UniformFloatHyperparameter(
        "svm:C", lower=1e-5, upper=10, default_value=1.0, log=True,
        docstring="Penalty parameter C of the error term."
    )
    svm_kernel = CategoricalHyperparameter(
        "svm:kernel", choices=["linear", "rbf", "poly", "sigmoid"], default_value="rbf",
        docstring="Specifies the kernel type to be used in the algorithm."
    )
    svm_gamma = UniformFloatHyperparameter(
        "svm:gamma", lower=1e-5, upper=1, default_value=0.1, log=True,
        docstring="Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’."
    )
    svm_degree = UniformIntegerHyperparameter(
        "svm:degree", lower=2, upper=5, default_value=3,
        docstring="Degree of the polynomial kernel function (‘poly’)."
    )


    cs.add_hyperparameters([svm_C, svm_kernel, svm_gamma, svm_degree])

    condition_svm = InCondition(child=svm_C, parent=model_type, values=["svm"])
    condition_kernel = InCondition(child=svm_kernel, parent=model_type, values=["svm"])
    condition_gamma = InCondition(child=svm_gamma, parent=model_type, values=["svm"])
    condition_degree = InCondition(child=svm_degree, parent=model_type, values=["svm"])

    cs.add_conditions([condition_svm, condition_kernel, condition_gamma, condition_degree])

    gamma_condition = InCondition(child=svm_gamma, parent=svm_kernel, values=["rbf", "poly", "sigmoid"])
    degree_condition = InCondition(child=svm_degree, parent=svm_kernel, values=["poly"])

    cs.add_conditions([gamma_condition, degree_condition])


    return cs
```
--------------------------------------------------------------------------------
[2025-06-20 11:26:57] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, InCondition

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization.
    """
    cs = ConfigurationSpace()

    # --- Model Choice ---
    model_type = CategoricalHyperparameter(
        "model_type", choices=["logistic_regression", "random_forest", "svm"], default_value="random_forest",
        docstring="The type of model to use."
    )
    cs.add_hyperparameter(model_type)

    # --- Logistic Regression Hyperparameters ---
    lr_C = UniformFloatHyperparameter(
        "logistic_regression:C", lower=1e-5, upper=10, default_value=1.0, log=True,
        docstring="Inverse of regularization strength. Smaller values specify stronger regularization."
    )
    lr_penalty = CategoricalHyperparameter(
        "logistic_regression:penalty", choices=["l1", "l2"], default_value="l2",
        docstring="The norm used in the penalization."
    )

    cs.add_hyperparameters([lr_C, lr_penalty])

    condition_lr = InCondition(child=lr_C, parent=model_type, values=["logistic_regression"])
    condition_penalty = InCondition(child=lr_penalty, parent=model_type, values=["logistic_regression"])

    cs.add_conditions([condition_lr, condition_penalty])


    # --- Random Forest Hyperparameters ---
    rf_n_estimators = UniformIntegerHyperparameter(
        "random_forest:n_estimators", lower=10, upper=200, default_value=100,
        docstring="The number of trees in the forest."
    )
    rf_max_depth = UniformIntegerHyperparameter(
        "random_forest:max_depth", lower=2, upper=10, default_value=5,
        docstring="The maximum depth of the tree."
    )
    rf_min_samples_split = UniformIntegerHyperparameter(
        "random_forest:min_samples_split", lower=2, upper=10, default_value=2,
        docstring="The minimum number of samples required to split an internal node."
    )
    rf_min_samples_leaf = UniformIntegerHyperparameter(
        "random_forest:min_samples_leaf", lower=1, upper=10, default_value=1,
        docstring="The minimum number of samples required to be at a leaf node."
    )

    cs.add_hyperparameters([rf_n_estimators, rf_max_depth, rf_min_samples_split, rf_min_samples_leaf])

    condition_rf = InCondition(child=rf_n_estimators, parent=model_type, values=["random_forest"])
    condition_max_depth = InCondition(child=rf_max_depth, parent=model_type, values=["random_forest"])
    condition_min_samples_split = InCondition(child=rf_min_samples_split, parent=model_type, values=["random_forest"])
    condition_min_samples_leaf = InCondition(child=rf_min_samples_leaf, parent=model_type, values=["random_forest"])

    cs.add_conditions([condition_rf, condition_max_depth, condition_min_samples_split, condition_min_samples_leaf])



    # --- SVM Hyperparameters ---
    svm_C = UniformFloatHyperparameter(
        "svm:C", lower=1e-5, upper=10, default_value=1.0, log=True,
        docstring="Penalty parameter C of the error term."
    )
    svm_kernel = CategoricalHyperparameter(
        "svm:kernel", choices=["linear", "rbf", "poly", "sigmoid"], default_value="rbf",
        docstring="Specifies the kernel type to be used in the algorithm."
    )
    svm_gamma = UniformFloatHyperparameter(
        "svm:gamma", lower=1e-5, upper=1, default_value=0.1, log=True,
        docstring="Kernel coefficient for ‘rbf’, ‘poly’ and ‘sigmoid’."
    )
    svm_degree = UniformIntegerHyperparameter(
        "svm:degree", lower=2, upper=5, default_value=3,
        docstring="Degree of the polynomial kernel function (‘poly’)."
    )


    cs.add_hyperparameters([svm_C, svm_kernel, svm_gamma, svm_degree])

    condition_svm = InCondition(child=svm_C, parent=model_type, values=["svm"])
    condition_kernel = InCondition(child=svm_kernel, parent=model_type, values=["svm"])
    condition_gamma = InCondition(child=svm_gamma, parent=model_type, values=["svm"])
    condition_degree = InCondition(child=svm_degree, parent=model_type, values=["svm"])

    cs.add_conditions([condition_svm, condition_kernel, condition_gamma, condition_degree])

    gamma_condition = InCondition(child=svm_gamma, parent=svm_kernel, values=["rbf", "poly", "sigmoid"])
    degree_condition = InCondition(child=svm_degree, parent=svm_kernel, values=["poly"])

    cs.add_conditions([gamma_condition, degree_condition])


    return cs

--------------------------------------------------------------------------------
[2025-06-20 11:26:57] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#1): CategoricalHyperparameter.__init__() got an unexpected keyword argument 'docstring'
--------------------------------------------------------------------------------
[2025-06-20 11:27:02] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, InCondition

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization.
    """
    cs = ConfigurationSpace()

    # --- Model Choice ---
    model_type = CategoricalHyperparameter(
        "model_type", choices=["logistic_regression", "random_forest", "svm"], default_value="random_forest"
    )
    cs.add_hyperparameter(model_type)

    # --- Logistic Regression Hyperparameters ---
    lr_C = UniformFloatHyperparameter(
        "logistic_regression:C", lower=1e-5, upper=10, default_value=1.0, log=True
    )
    lr_penalty = CategoricalHyperparameter(
        "logistic_regression:penalty", choices=["l1", "l2"], default_value="l2"
    )

    cs.add_hyperparameters([lr_C, lr_penalty])

    condition_lr = InCondition(child=lr_C, parent=model_type, values=["logistic_regression"])
    condition_penalty = InCondition(child=lr_penalty, parent=model_type, values=["logistic_regression"])

    cs.add_conditions([condition_lr, condition_penalty])


    # --- Random Forest Hyperparameters ---
    rf_n_estimators = UniformIntegerHyperparameter(
        "random_forest:n_estimators", lower=10, upper=200, default_value=100
    )
    rf_max_depth = UniformIntegerHyperparameter(
        "random_forest:max_depth", lower=2, upper=10, default_value=5
    )
    rf_min_samples_split = UniformIntegerHyperparameter(
        "random_forest:min_samples_split", lower=2, upper=10, default_value=2
    )
    rf_min_samples_leaf = UniformIntegerHyperparameter(
        "random_forest:min_samples_leaf", lower=1, upper=10, default_value=1
    )

    cs.add_hyperparameters([rf_n_estimators, rf_max_depth, rf_min_samples_split, rf_min_samples_leaf])

    condition_rf = InCondition(child=rf_n_estimators, parent=model_type, values=["random_forest"])
    condition_max_depth = InCondition(child=rf_max_depth, parent=model_type, values=["random_forest"])
    condition_min_samples_split = InCondition(child=rf_min_samples_split, parent=model_type, values=["random_forest"])
    condition_min_samples_leaf = InCondition(child=rf_min_samples_leaf, parent=model_type, values=["random_forest"])

    cs.add_conditions([condition_rf, condition_max_depth, condition_min_samples_split, condition_min_samples_leaf])



    # --- SVM Hyperparameters ---
    svm_C = UniformFloatHyperparameter(
        "svm:C", lower=1e-5, upper=10, default_value=1.0, log=True
    )
    svm_kernel = CategoricalHyperparameter(
        "svm:kernel", choices=["linear", "rbf", "poly", "sigmoid"], default_value="rbf"
    )
    svm_gamma = UniformFloatHyperparameter(
        "svm:gamma", lower=1e-5, upper=1, default_value=0.1, log=True
    )
    svm_degree = UniformIntegerHyperparameter(
        "svm:degree", lower=2, upper=5, default_value=3
    )


    cs.add_hyperparameters([svm_C, svm_kernel, svm_gamma, svm_degree])

    condition_svm = InCondition(child=svm_C, parent=model_type, values=["svm"])
    condition_kernel = InCondition(child=svm_kernel, parent=model_type, values=["svm"])
    condition_gamma = InCondition(child=svm_gamma, parent=model_type, values=["svm"])
    condition_degree = InCondition(child=svm_degree, parent=model_type, values=["svm"])

    cs.add_conditions([condition_svm, condition_kernel, condition_gamma, condition_degree])

    gamma_condition = InCondition(child=svm_gamma, parent=svm_kernel, values=["rbf", "poly", "sigmoid"])
    degree_condition = InCondition(child=svm_degree, parent=svm_kernel, values=["poly"])

    cs.add_conditions([gamma_condition, degree_condition])


    return cs
```
--------------------------------------------------------------------------------
[2025-06-20 11:27:02] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, InCondition

def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization.
    """
    cs = ConfigurationSpace()

    # --- Model Choice ---
    model_type = CategoricalHyperparameter(
        "model_type", choices=["logistic_regression", "random_forest", "svm"], default_value="random_forest"
    )
    cs.add_hyperparameter(model_type)

    # --- Logistic Regression Hyperparameters ---
    lr_C = UniformFloatHyperparameter(
        "logistic_regression:C", lower=1e-5, upper=10, default_value=1.0, log=True
    )
    lr_penalty = CategoricalHyperparameter(
        "logistic_regression:penalty", choices=["l1", "l2"], default_value="l2"
    )

    cs.add_hyperparameters([lr_C, lr_penalty])

    condition_lr = InCondition(child=lr_C, parent=model_type, values=["logistic_regression"])
    condition_penalty = InCondition(child=lr_penalty, parent=model_type, values=["logistic_regression"])

    cs.add_conditions([condition_lr, condition_penalty])


    # --- Random Forest Hyperparameters ---
    rf_n_estimators = UniformIntegerHyperparameter(
        "random_forest:n_estimators", lower=10, upper=200, default_value=100
    )
    rf_max_depth = UniformIntegerHyperparameter(
        "random_forest:max_depth", lower=2, upper=10, default_value=5
    )
    rf_min_samples_split = UniformIntegerHyperparameter(
        "random_forest:min_samples_split", lower=2, upper=10, default_value=2
    )
    rf_min_samples_leaf = UniformIntegerHyperparameter(
        "random_forest:min_samples_leaf", lower=1, upper=10, default_value=1
    )

    cs.add_hyperparameters([rf_n_estimators, rf_max_depth, rf_min_samples_split, rf_min_samples_leaf])

    condition_rf = InCondition(child=rf_n_estimators, parent=model_type, values=["random_forest"])
    condition_max_depth = InCondition(child=rf_max_depth, parent=model_type, values=["random_forest"])
    condition_min_samples_split = InCondition(child=rf_min_samples_split, parent=model_type, values=["random_forest"])
    condition_min_samples_leaf = InCondition(child=rf_min_samples_leaf, parent=model_type, values=["random_forest"])

    cs.add_conditions([condition_rf, condition_max_depth, condition_min_samples_split, condition_min_samples_leaf])



    # --- SVM Hyperparameters ---
    svm_C = UniformFloatHyperparameter(
        "svm:C", lower=1e-5, upper=10, default_value=1.0, log=True
    )
    svm_kernel = CategoricalHyperparameter(
        "svm:kernel", choices=["linear", "rbf", "poly", "sigmoid"], default_value="rbf"
    )
    svm_gamma = UniformFloatHyperparameter(
        "svm:gamma", lower=1e-5, upper=1, default_value=0.1, log=True
    )
    svm_degree = UniformIntegerHyperparameter(
        "svm:degree", lower=2, upper=5, default_value=3
    )


    cs.add_hyperparameters([svm_C, svm_kernel, svm_gamma, svm_degree])

    condition_svm = InCondition(child=svm_C, parent=model_type, values=["svm"])
    condition_kernel = InCondition(child=svm_kernel, parent=model_type, values=["svm"])
    condition_gamma = InCondition(child=svm_gamma, parent=model_type, values=["svm"])
    condition_degree = InCondition(child=svm_degree, parent=model_type, values=["svm"])

    cs.add_conditions([condition_svm, condition_kernel, condition_gamma, condition_degree])

    gamma_condition = InCondition(child=svm_gamma, parent=svm_kernel, values=["rbf", "poly", "sigmoid"])
    degree_condition = InCondition(child=svm_degree, parent=svm_kernel, values=["poly"])

    cs.add_conditions([gamma_condition, degree_condition])


    return cs

--------------------------------------------------------------------------------
[2025-06-20 11:27:02] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#2): Adding a second parent condition for a for a hyperparameter is ambiguous and therefore forbidden. Use an `OrConjunction` or `AndConjunction` to combine conditions instead.
Already inserted: svm:gamma | model_type in {'svm'}
New one: svm:gamma | svm:kernel in {'rbf', 'poly', 'sigmoid'}
--------------------------------------------------------------------------------
[2025-06-20 11:27:07] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, InCondition
from ConfigSpace.forbidden import ForbiddenEqualsClause, ForbiddenAndConjunction


def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization.
    """
    cs = ConfigurationSpace()

    # --- Model Choice ---
    model_type = CategoricalHyperparameter(
        "model_type", choices=["logistic_regression", "random_forest", "svm"], default_value="random_forest"
    )
    cs.add_hyperparameter(model_type)

    # --- Logistic Regression Hyperparameters ---
    lr_C = UniformFloatHyperparameter(
        "logistic_regression:C", lower=1e-5, upper=10, default_value=1.0, log=True
    )
    lr_penalty = CategoricalHyperparameter(
        "logistic_regression:penalty", choices=["l1", "l2"], default_value="l2"
    )

    cs.add_hyperparameters([lr_C, lr_penalty])

    condition_lr = InCondition(child=lr_C, parent=model_type, values=["logistic_regression"])
    condition_penalty = InCondition(child=lr_penalty, parent=model_type, values=["logistic_regression"])

    cs.add_conditions([condition_lr, condition_penalty])


    # --- Random Forest Hyperparameters ---
    rf_n_estimators = UniformIntegerHyperparameter(
        "random_forest:n_estimators", lower=10, upper=200, default_value=100
    )
    rf_max_depth = UniformIntegerHyperparameter(
        "random_forest:max_depth", lower=2, upper=10, default_value=5
    )
    rf_min_samples_split = UniformIntegerHyperparameter(
        "random_forest:min_samples_split", lower=2, upper=10, default_value=2
    )
    rf_min_samples_leaf = UniformIntegerHyperparameter(
        "random_forest:min_samples_leaf", lower=1, upper=10, default_value=1
    )

    cs.add_hyperparameters([rf_n_estimators, rf_max_depth, rf_min_samples_split, rf_min_samples_leaf])

    condition_rf = InCondition(child=rf_n_estimators, parent=model_type, values=["random_forest"])
    condition_max_depth = InCondition(child=rf_max_depth, parent=model_type, values=["random_forest"])
    condition_min_samples_split = InCondition(child=rf_min_samples_split, parent=model_type, values=["random_forest"])
    condition_min_samples_leaf = InCondition(child=rf_min_samples_leaf, parent=model_type, values=["random_forest"])

    cs.add_conditions([condition_rf, condition_max_depth, condition_min_samples_split, condition_min_samples_leaf])



    # --- SVM Hyperparameters ---
    svm_C = UniformFloatHyperparameter(
        "svm:C", lower=1e-5, upper=10, default_value=1.0, log=True
    )
    svm_kernel = CategoricalHyperparameter(
        "svm:kernel", choices=["linear", "rbf", "poly", "sigmoid"], default_value="rbf"
    )
    svm_gamma = UniformFloatHyperparameter(
        "svm:gamma", lower=1e-5, upper=1, default_value=0.1, log=True
    )
    svm_degree = UniformIntegerHyperparameter(
        "svm:degree", lower=2, upper=5, default_value=3
    )


    cs.add_hyperparameters([svm_C, svm_kernel, svm_gamma, svm_degree])

    condition_svm = InCondition(child=svm_C, parent=model_type, values=["svm"])
    condition_kernel = InCondition(child=svm_kernel, parent=model_type, values=["svm"])
    condition_gamma = InCondition(child=svm_gamma, parent=model_type, values=["svm"])
    condition_degree = InCondition(child=svm_degree, parent=model_type, values=["svm"])

    cs.add_conditions([condition_svm, condition_kernel, condition_gamma, condition_degree])
    
    # Fix for "Adding a second parent condition" error
    forbidden_gamma = ForbiddenAndConjunction(
        ForbiddenEqualsClause(svm_kernel, "linear"),
        ForbiddenEqualsClause(svm_gamma, svm_gamma.default_value) 
    )
    
    cs.add_forbidden_clauses([forbidden_gamma])
    
    gamma_condition = InCondition(child=svm_gamma, parent=svm_kernel, values=["rbf", "poly", "sigmoid"])
    degree_condition = InCondition(child=svm_degree, parent=svm_kernel, values=["poly"])

    cs.add_conditions([gamma_condition, degree_condition])


    return cs
```
--------------------------------------------------------------------------------
[2025-06-20 11:27:07] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, InCondition
from ConfigSpace.forbidden import ForbiddenEqualsClause, ForbiddenAndConjunction


def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization.
    """
    cs = ConfigurationSpace()

    # --- Model Choice ---
    model_type = CategoricalHyperparameter(
        "model_type", choices=["logistic_regression", "random_forest", "svm"], default_value="random_forest"
    )
    cs.add_hyperparameter(model_type)

    # --- Logistic Regression Hyperparameters ---
    lr_C = UniformFloatHyperparameter(
        "logistic_regression:C", lower=1e-5, upper=10, default_value=1.0, log=True
    )
    lr_penalty = CategoricalHyperparameter(
        "logistic_regression:penalty", choices=["l1", "l2"], default_value="l2"
    )

    cs.add_hyperparameters([lr_C, lr_penalty])

    condition_lr = InCondition(child=lr_C, parent=model_type, values=["logistic_regression"])
    condition_penalty = InCondition(child=lr_penalty, parent=model_type, values=["logistic_regression"])

    cs.add_conditions([condition_lr, condition_penalty])


    # --- Random Forest Hyperparameters ---
    rf_n_estimators = UniformIntegerHyperparameter(
        "random_forest:n_estimators", lower=10, upper=200, default_value=100
    )
    rf_max_depth = UniformIntegerHyperparameter(
        "random_forest:max_depth", lower=2, upper=10, default_value=5
    )
    rf_min_samples_split = UniformIntegerHyperparameter(
        "random_forest:min_samples_split", lower=2, upper=10, default_value=2
    )
    rf_min_samples_leaf = UniformIntegerHyperparameter(
        "random_forest:min_samples_leaf", lower=1, upper=10, default_value=1
    )

    cs.add_hyperparameters([rf_n_estimators, rf_max_depth, rf_min_samples_split, rf_min_samples_leaf])

    condition_rf = InCondition(child=rf_n_estimators, parent=model_type, values=["random_forest"])
    condition_max_depth = InCondition(child=rf_max_depth, parent=model_type, values=["random_forest"])
    condition_min_samples_split = InCondition(child=rf_min_samples_split, parent=model_type, values=["random_forest"])
    condition_min_samples_leaf = InCondition(child=rf_min_samples_leaf, parent=model_type, values=["random_forest"])

    cs.add_conditions([condition_rf, condition_max_depth, condition_min_samples_split, condition_min_samples_leaf])



    # --- SVM Hyperparameters ---
    svm_C = UniformFloatHyperparameter(
        "svm:C", lower=1e-5, upper=10, default_value=1.0, log=True
    )
    svm_kernel = CategoricalHyperparameter(
        "svm:kernel", choices=["linear", "rbf", "poly", "sigmoid"], default_value="rbf"
    )
    svm_gamma = UniformFloatHyperparameter(
        "svm:gamma", lower=1e-5, upper=1, default_value=0.1, log=True
    )
    svm_degree = UniformIntegerHyperparameter(
        "svm:degree", lower=2, upper=5, default_value=3
    )


    cs.add_hyperparameters([svm_C, svm_kernel, svm_gamma, svm_degree])

    condition_svm = InCondition(child=svm_C, parent=model_type, values=["svm"])
    condition_kernel = InCondition(child=svm_kernel, parent=model_type, values=["svm"])
    condition_gamma = InCondition(child=svm_gamma, parent=model_type, values=["svm"])
    condition_degree = InCondition(child=svm_degree, parent=model_type, values=["svm"])

    cs.add_conditions([condition_svm, condition_kernel, condition_gamma, condition_degree])
    
    # Fix for "Adding a second parent condition" error
    forbidden_gamma = ForbiddenAndConjunction(
        ForbiddenEqualsClause(svm_kernel, "linear"),
        ForbiddenEqualsClause(svm_gamma, svm_gamma.default_value) 
    )
    
    cs.add_forbidden_clauses([forbidden_gamma])
    
    gamma_condition = InCondition(child=svm_gamma, parent=svm_kernel, values=["rbf", "poly", "sigmoid"])
    degree_condition = InCondition(child=svm_degree, parent=svm_kernel, values=["poly"])

    cs.add_conditions([gamma_condition, degree_condition])


    return cs

--------------------------------------------------------------------------------
[2025-06-20 11:27:07] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#3): Adding a second parent condition for a for a hyperparameter is ambiguous and therefore forbidden. Use an `OrConjunction` or `AndConjunction` to combine conditions instead.
Already inserted: svm:gamma | model_type in {'svm'}
New one: svm:gamma | svm:kernel in {'rbf', 'poly', 'sigmoid'}
--------------------------------------------------------------------------------
[2025-06-20 11:27:13] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, InCondition
from ConfigSpace.forbidden import ForbiddenEqualsClause, ForbiddenAndConjunction


def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization.
    """
    cs = ConfigurationSpace()

    # --- Model Choice ---
    model_type = CategoricalHyperparameter(
        "model_type", choices=["logistic_regression", "random_forest", "svm"], default_value="random_forest"
    )
    cs.add_hyperparameter(model_type)

    # --- Logistic Regression Hyperparameters ---
    lr_C = UniformFloatHyperparameter(
        "logistic_regression:C", lower=1e-5, upper=10, default_value=1.0, log=True
    )
    lr_penalty = CategoricalHyperparameter(
        "logistic_regression:penalty", choices=["l1", "l2"], default_value="l2"
    )

    cs.add_hyperparameters([lr_C, lr_penalty])

    condition_lr = InCondition(child=lr_C, parent=model_type, values=["logistic_regression"])
    condition_penalty = InCondition(child=lr_penalty, parent=model_type, values=["logistic_regression"])

    cs.add_conditions([condition_lr, condition_penalty])


    # --- Random Forest Hyperparameters ---
    rf_n_estimators = UniformIntegerHyperparameter(
        "random_forest:n_estimators", lower=10, upper=200, default_value=100
    )
    rf_max_depth = UniformIntegerHyperparameter(
        "random_forest:max_depth", lower=2, upper=10, default_value=5
    )
    rf_min_samples_split = UniformIntegerHyperparameter(
        "random_forest:min_samples_split", lower=2, upper=10, default_value=2
    )
    rf_min_samples_leaf = UniformIntegerHyperparameter(
        "random_forest:min_samples_leaf", lower=1, upper=10, default_value=1
    )

    cs.add_hyperparameters([rf_n_estimators, rf_max_depth, rf_min_samples_split, rf_min_samples_leaf])

    condition_rf = InCondition(child=rf_n_estimators, parent=model_type, values=["random_forest"])
    condition_max_depth = InCondition(child=rf_max_depth, parent=model_type, values=["random_forest"])
    condition_min_samples_split = InCondition(child=rf_min_samples_split, parent=model_type, values=["random_forest"])
    condition_min_samples_leaf = InCondition(child=rf_min_samples_leaf, parent=model_type, values=["random_forest"])

    cs.add_conditions([condition_rf, condition_max_depth, condition_min_samples_split, condition_min_samples_leaf])



    # --- SVM Hyperparameters ---
    svm_C = UniformFloatHyperparameter(
        "svm:C", lower=1e-5, upper=10, default_value=1.0, log=True
    )
    svm_kernel = CategoricalHyperparameter(
        "svm:kernel", choices=["linear", "rbf", "poly", "sigmoid"], default_value="rbf"
    )
    svm_gamma = UniformFloatHyperparameter(
        "svm:gamma", lower=1e-5, upper=1, default_value=0.1, log=True
    )
    svm_degree = UniformIntegerHyperparameter(
        "svm:degree", lower=2, upper=5, default_value=3
    )


    cs.add_hyperparameters([svm_C, svm_kernel, svm_gamma, svm_degree])

    condition_svm = InCondition(child=svm_C, parent=model_type, values=["svm"])
    condition_kernel = InCondition(child=svm_kernel, parent=model_type, values=["svm"])


    cs.add_conditions([condition_svm, condition_kernel])
    
    gamma_condition = InCondition(child=svm_gamma, parent=model_type, values=["svm"])
    degree_condition = InCondition(child=svm_degree, parent=model_type, values=["svm"])

    cs.add_conditions([gamma_condition, degree_condition])


    # Fix for "Adding a second parent condition" error
    forbidden_degree = ForbiddenAndConjunction(
        ForbiddenEqualsClause(svm_kernel, "linear"),
        ForbiddenEqualsClause(svm_degree, 3)
    )
    cs.add_forbidden_clauses([forbidden_degree])

    forbidden_degree_sig = ForbiddenAndConjunction(
        ForbiddenEqualsClause(svm_kernel, "sigmoid"),
        ForbiddenEqualsClause(svm_degree, 3)
    )
    cs.add_forbidden_clauses([forbidden_degree_sig])

    forbidden_gamma = ForbiddenAndConjunction(
        ForbiddenEqualsClause(svm_kernel, "linear"),
        ForbiddenEqualsClause(svm_gamma, 0.1)
    )

    cs.add_forbidden_clauses([forbidden_gamma])

    return cs
```
--------------------------------------------------------------------------------
[2025-06-20 11:27:13] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, InCondition
from ConfigSpace.forbidden import ForbiddenEqualsClause, ForbiddenAndConjunction


def get_configspace() -> ConfigurationSpace:
    """
    Returns a ConfigurationSpace object for hyperparameter optimization.
    """
    cs = ConfigurationSpace()

    # --- Model Choice ---
    model_type = CategoricalHyperparameter(
        "model_type", choices=["logistic_regression", "random_forest", "svm"], default_value="random_forest"
    )
    cs.add_hyperparameter(model_type)

    # --- Logistic Regression Hyperparameters ---
    lr_C = UniformFloatHyperparameter(
        "logistic_regression:C", lower=1e-5, upper=10, default_value=1.0, log=True
    )
    lr_penalty = CategoricalHyperparameter(
        "logistic_regression:penalty", choices=["l1", "l2"], default_value="l2"
    )

    cs.add_hyperparameters([lr_C, lr_penalty])

    condition_lr = InCondition(child=lr_C, parent=model_type, values=["logistic_regression"])
    condition_penalty = InCondition(child=lr_penalty, parent=model_type, values=["logistic_regression"])

    cs.add_conditions([condition_lr, condition_penalty])


    # --- Random Forest Hyperparameters ---
    rf_n_estimators = UniformIntegerHyperparameter(
        "random_forest:n_estimators", lower=10, upper=200, default_value=100
    )
    rf_max_depth = UniformIntegerHyperparameter(
        "random_forest:max_depth", lower=2, upper=10, default_value=5
    )
    rf_min_samples_split = UniformIntegerHyperparameter(
        "random_forest:min_samples_split", lower=2, upper=10, default_value=2
    )
    rf_min_samples_leaf = UniformIntegerHyperparameter(
        "random_forest:min_samples_leaf", lower=1, upper=10, default_value=1
    )

    cs.add_hyperparameters([rf_n_estimators, rf_max_depth, rf_min_samples_split, rf_min_samples_leaf])

    condition_rf = InCondition(child=rf_n_estimators, parent=model_type, values=["random_forest"])
    condition_max_depth = InCondition(child=rf_max_depth, parent=model_type, values=["random_forest"])
    condition_min_samples_split = InCondition(child=rf_min_samples_split, parent=model_type, values=["random_forest"])
    condition_min_samples_leaf = InCondition(child=rf_min_samples_leaf, parent=model_type, values=["random_forest"])

    cs.add_conditions([condition_rf, condition_max_depth, condition_min_samples_split, condition_min_samples_leaf])



    # --- SVM Hyperparameters ---
    svm_C = UniformFloatHyperparameter(
        "svm:C", lower=1e-5, upper=10, default_value=1.0, log=True
    )
    svm_kernel = CategoricalHyperparameter(
        "svm:kernel", choices=["linear", "rbf", "poly", "sigmoid"], default_value="rbf"
    )
    svm_gamma = UniformFloatHyperparameter(
        "svm:gamma", lower=1e-5, upper=1, default_value=0.1, log=True
    )
    svm_degree = UniformIntegerHyperparameter(
        "svm:degree", lower=2, upper=5, default_value=3
    )


    cs.add_hyperparameters([svm_C, svm_kernel, svm_gamma, svm_degree])

    condition_svm = InCondition(child=svm_C, parent=model_type, values=["svm"])
    condition_kernel = InCondition(child=svm_kernel, parent=model_type, values=["svm"])


    cs.add_conditions([condition_svm, condition_kernel])
    
    gamma_condition = InCondition(child=svm_gamma, parent=model_type, values=["svm"])
    degree_condition = InCondition(child=svm_degree, parent=model_type, values=["svm"])

    cs.add_conditions([gamma_condition, degree_condition])


    # Fix for "Adding a second parent condition" error
    forbidden_degree = ForbiddenAndConjunction(
        ForbiddenEqualsClause(svm_kernel, "linear"),
        ForbiddenEqualsClause(svm_degree, 3)
    )
    cs.add_forbidden_clauses([forbidden_degree])

    forbidden_degree_sig = ForbiddenAndConjunction(
        ForbiddenEqualsClause(svm_kernel, "sigmoid"),
        ForbiddenEqualsClause(svm_degree, 3)
    )
    cs.add_forbidden_clauses([forbidden_degree_sig])

    forbidden_gamma = ForbiddenAndConjunction(
        ForbiddenEqualsClause(svm_kernel, "linear"),
        ForbiddenEqualsClause(svm_gamma, 0.1)
    )

    cs.add_forbidden_clauses([forbidden_gamma])

    return cs

--------------------------------------------------------------------------------
[2025-06-20 11:27:13] [Metadata: {'component': 'config', 'status': 'success'}] Configuration space generated successfully
--------------------------------------------------------------------------------
[2025-06-20 11:27:14] [Metadata: {'component': 'scenario'}] ```python
from smac import Scenario
from ConfigSpace import ConfigurationSpace

def generate_scenario(cs: ConfigurationSpace) -> Scenario:
    scenario = Scenario(
        configspace=cs,
        name="HyperparameterOptimization",
        output_directory="./logs/gemini-2.0-flash_credit-g_20250620_112642",
        deterministic=False,
        n_trials=10,
        n_workers=1,
        min_budget=1,
        max_budget=3
    )
    return scenario
```
--------------------------------------------------------------------------------
[2025-06-20 11:27:14] [Metadata: {'component': 'scenario', 'action': 'run'}] from smac import Scenario
from ConfigSpace import ConfigurationSpace

def generate_scenario(cs: ConfigurationSpace) -> Scenario:
    scenario = Scenario(
        configspace=cs,
        name="HyperparameterOptimization",
        output_directory="./logs/gemini-2.0-flash_credit-g_20250620_112642",
        deterministic=False,
        n_trials=10,
        n_workers=1,
        min_budget=1,
        max_budget=3
    )
    return scenario

--------------------------------------------------------------------------------
[2025-06-20 11:27:14] [Metadata: {'component': 'scenario', 'status': 'success'}] Scenario generated successfully
--------------------------------------------------------------------------------
[2025-06-20 11:27:18] [Metadata: {'component': 'train_function'}] ```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from ConfigSpace import Configuration
from typing import Any

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A Configuration object containing the hyperparameters.
        dataset (Any): A dictionary containing the training data ('X' and 'y').
        seed (int): Random seed for reproducibility.

    Returns:
        float: Negative validation accuracy.
    """
    np.random.seed(seed)

    X = dataset['X']
    y = dataset['y']

    # Preprocessing: Label Encoding and Scaling
    for col in X.select_dtypes(include='object').columns:
        X[col] = X[col].astype('category')
        X[col] = X[col].cat.codes

    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # Split data
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)

    model_type = cfg.get('model_type')

    if model_type == 'logistic_regression':
        C = cfg.get('logistic_regression:C')
        penalty = cfg.get('logistic_regression:penalty')
        model = LogisticRegression(C=C, penalty=penalty, solver='liblinear', random_state=seed)
    elif model_type == 'random_forest':
        n_estimators = cfg.get('random_forest:n_estimators')
        max_depth = cfg.get('random_forest:max_depth')
        min_samples_split = cfg.get('random_forest:min_samples_split')
        min_samples_leaf = cfg.get('random_forest:min_samples_leaf')
        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                       min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,
                                       random_state=seed, n_jobs=-1)
    elif model_type == 'svm':
        C = cfg.get('svm:C')
        kernel = cfg.get('svm:kernel')
        gamma = cfg.get('svm:gamma')
        degree = cfg.get('svm:degree')
        model = SVC(C=C, kernel=kernel, gamma=gamma, degree=degree, random_state=seed)
    else:
        raise ValueError(f"Unknown model type: {model_type}")

    # Train model
    model.fit(X_train, y_train)

    # Evaluate model
    y_pred = model.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred)

    return -accuracy
```
--------------------------------------------------------------------------------
[2025-06-20 11:27:18] [Metadata: {'component': 'train_function', 'action': 'run'}] import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from ConfigSpace import Configuration
from typing import Any

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A Configuration object containing the hyperparameters.
        dataset (Any): A dictionary containing the training data ('X' and 'y').
        seed (int): Random seed for reproducibility.

    Returns:
        float: Negative validation accuracy.
    """
    np.random.seed(seed)

    X = dataset['X']
    y = dataset['y']

    # Preprocessing: Label Encoding and Scaling
    for col in X.select_dtypes(include='object').columns:
        X[col] = X[col].astype('category')
        X[col] = X[col].cat.codes

    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # Split data
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)

    model_type = cfg.get('model_type')

    if model_type == 'logistic_regression':
        C = cfg.get('logistic_regression:C')
        penalty = cfg.get('logistic_regression:penalty')
        model = LogisticRegression(C=C, penalty=penalty, solver='liblinear', random_state=seed)
    elif model_type == 'random_forest':
        n_estimators = cfg.get('random_forest:n_estimators')
        max_depth = cfg.get('random_forest:max_depth')
        min_samples_split = cfg.get('random_forest:min_samples_split')
        min_samples_leaf = cfg.get('random_forest:min_samples_leaf')
        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                       min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,
                                       random_state=seed, n_jobs=-1)
    elif model_type == 'svm':
        C = cfg.get('svm:C')
        kernel = cfg.get('svm:kernel')
        gamma = cfg.get('svm:gamma')
        degree = cfg.get('svm:degree')
        model = SVC(C=C, kernel=kernel, gamma=gamma, degree=degree, random_state=seed)
    else:
        raise ValueError(f"Unknown model type: {model_type}")

    # Train model
    model.fit(X_train, y_train)

    # Evaluate model
    y_pred = model.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred)

    return -accuracy

--------------------------------------------------------------------------------
[2025-06-20 11:27:18] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#1): could not convert string to float: '<0'
--------------------------------------------------------------------------------
[2025-06-20 11:27:22] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from ConfigSpace import Configuration
from typing import Any

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A Configuration object containing the hyperparameters.
        dataset (Any): A dictionary containing the training data ('X' and 'y').
        seed (int): Random seed for reproducibility.

    Returns:
        float: Negative validation accuracy.
    """
    np.random.seed(seed)

    X = dataset['X'].copy()
    y = dataset['y'].copy()

    # Preprocessing: Label Encoding and Scaling
    for col in X.select_dtypes(include='object').columns:
        X[col] = X[col].astype(str)
        try:
            X[col] = pd.to_numeric(X[col], errors='raise')
        except ValueError:
            X[col] = X[col].astype('category')
            X[col] = X[col].cat.codes

    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # Split data
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)

    model_type = cfg.get('model_type')

    if model_type == 'logistic_regression':
        C = cfg.get('logistic_regression:C')
        penalty = cfg.get('logistic_regression:penalty')
        model = LogisticRegression(C=C, penalty=penalty, solver='liblinear', random_state=seed)
    elif model_type == 'random_forest':
        n_estimators = cfg.get('random_forest:n_estimators')
        max_depth = cfg.get('random_forest:max_depth')
        min_samples_split = cfg.get('random_forest:min_samples_split')
        min_samples_leaf = cfg.get('random_forest:min_samples_leaf')
        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                       min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,
                                       random_state=seed, n_jobs=-1)
    elif model_type == 'svm':
        C = cfg.get('svm:C')
        kernel = cfg.get('svm:kernel')
        gamma = cfg.get('svm:gamma')
        degree = cfg.get('svm:degree')
        model = SVC(C=C, kernel=kernel, gamma=gamma, degree=degree, random_state=seed)
    else:
        raise ValueError(f"Unknown model type: {{model_type}}")

    # Train model
    model.fit(X_train, y_train)

    # Evaluate model
    y_pred = model.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred)

    return -accuracy
```
--------------------------------------------------------------------------------
[2025-06-20 11:27:22] [Metadata: {'component': 'train_function', 'action': 'run'}] import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from ConfigSpace import Configuration
from typing import Any

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A Configuration object containing the hyperparameters.
        dataset (Any): A dictionary containing the training data ('X' and 'y').
        seed (int): Random seed for reproducibility.

    Returns:
        float: Negative validation accuracy.
    """
    np.random.seed(seed)

    X = dataset['X'].copy()
    y = dataset['y'].copy()

    # Preprocessing: Label Encoding and Scaling
    for col in X.select_dtypes(include='object').columns:
        X[col] = X[col].astype(str)
        try:
            X[col] = pd.to_numeric(X[col], errors='raise')
        except ValueError:
            X[col] = X[col].astype('category')
            X[col] = X[col].cat.codes

    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # Split data
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)

    model_type = cfg.get('model_type')

    if model_type == 'logistic_regression':
        C = cfg.get('logistic_regression:C')
        penalty = cfg.get('logistic_regression:penalty')
        model = LogisticRegression(C=C, penalty=penalty, solver='liblinear', random_state=seed)
    elif model_type == 'random_forest':
        n_estimators = cfg.get('random_forest:n_estimators')
        max_depth = cfg.get('random_forest:max_depth')
        min_samples_split = cfg.get('random_forest:min_samples_split')
        min_samples_leaf = cfg.get('random_forest:min_samples_leaf')
        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                       min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,
                                       random_state=seed, n_jobs=-1)
    elif model_type == 'svm':
        C = cfg.get('svm:C')
        kernel = cfg.get('svm:kernel')
        gamma = cfg.get('svm:gamma')
        degree = cfg.get('svm:degree')
        model = SVC(C=C, kernel=kernel, gamma=gamma, degree=degree, random_state=seed)
    else:
        raise ValueError(f"Unknown model type: {{model_type}}")

    # Train model
    model.fit(X_train, y_train)

    # Evaluate model
    y_pred = model.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred)

    return -accuracy

--------------------------------------------------------------------------------
[2025-06-20 11:27:22] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#2): could not convert string to float: '<0'
--------------------------------------------------------------------------------
[2025-06-20 11:27:26] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from ConfigSpace import Configuration
from typing import Any

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A Configuration object containing the hyperparameters.
        dataset (Any): A dictionary containing the training data ('X' and 'y').
        seed (int): Random seed for reproducibility.

    Returns:
        float: Negative validation accuracy.
    """
    np.random.seed(seed)

    X = dataset['X'].copy()
    y = dataset['y'].copy()

    # Preprocessing: Label Encoding and Scaling
    for col in X.select_dtypes(include='object').columns:
        X[col] = X[col].astype(str)
        try:
            X[col] = pd.to_numeric(X[col], errors='raise')
        except ValueError:
            X[col] = X[col].astype('category')
            X[col] = X[col].cat.codes

    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # Split data
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)

    model_type = cfg.get('model_type')

    if model_type == 'logistic_regression':
        C = cfg.get('logistic_regression:C')
        penalty = cfg.get('logistic_regression:penalty')
        model = LogisticRegression(C=C, penalty=penalty, solver='liblinear', random_state=seed)
    elif model_type == 'random_forest':
        n_estimators = cfg.get('random_forest:n_estimators')
        max_depth = cfg.get('random_forest:max_depth')
        min_samples_split = cfg.get('random_forest:min_samples_split')
        min_samples_leaf = cfg.get('random_forest:min_samples_leaf')
        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                       min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,
                                       random_state=seed, n_jobs=-1)
    elif model_type == 'svm':
        C = cfg.get('svm:C')
        kernel = cfg.get('svm:kernel')
        gamma = cfg.get('svm:gamma')
        degree = cfg.get('svm:degree')
        model = SVC(C=C, kernel=kernel, gamma=gamma, degree=degree, random_state=seed)
    else:
        raise ValueError(f"Unknown model type: {model_type}")

    # Train model
    model.fit(X_train, y_train)

    # Evaluate model
    y_pred = model.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred)

    return -accuracy
```
--------------------------------------------------------------------------------
[2025-06-20 11:27:26] [Metadata: {'component': 'train_function', 'action': 'run'}] import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from ConfigSpace import Configuration
from typing import Any

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A Configuration object containing the hyperparameters.
        dataset (Any): A dictionary containing the training data ('X' and 'y').
        seed (int): Random seed for reproducibility.

    Returns:
        float: Negative validation accuracy.
    """
    np.random.seed(seed)

    X = dataset['X'].copy()
    y = dataset['y'].copy()

    # Preprocessing: Label Encoding and Scaling
    for col in X.select_dtypes(include='object').columns:
        X[col] = X[col].astype(str)
        try:
            X[col] = pd.to_numeric(X[col], errors='raise')
        except ValueError:
            X[col] = X[col].astype('category')
            X[col] = X[col].cat.codes

    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # Split data
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)

    model_type = cfg.get('model_type')

    if model_type == 'logistic_regression':
        C = cfg.get('logistic_regression:C')
        penalty = cfg.get('logistic_regression:penalty')
        model = LogisticRegression(C=C, penalty=penalty, solver='liblinear', random_state=seed)
    elif model_type == 'random_forest':
        n_estimators = cfg.get('random_forest:n_estimators')
        max_depth = cfg.get('random_forest:max_depth')
        min_samples_split = cfg.get('random_forest:min_samples_split')
        min_samples_leaf = cfg.get('random_forest:min_samples_leaf')
        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                       min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,
                                       random_state=seed, n_jobs=-1)
    elif model_type == 'svm':
        C = cfg.get('svm:C')
        kernel = cfg.get('svm:kernel')
        gamma = cfg.get('svm:gamma')
        degree = cfg.get('svm:degree')
        model = SVC(C=C, kernel=kernel, gamma=gamma, degree=degree, random_state=seed)
    else:
        raise ValueError(f"Unknown model type: {model_type}")

    # Train model
    model.fit(X_train, y_train)

    # Evaluate model
    y_pred = model.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred)

    return -accuracy

--------------------------------------------------------------------------------
[2025-06-20 11:27:26] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#3): could not convert string to float: '<0'
--------------------------------------------------------------------------------
[2025-06-20 11:27:29] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from ConfigSpace import Configuration
from typing import Any

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A Configuration object containing the hyperparameters.
        dataset (Any): A dictionary containing the training data ('X' and 'y').
        seed (int): Random seed for reproducibility.

    Returns:
        float: Negative validation accuracy.
    """
    np.random.seed(seed)

    X = dataset['X'].copy()
    y = dataset['y'].copy()

    # Preprocessing: Label Encoding and Scaling
    for col in X.select_dtypes(include='object').columns:
        X[col] = X[col].astype(str)
        X[col] = pd.to_numeric(X[col], errors='coerce')
        X[col] = X[col].fillna(X[col].median())

    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # Split data
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)

    model_type = cfg.get('model_type')

    if model_type == 'logistic_regression':
        C = cfg.get('logistic_regression:C')
        penalty = cfg.get('logistic_regression:penalty')
        model = LogisticRegression(C=C, penalty=penalty, solver='liblinear', random_state=seed)
    elif model_type == 'random_forest':
        n_estimators = cfg.get('random_forest:n_estimators')
        max_depth = cfg.get('random_forest:max_depth')
        min_samples_split = cfg.get('random_forest:min_samples_split')
        min_samples_leaf = cfg.get('random_forest:min_samples_leaf')
        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                       min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,
                                       random_state=seed, n_jobs=-1)
    elif model_type == 'svm':
        C = cfg.get('svm:C')
        kernel = cfg.get('svm:kernel')
        gamma = cfg.get('svm:gamma')
        degree = cfg.get('svm:degree')
        model = SVC(C=C, kernel=kernel, gamma=gamma, degree=degree, random_state=seed)
    else:
        raise ValueError(f"Unknown model type: {model_type}")

    # Train model
    model.fit(X_train, y_train)

    # Evaluate model
    y_pred = model.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred)

    return -accuracy
```
--------------------------------------------------------------------------------
[2025-06-20 11:27:30] [Metadata: {'component': 'train_function', 'action': 'run'}] import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from ConfigSpace import Configuration
from typing import Any

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A Configuration object containing the hyperparameters.
        dataset (Any): A dictionary containing the training data ('X' and 'y').
        seed (int): Random seed for reproducibility.

    Returns:
        float: Negative validation accuracy.
    """
    np.random.seed(seed)

    X = dataset['X'].copy()
    y = dataset['y'].copy()

    # Preprocessing: Label Encoding and Scaling
    for col in X.select_dtypes(include='object').columns:
        X[col] = X[col].astype(str)
        X[col] = pd.to_numeric(X[col], errors='coerce')
        X[col] = X[col].fillna(X[col].median())

    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # Split data
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)

    model_type = cfg.get('model_type')

    if model_type == 'logistic_regression':
        C = cfg.get('logistic_regression:C')
        penalty = cfg.get('logistic_regression:penalty')
        model = LogisticRegression(C=C, penalty=penalty, solver='liblinear', random_state=seed)
    elif model_type == 'random_forest':
        n_estimators = cfg.get('random_forest:n_estimators')
        max_depth = cfg.get('random_forest:max_depth')
        min_samples_split = cfg.get('random_forest:min_samples_split')
        min_samples_leaf = cfg.get('random_forest:min_samples_leaf')
        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                       min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,
                                       random_state=seed, n_jobs=-1)
    elif model_type == 'svm':
        C = cfg.get('svm:C')
        kernel = cfg.get('svm:kernel')
        gamma = cfg.get('svm:gamma')
        degree = cfg.get('svm:degree')
        model = SVC(C=C, kernel=kernel, gamma=gamma, degree=degree, random_state=seed)
    else:
        raise ValueError(f"Unknown model type: {model_type}")

    # Train model
    model.fit(X_train, y_train)

    # Evaluate model
    y_pred = model.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred)

    return -accuracy

--------------------------------------------------------------------------------
[2025-06-20 11:27:30] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#4): could not convert string to float: '<0'
--------------------------------------------------------------------------------
[2025-06-20 11:27:33] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from ConfigSpace import Configuration
from typing import Any

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A Configuration object containing the hyperparameters.
        dataset (Any): A dictionary containing the training data ('X' and 'y').
        seed (int): Random seed for reproducibility.

    Returns:
        float: Negative validation accuracy.
    """
    np.random.seed(seed)

    X = dataset['X'].copy()
    y = dataset['y'].copy()

    # Preprocessing: Label Encoding and Scaling
    for col in X.select_dtypes(include='object').columns:
        X[col] = X[col].astype(str)
        # Attempt to convert to numeric, coerce errors to NaN
        X[col] = pd.to_numeric(X[col], errors='coerce')
        # Fill NaN values with the median of the column
        X[col] = X[col].fillna(X[col].median())

    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # Split data
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)

    model_type = cfg.get('model_type')

    if model_type == 'logistic_regression':
        C = cfg.get('logistic_regression:C')
        penalty = cfg.get('logistic_regression:penalty')
        model = LogisticRegression(C=C, penalty=penalty, solver='liblinear', random_state=seed)
    elif model_type == 'random_forest':
        n_estimators = cfg.get('random_forest:n_estimators')
        max_depth = cfg.get('random_forest:max_depth')
        min_samples_split = cfg.get('random_forest:min_samples_split')
        min_samples_leaf = cfg.get('random_forest:min_samples_leaf')
        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                       min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,
                                       random_state=seed, n_jobs=-1)
    elif model_type == 'svm':
        C = cfg.get('svm:C')
        kernel = cfg.get('svm:kernel')
        gamma = cfg.get('svm:gamma')
        degree = cfg.get('svm:degree')
        model = SVC(C=C, kernel=kernel, gamma=gamma, degree=degree, random_state=seed)
    else:
        raise ValueError(f"Unknown model type: {model_type}")

    # Train model
    model.fit(X_train, y_train)

    # Evaluate model
    y_pred = model.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred)

    return -accuracy
```
--------------------------------------------------------------------------------
[2025-06-20 11:27:33] [Metadata: {'component': 'train_function', 'action': 'run'}] import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from ConfigSpace import Configuration
from typing import Any

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A Configuration object containing the hyperparameters.
        dataset (Any): A dictionary containing the training data ('X' and 'y').
        seed (int): Random seed for reproducibility.

    Returns:
        float: Negative validation accuracy.
    """
    np.random.seed(seed)

    X = dataset['X'].copy()
    y = dataset['y'].copy()

    # Preprocessing: Label Encoding and Scaling
    for col in X.select_dtypes(include='object').columns:
        X[col] = X[col].astype(str)
        # Attempt to convert to numeric, coerce errors to NaN
        X[col] = pd.to_numeric(X[col], errors='coerce')
        # Fill NaN values with the median of the column
        X[col] = X[col].fillna(X[col].median())

    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # Split data
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)

    model_type = cfg.get('model_type')

    if model_type == 'logistic_regression':
        C = cfg.get('logistic_regression:C')
        penalty = cfg.get('logistic_regression:penalty')
        model = LogisticRegression(C=C, penalty=penalty, solver='liblinear', random_state=seed)
    elif model_type == 'random_forest':
        n_estimators = cfg.get('random_forest:n_estimators')
        max_depth = cfg.get('random_forest:max_depth')
        min_samples_split = cfg.get('random_forest:min_samples_split')
        min_samples_leaf = cfg.get('random_forest:min_samples_leaf')
        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                       min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,
                                       random_state=seed, n_jobs=-1)
    elif model_type == 'svm':
        C = cfg.get('svm:C')
        kernel = cfg.get('svm:kernel')
        gamma = cfg.get('svm:gamma')
        degree = cfg.get('svm:degree')
        model = SVC(C=C, kernel=kernel, gamma=gamma, degree=degree, random_state=seed)
    else:
        raise ValueError(f"Unknown model type: {model_type}")

    # Train model
    model.fit(X_train, y_train)

    # Evaluate model
    y_pred = model.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred)

    return -accuracy

--------------------------------------------------------------------------------
[2025-06-20 11:27:33] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#5): could not convert string to float: '<0'
--------------------------------------------------------------------------------
[2025-06-20 11:27:37] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from ConfigSpace import Configuration
from typing import Any

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A Configuration object containing the hyperparameters.
        dataset (Any): A dictionary containing the training data ('X' and 'y').
        seed (int): Random seed for reproducibility.

    Returns:
        float: Negative validation accuracy.
    """
    np.random.seed(seed)

    X = dataset['X'].copy()
    y = dataset['y'].copy()

    # Preprocessing: Handle non-numeric values and scale
    for col in X.columns:
        if pd.api.types.is_object_dtype(X[col]):
            try:
                X[col] = pd.to_numeric(X[col])
            except ValueError:
                # If conversion to numeric fails, use Label Encoding
                le = LabelEncoder()
                X[col] = le.fit_transform(X[col].astype(str))
        
        if pd.api.types.is_numeric_dtype(X[col]):
            X[col] = X[col].fillna(X[col].median())  # Fill NaN with median

    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # Split data
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)

    model_type = cfg.get('model_type')

    if model_type == 'logistic_regression':
        C = cfg.get('logistic_regression:C')
        penalty = cfg.get('logistic_regression:penalty')
        model = LogisticRegression(C=C, penalty=penalty, solver='liblinear', random_state=seed)
    elif model_type == 'random_forest':
        n_estimators = cfg.get('random_forest:n_estimators')
        max_depth = cfg.get('random_forest:max_depth')
        min_samples_split = cfg.get('random_forest:min_samples_split')
        min_samples_leaf = cfg.get('random_forest:min_samples_leaf')
        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                       min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,
                                       random_state=seed, n_jobs=-1)
    elif model_type == 'svm':
        C = cfg.get('svm:C')
        kernel = cfg.get('svm:kernel')
        gamma = cfg.get('svm:gamma')
        degree = cfg.get('svm:degree')
        model = SVC(C=C, kernel=kernel, gamma=gamma, degree=degree, random_state=seed)
    else:
        raise ValueError(f"Unknown model type: {model_type}")

    # Train model
    model.fit(X_train, y_train)

    # Evaluate model
    y_pred = model.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred)

    return -accuracy
```
--------------------------------------------------------------------------------
[2025-06-20 11:27:37] [Metadata: {'component': 'train_function', 'action': 'run'}] import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from ConfigSpace import Configuration
from typing import Any

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A Configuration object containing the hyperparameters.
        dataset (Any): A dictionary containing the training data ('X' and 'y').
        seed (int): Random seed for reproducibility.

    Returns:
        float: Negative validation accuracy.
    """
    np.random.seed(seed)

    X = dataset['X'].copy()
    y = dataset['y'].copy()

    # Preprocessing: Handle non-numeric values and scale
    for col in X.columns:
        if pd.api.types.is_object_dtype(X[col]):
            try:
                X[col] = pd.to_numeric(X[col])
            except ValueError:
                # If conversion to numeric fails, use Label Encoding
                le = LabelEncoder()
                X[col] = le.fit_transform(X[col].astype(str))
        
        if pd.api.types.is_numeric_dtype(X[col]):
            X[col] = X[col].fillna(X[col].median())  # Fill NaN with median

    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # Split data
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)

    model_type = cfg.get('model_type')

    if model_type == 'logistic_regression':
        C = cfg.get('logistic_regression:C')
        penalty = cfg.get('logistic_regression:penalty')
        model = LogisticRegression(C=C, penalty=penalty, solver='liblinear', random_state=seed)
    elif model_type == 'random_forest':
        n_estimators = cfg.get('random_forest:n_estimators')
        max_depth = cfg.get('random_forest:max_depth')
        min_samples_split = cfg.get('random_forest:min_samples_split')
        min_samples_leaf = cfg.get('random_forest:min_samples_leaf')
        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                       min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,
                                       random_state=seed, n_jobs=-1)
    elif model_type == 'svm':
        C = cfg.get('svm:C')
        kernel = cfg.get('svm:kernel')
        gamma = cfg.get('svm:gamma')
        degree = cfg.get('svm:degree')
        model = SVC(C=C, kernel=kernel, gamma=gamma, degree=degree, random_state=seed)
    else:
        raise ValueError(f"Unknown model type: {model_type}")

    # Train model
    model.fit(X_train, y_train)

    # Evaluate model
    y_pred = model.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred)

    return -accuracy

--------------------------------------------------------------------------------
[2025-06-20 11:27:37] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#6): could not convert string to float: '<0'
--------------------------------------------------------------------------------
[2025-06-20 11:27:41] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from ConfigSpace import Configuration
from typing import Any

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A Configuration object containing the hyperparameters.
        dataset (Any): A dictionary containing the training data ('X' and 'y').
        seed (int): Random seed for reproducibility.

    Returns:
        float: Negative validation accuracy.
    """
    np.random.seed(seed)

    X = dataset['X'].copy()
    y = dataset['y'].copy()

    # Preprocessing: Handle non-numeric values and scale
    for col in X.columns:
        if pd.api.types.is_object_dtype(X[col]):
            try:
                X[col] = pd.to_numeric(X[col], errors='coerce')
            except ValueError:
                # If conversion to numeric fails, use Label Encoding
                le = LabelEncoder()
                X[col] = le.fit_transform(X[col].astype(str))
        
        if pd.api.types.is_numeric_dtype(X[col]):
            X[col] = X[col].fillna(X[col].median())  # Fill NaN with median

    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # Split data
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)

    model_type = cfg.get('model_type')

    if model_type == 'logistic_regression':
        C = cfg.get('logistic_regression:C')
        penalty = cfg.get('logistic_regression:penalty')
        model = LogisticRegression(C=C, penalty=penalty, solver='liblinear', random_state=seed)
    elif model_type == 'random_forest':
        n_estimators = cfg.get('random_forest:n_estimators')
        max_depth = cfg.get('random_forest:max_depth')
        min_samples_split = cfg.get('random_forest:min_samples_split')
        min_samples_leaf = cfg.get('random_forest:min_samples_leaf')
        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                       min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,
                                       random_state=seed, n_jobs=-1)
    elif model_type == 'svm':
        C = cfg.get('svm:C')
        kernel = cfg.get('svm:kernel')
        gamma = cfg.get('svm:gamma')
        degree = cfg.get('svm:degree')
        model = SVC(C=C, kernel=kernel, gamma=gamma, degree=degree, random_state=seed)
    else:
        raise ValueError(f"Unknown model type: {{model_type}}")

    # Train model
    model.fit(X_train, y_train)

    # Evaluate model
    y_pred = model.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred)

    return -accuracy
```
--------------------------------------------------------------------------------
[2025-06-20 11:27:41] [Metadata: {'component': 'train_function', 'action': 'run'}] import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from ConfigSpace import Configuration
from typing import Any

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A Configuration object containing the hyperparameters.
        dataset (Any): A dictionary containing the training data ('X' and 'y').
        seed (int): Random seed for reproducibility.

    Returns:
        float: Negative validation accuracy.
    """
    np.random.seed(seed)

    X = dataset['X'].copy()
    y = dataset['y'].copy()

    # Preprocessing: Handle non-numeric values and scale
    for col in X.columns:
        if pd.api.types.is_object_dtype(X[col]):
            try:
                X[col] = pd.to_numeric(X[col], errors='coerce')
            except ValueError:
                # If conversion to numeric fails, use Label Encoding
                le = LabelEncoder()
                X[col] = le.fit_transform(X[col].astype(str))
        
        if pd.api.types.is_numeric_dtype(X[col]):
            X[col] = X[col].fillna(X[col].median())  # Fill NaN with median

    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # Split data
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)

    model_type = cfg.get('model_type')

    if model_type == 'logistic_regression':
        C = cfg.get('logistic_regression:C')
        penalty = cfg.get('logistic_regression:penalty')
        model = LogisticRegression(C=C, penalty=penalty, solver='liblinear', random_state=seed)
    elif model_type == 'random_forest':
        n_estimators = cfg.get('random_forest:n_estimators')
        max_depth = cfg.get('random_forest:max_depth')
        min_samples_split = cfg.get('random_forest:min_samples_split')
        min_samples_leaf = cfg.get('random_forest:min_samples_leaf')
        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                       min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,
                                       random_state=seed, n_jobs=-1)
    elif model_type == 'svm':
        C = cfg.get('svm:C')
        kernel = cfg.get('svm:kernel')
        gamma = cfg.get('svm:gamma')
        degree = cfg.get('svm:degree')
        model = SVC(C=C, kernel=kernel, gamma=gamma, degree=degree, random_state=seed)
    else:
        raise ValueError(f"Unknown model type: {{model_type}}")

    # Train model
    model.fit(X_train, y_train)

    # Evaluate model
    y_pred = model.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred)

    return -accuracy

--------------------------------------------------------------------------------
[2025-06-20 11:27:41] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#7): could not convert string to float: '<0'
--------------------------------------------------------------------------------
[2025-06-20 11:27:46] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from ConfigSpace import Configuration
from typing import Any

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A Configuration object containing the hyperparameters.
        dataset (Any): A dictionary containing the training data ('X' and 'y').
        seed (int): Random seed for reproducibility.

    Returns:
        float: Negative validation accuracy.
    """
    np.random.seed(seed)

    X = dataset['X'].copy()
    y = dataset['y'].copy()

    # Preprocessing: Handle non-numeric values and scale
    for col in X.columns:
        if pd.api.types.is_object_dtype(X[col]):
            # Attempt conversion to numeric, coercing errors to NaN
            X[col] = pd.to_numeric(X[col], errors='coerce')
            # Fill NaN values resulting from failed conversion with the median
            X[col] = X[col].fillna(X[col].median())
            # If there are still NaN values after filling with median, use Label Encoding
            if X[col].isnull().any():
                le = LabelEncoder()
                X[col] = le.fit_transform(X[col].astype(str))
        elif pd.api.types.is_numeric_dtype(X[col]):
            X[col] = X[col].fillna(X[col].median())  # Fill NaN with median
        else:
            # Handle cases where the column is neither object nor numeric
            X[col] = X[col].astype(str)
            le = LabelEncoder()
            X[col] = le.fit_transform(X[col])

    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # Split data
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)

    model_type = cfg.get('model_type')

    if model_type == 'logistic_regression':
        C = cfg.get('logistic_regression:C')
        penalty = cfg.get('logistic_regression:penalty')
        model = LogisticRegression(C=C, penalty=penalty, solver='liblinear', random_state=seed)
    elif model_type == 'random_forest':
        n_estimators = cfg.get('random_forest:n_estimators')
        max_depth = cfg.get('random_forest:max_depth')
        min_samples_split = cfg.get('random_forest:min_samples_split')
        min_samples_leaf = cfg.get('random_forest:min_samples_leaf')
        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                       min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,
                                       random_state=seed, n_jobs=-1)
    elif model_type == 'svm':
        C = cfg.get('svm:C')
        kernel = cfg.get('svm:kernel')
        gamma = cfg.get('svm:gamma')
        degree = cfg.get('svm:degree')
        model = SVC(C=C, kernel=kernel, gamma=gamma, degree=degree, random_state=seed)
    else:
        raise ValueError(f"Unknown model type: {model_type}")

    # Train model
    model.fit(X_train, y_train)

    # Evaluate model
    y_pred = model.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred)

    return -accuracy
```
--------------------------------------------------------------------------------
[2025-06-20 11:27:46] [Metadata: {'component': 'train_function', 'action': 'run'}] import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from ConfigSpace import Configuration
from typing import Any

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model based on the provided configuration and dataset.

    Args:
        cfg (Configuration): A Configuration object containing the hyperparameters.
        dataset (Any): A dictionary containing the training data ('X' and 'y').
        seed (int): Random seed for reproducibility.

    Returns:
        float: Negative validation accuracy.
    """
    np.random.seed(seed)

    X = dataset['X'].copy()
    y = dataset['y'].copy()

    # Preprocessing: Handle non-numeric values and scale
    for col in X.columns:
        if pd.api.types.is_object_dtype(X[col]):
            # Attempt conversion to numeric, coercing errors to NaN
            X[col] = pd.to_numeric(X[col], errors='coerce')
            # Fill NaN values resulting from failed conversion with the median
            X[col] = X[col].fillna(X[col].median())
            # If there are still NaN values after filling with median, use Label Encoding
            if X[col].isnull().any():
                le = LabelEncoder()
                X[col] = le.fit_transform(X[col].astype(str))
        elif pd.api.types.is_numeric_dtype(X[col]):
            X[col] = X[col].fillna(X[col].median())  # Fill NaN with median
        else:
            # Handle cases where the column is neither object nor numeric
            X[col] = X[col].astype(str)
            le = LabelEncoder()
            X[col] = le.fit_transform(X[col])

    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    # Split data
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed, stratify=y)

    model_type = cfg.get('model_type')

    if model_type == 'logistic_regression':
        C = cfg.get('logistic_regression:C')
        penalty = cfg.get('logistic_regression:penalty')
        model = LogisticRegression(C=C, penalty=penalty, solver='liblinear', random_state=seed)
    elif model_type == 'random_forest':
        n_estimators = cfg.get('random_forest:n_estimators')
        max_depth = cfg.get('random_forest:max_depth')
        min_samples_split = cfg.get('random_forest:min_samples_split')
        min_samples_leaf = cfg.get('random_forest:min_samples_leaf')
        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth,
                                       min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf,
                                       random_state=seed, n_jobs=-1)
    elif model_type == 'svm':
        C = cfg.get('svm:C')
        kernel = cfg.get('svm:kernel')
        gamma = cfg.get('svm:gamma')
        degree = cfg.get('svm:degree')
        model = SVC(C=C, kernel=kernel, gamma=gamma, degree=degree, random_state=seed)
    else:
        raise ValueError(f"Unknown model type: {model_type}")

    # Train model
    model.fit(X_train, y_train)

    # Evaluate model
    y_pred = model.predict(X_val)
    accuracy = accuracy_score(y_val, y_pred)

    return -accuracy

--------------------------------------------------------------------------------
[2025-06-20 11:27:46] [Metadata: {'component': 'train_function', 'status': 'success', 'loss': -0.7}] Training executed successfully, loss: -0.7
--------------------------------------------------------------------------------
