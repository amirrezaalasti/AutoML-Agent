[2025-06-07 11:34:45] [Metadata: {'component': 'config'}] ```python
from ConfigSpace import (
    ConfigurationSpace,
    Categorical,
    Float,
    Integer,
    ForbiddenAndConjunction,
    ForbiddenEqualsClause,
)


def get_configspace():
    cs = ConfigurationSpace()

    # Define hyperparameters
    model_type = Categorical("model_type", ["cnn", "mlp"], default="cnn")
    batch_size = Integer("batch_size", (32, 256), default=64, log=True)
    learning_rate = Float("learning_rate", (1e-5, 1e-2), default=1e-3, log=True)
    optimizer = Categorical("optimizer", ["adam", "sgd"], default="adam")
    num_epochs = Integer("num_epochs", (5, 50), default=10)

    # CNN specific hyperparameters
    num_conv_layers = Integer("num_conv_layers", (1, 5), default=3)
    num_filters_l0 = Integer("num_filters_l0", (32, 128), default=64, log=True)
    kernel_size_l0 = Integer("kernel_size_l0", (3, 7), default=3)
    activation_cnn = Categorical("activation_cnn", ["relu", "tanh"], default="relu")
    use_batch_norm = Categorical("use_batch_norm", [True, False], default=True)

    # MLP specific hyperparameters
    num_hidden_layers = Integer("num_hidden_layers", (1, 5), default=2)
    num_units_l0 = Integer("num_units_l0", (64, 512), default=128, log=True)
    activation_mlp = Categorical("activation_mlp", ["relu", "tanh"], default="relu")
    dropout_rate = Float("dropout_rate", (0.0, 0.9), default=0.5)

    # Add hyperparameters to the configuration space
    cs.add_hyperparameters([
        model_type,
        batch_size,
        learning_rate,
        optimizer,
        num_epochs,
    ])

    cs.add_hyperparameters([
        num_conv_layers,
        num_filters_l0,
        kernel_size_l0,
        activation_cnn,
        use_batch_norm
    ])

    cs.add_hyperparameters([
        num_hidden_layers,
        num_units_l0,
        activation_mlp,
        dropout_rate
    ])

    # Forbidden Clauses
    forbidden_1 = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "cnn"),
        ForbiddenEqualsClause(activation_mlp, "relu"),
    )
    cs.add_forbidden_clause(forbidden_1)

    return cs
```
--------------------------------------------------------------------------------
[2025-06-07 11:34:45] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import (
    ConfigurationSpace,
    Categorical,
    Float,
    Integer,
    ForbiddenAndConjunction,
    ForbiddenEqualsClause,
)


def get_configspace():
    cs = ConfigurationSpace()

    # Define hyperparameters
    model_type = Categorical("model_type", ["cnn", "mlp"], default="cnn")
    batch_size = Integer("batch_size", (32, 256), default=64, log=True)
    learning_rate = Float("learning_rate", (1e-5, 1e-2), default=1e-3, log=True)
    optimizer = Categorical("optimizer", ["adam", "sgd"], default="adam")
    num_epochs = Integer("num_epochs", (5, 50), default=10)

    # CNN specific hyperparameters
    num_conv_layers = Integer("num_conv_layers", (1, 5), default=3)
    num_filters_l0 = Integer("num_filters_l0", (32, 128), default=64, log=True)
    kernel_size_l0 = Integer("kernel_size_l0", (3, 7), default=3)
    activation_cnn = Categorical("activation_cnn", ["relu", "tanh"], default="relu")
    use_batch_norm = Categorical("use_batch_norm", [True, False], default=True)

    # MLP specific hyperparameters
    num_hidden_layers = Integer("num_hidden_layers", (1, 5), default=2)
    num_units_l0 = Integer("num_units_l0", (64, 512), default=128, log=True)
    activation_mlp = Categorical("activation_mlp", ["relu", "tanh"], default="relu")
    dropout_rate = Float("dropout_rate", (0.0, 0.9), default=0.5)

    # Add hyperparameters to the configuration space
    cs.add_hyperparameters([
        model_type,
        batch_size,
        learning_rate,
        optimizer,
        num_epochs,
    ])

    cs.add_hyperparameters([
        num_conv_layers,
        num_filters_l0,
        kernel_size_l0,
        activation_cnn,
        use_batch_norm
    ])

    cs.add_hyperparameters([
        num_hidden_layers,
        num_units_l0,
        activation_mlp,
        dropout_rate
    ])

    # Forbidden Clauses
    forbidden_1 = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "cnn"),
        ForbiddenEqualsClause(activation_mlp, "relu"),
    )
    cs.add_forbidden_clause(forbidden_1)

    return cs

--------------------------------------------------------------------------------
[2025-06-07 11:34:45] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#1): Given vector violates forbidden clause: (Forbidden: activation_mlp == 'relu' && Forbidden: model_type == 'cnn')
--------------------------------------------------------------------------------
[2025-06-07 11:34:49] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import (
    ConfigurationSpace,
    Categorical,
    Float,
    Integer,
    ForbiddenAndConjunction,
    ForbiddenEqualsClause,
)


def get_configspace():
    cs = ConfigurationSpace()

    # Define hyperparameters
    model_type = Categorical("model_type", ["cnn", "mlp"], default="cnn")
    batch_size = Integer("batch_size", (32, 256), default=64, log=True)
    learning_rate = Float("learning_rate", (1e-5, 1e-2), default=1e-3, log=True)
    optimizer = Categorical("optimizer", ["adam", "sgd"], default="adam")
    num_epochs = Integer("num_epochs", (5, 50), default=10)

    # CNN specific hyperparameters
    num_conv_layers = Integer("num_conv_layers", (1, 5), default=3)
    num_filters_l0 = Integer("num_filters_l0", (32, 128), default=64, log=True)
    kernel_size_l0 = Integer("kernel_size_l0", (3, 7), default=3)
    activation_cnn = Categorical("activation_cnn", ["relu", "tanh"], default="relu")
    use_batch_norm = Categorical("use_batch_norm", [True, False], default=True)

    # MLP specific hyperparameters
    num_hidden_layers = Integer("num_hidden_layers", (1, 5), default=2)
    num_units_l0 = Integer("num_units_l0", (64, 512), default=128, log=True)
    activation_mlp = Categorical("activation_mlp", ["relu", "tanh"], default="relu")
    dropout_rate = Float("dropout_rate", (0.0, 0.9), default=0.5)

    # Add hyperparameters to the configuration space
    cs.add_hyperparameters([
        model_type,
        batch_size,
        learning_rate,
        optimizer,
        num_epochs,
    ])

    cs.add_hyperparameters([
        num_conv_layers,
        num_filters_l0,
        kernel_size_l0,
        activation_cnn,
        use_batch_norm
    ])

    cs.add_hyperparameters([
        num_hidden_layers,
        num_units_l0,
        activation_mlp,
        dropout_rate
    ])

    # Forbidden Clauses
    forbidden_1 = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "cnn"),
        ForbiddenEqualsClause(activation_mlp, "relu"),
    )
    forbidden_2 = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "mlp"),
        ForbiddenEqualsClause(activation_cnn, "relu"),
    )
    cs.add_forbidden_clause(forbidden_2)

    return cs
```
--------------------------------------------------------------------------------
[2025-06-07 11:34:49] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import (
    ConfigurationSpace,
    Categorical,
    Float,
    Integer,
    ForbiddenAndConjunction,
    ForbiddenEqualsClause,
)


def get_configspace():
    cs = ConfigurationSpace()

    # Define hyperparameters
    model_type = Categorical("model_type", ["cnn", "mlp"], default="cnn")
    batch_size = Integer("batch_size", (32, 256), default=64, log=True)
    learning_rate = Float("learning_rate", (1e-5, 1e-2), default=1e-3, log=True)
    optimizer = Categorical("optimizer", ["adam", "sgd"], default="adam")
    num_epochs = Integer("num_epochs", (5, 50), default=10)

    # CNN specific hyperparameters
    num_conv_layers = Integer("num_conv_layers", (1, 5), default=3)
    num_filters_l0 = Integer("num_filters_l0", (32, 128), default=64, log=True)
    kernel_size_l0 = Integer("kernel_size_l0", (3, 7), default=3)
    activation_cnn = Categorical("activation_cnn", ["relu", "tanh"], default="relu")
    use_batch_norm = Categorical("use_batch_norm", [True, False], default=True)

    # MLP specific hyperparameters
    num_hidden_layers = Integer("num_hidden_layers", (1, 5), default=2)
    num_units_l0 = Integer("num_units_l0", (64, 512), default=128, log=True)
    activation_mlp = Categorical("activation_mlp", ["relu", "tanh"], default="relu")
    dropout_rate = Float("dropout_rate", (0.0, 0.9), default=0.5)

    # Add hyperparameters to the configuration space
    cs.add_hyperparameters([
        model_type,
        batch_size,
        learning_rate,
        optimizer,
        num_epochs,
    ])

    cs.add_hyperparameters([
        num_conv_layers,
        num_filters_l0,
        kernel_size_l0,
        activation_cnn,
        use_batch_norm
    ])

    cs.add_hyperparameters([
        num_hidden_layers,
        num_units_l0,
        activation_mlp,
        dropout_rate
    ])

    # Forbidden Clauses
    forbidden_1 = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "cnn"),
        ForbiddenEqualsClause(activation_mlp, "relu"),
    )
    forbidden_2 = ForbiddenAndConjunction(
        ForbiddenEqualsClause(model_type, "mlp"),
        ForbiddenEqualsClause(activation_cnn, "relu"),
    )
    cs.add_forbidden_clause(forbidden_2)

    return cs

--------------------------------------------------------------------------------
[2025-06-07 11:34:49] [Metadata: {'component': 'config', 'status': 'success'}] Configuration space generated successfully
--------------------------------------------------------------------------------
[2025-06-07 11:34:50] [Metadata: {'component': 'scenario'}] ```python
import ConfigSpace as CS
from smac import Scenario

def generate_scenario(cs):
    return Scenario(
        configspace=cs,
        name="gemini-2.0-flashmnist20250607_113449",
        output_directory="./automl_results",
        deterministic=False,
        n_workers=2,
        min_budget=1,
        max_budget=9,
        n_trials=10,
    )
```
--------------------------------------------------------------------------------
[2025-06-07 11:34:50] [Metadata: {'component': 'scenario', 'action': 'run'}] import ConfigSpace as CS
from smac import Scenario

def generate_scenario(cs):
    return Scenario(
        configspace=cs,
        name="gemini-2.0-flashmnist20250607_113449",
        output_directory="./automl_results",
        deterministic=False,
        n_workers=2,
        min_budget=1,
        max_budget=9,
        n_trials=10,
    )

--------------------------------------------------------------------------------
[2025-06-07 11:34:50] [Metadata: {'component': 'scenario', 'status': 'success'}] Scenario generated successfully
--------------------------------------------------------------------------------
[2025-06-07 11:35:00] [Metadata: {'component': 'train_function'}] ```python
from typing import Any
from ConfigSpace import Configuration

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from math import sqrt

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model on the given dataset according to the provided configuration.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dictionary containing the dataset with 'X' and 'y' keys.
        seed (int): Random seed for reproducibility.

    Returns:
        float: Average training loss over 10 epochs.
    """

    X = dataset['X']
    y = dataset['y']

    # Convert to numpy arrays if they are not already
    if not isinstance(X, np.ndarray):
        X = np.array(X)
    if not isinstance(y, np.ndarray):
        y = np.array(y)

    # Data preprocessing
    X = X.astype(np.float32) / 255.0  # Normalize pixel values

    # Determine input dimensions
    n_samples = X.shape[0]
    n_features = X.shape[1]

    # Model type
    model_type = cfg.get("model_type")

    if model_type == "cnn":
        # Reshape input for CNN
        height = int(sqrt(n_features))
        if height * height != n_features:
            raise ValueError("Input dimension is not a square number.")
        X = X.reshape(n_samples, 1, height, height)  # (N, C, H, W)

    elif model_type == "mlp":
        # Flatten input for MLP
        X = X.reshape(n_samples, -1)


    # Convert data to PyTorch tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.long)

    # Create DataLoader
    batch_size = cfg.get("batch_size")
    train_dataset = TensorDataset(X_tensor, y_tensor)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    # Define the model
    if model_type == "cnn":
        class CNN(nn.Module):
            def __init__(self, cfg, input_channels=1):
                super(CNN, self).__init__()
                self.layers = nn.ModuleList()
                num_conv_layers = cfg.get("num_conv_layers")
                num_filters_l0 = cfg.get("num_filters_l0")
                kernel_size_l0 = cfg.get("kernel_size_l0")
                activation_cnn_str = cfg.get("activation_cnn")
                use_batch_norm = cfg.get("use_batch_norm")

                if activation_cnn_str == "relu":
                    activation_cnn = nn.ReLU()
                elif activation_cnn_str == "tanh":
                    activation_cnn = nn.Tanh()
                else:
                    raise ValueError(f"Unsupported activation function: {activation_cnn_str}")


                # First Convolutional Layer
                self.layers.append(nn.Conv2d(input_channels, num_filters_l0, kernel_size=kernel_size_l0))
                if use_batch_norm:
                    self.layers.append(nn.BatchNorm2d(num_filters_l0))
                self.layers.append(activation_cnn)
                self.layers.append(nn.MaxPool2d(kernel_size=2, stride=2))

                # Additional Convolutional Layers
                in_channels = num_filters_l0
                for i in range(1, num_conv_layers):
                    num_filters = int(num_filters_l0 * (2 ** i))
                    self.layers.append(nn.Conv2d(in_channels, num_filters, kernel_size=3))
                    if use_batch_norm:
                        self.layers.append(nn.BatchNorm2d(num_filters))
                    self.layers.append(activation_cnn)
                    self.layers.append(nn.MaxPool2d(kernel_size=2, stride=2))
                    in_channels = num_filters

                # Calculate the output size of the convolutional layers
                self._to_linear = None
                def _get_linear_size():
                  bs = 1
                  input = torch.randn(bs, input_channels, 28, 28) #Dummy Input Size to derive final size after convolutions
                  x = input
                  for layer in self.layers:
                    x = layer(x)
                  return x.size(1) * x.size(2) * x.size(3)

                self._to_linear = _get_linear_size()
                self.fc = nn.Linear(self._to_linear, 10)  # 10 output classes

            def forward(self, x):
                for layer in self.layers:
                    x = layer(x)

                x = x.reshape(x.size(0), -1)  # Flatten the tensor
                x = self.fc(x)
                return x


        model = CNN(cfg)


    elif model_type == "mlp":
        class MLP(nn.Module):
            def __init__(self, cfg, input_size):
                super(MLP, self).__init__()
                self.layers = nn.ModuleList()
                num_hidden_layers = cfg.get("num_hidden_layers")
                num_units_l0 = cfg.get("num_units_l0")
                activation_mlp_str = cfg.get("activation_mlp")
                dropout_rate = cfg.get("dropout_rate")

                if activation_mlp_str == "relu":
                    activation_mlp = nn.ReLU()
                elif activation_mlp_str == "tanh":
                    activation_mlp = nn.Tanh()
                else:
                    raise ValueError(f"Unsupported activation function: {activation_mlp_str}")

                # First hidden layer
                self.layers.append(nn.Linear(input_size, num_units_l0))
                self.layers.append(activation_mlp)
                self.layers.append(nn.Dropout(dropout_rate))

                # Additional hidden layers
                in_features = num_units_l0
                for _ in range(1, num_hidden_layers):
                    num_units = int(num_units_l0 / 2)
                    self.layers.append(nn.Linear(in_features, num_units))
                    self.layers.append(activation_mlp)
                    self.layers.append(nn.Dropout(dropout_rate))
                    in_features = num_units

                # Output layer
                self.layers.append(nn.Linear(in_features, 10)) # 10 output classes

            def forward(self, x):
                for layer in self.layers:
                    x = layer(x)
                return x

        model = MLP(cfg, input_size=n_features)

    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Define optimizer
    learning_rate = cfg.get("learning_rate")
    optimizer_name = cfg.get("optimizer")
    if optimizer_name == "adam":
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == "sgd":
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Unsupported optimizer: {optimizer_name}")

    # Define loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    num_epochs = 10
    total_loss = 0.0
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for i, (inputs, labels) in enumerate(train_loader):
            # Zero the parameter gradients
            optimizer.zero_grad()

            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_epoch_loss = epoch_loss / len(train_loader)
        total_loss += avg_epoch_loss

    avg_loss = total_loss / num_epochs

    return float(avg_loss)
```
--------------------------------------------------------------------------------
[2025-06-07 11:35:00] [Metadata: {'component': 'train_function', 'action': 'run'}] from typing import Any
from ConfigSpace import Configuration

import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader
from math import sqrt

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a machine learning model on the given dataset according to the provided configuration.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dictionary containing the dataset with 'X' and 'y' keys.
        seed (int): Random seed for reproducibility.

    Returns:
        float: Average training loss over 10 epochs.
    """

    X = dataset['X']
    y = dataset['y']

    # Convert to numpy arrays if they are not already
    if not isinstance(X, np.ndarray):
        X = np.array(X)
    if not isinstance(y, np.ndarray):
        y = np.array(y)

    # Data preprocessing
    X = X.astype(np.float32) / 255.0  # Normalize pixel values

    # Determine input dimensions
    n_samples = X.shape[0]
    n_features = X.shape[1]

    # Model type
    model_type = cfg.get("model_type")

    if model_type == "cnn":
        # Reshape input for CNN
        height = int(sqrt(n_features))
        if height * height != n_features:
            raise ValueError("Input dimension is not a square number.")
        X = X.reshape(n_samples, 1, height, height)  # (N, C, H, W)

    elif model_type == "mlp":
        # Flatten input for MLP
        X = X.reshape(n_samples, -1)


    # Convert data to PyTorch tensors
    X_tensor = torch.tensor(X, dtype=torch.float32)
    y_tensor = torch.tensor(y, dtype=torch.long)

    # Create DataLoader
    batch_size = cfg.get("batch_size")
    train_dataset = TensorDataset(X_tensor, y_tensor)
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

    # Define the model
    if model_type == "cnn":
        class CNN(nn.Module):
            def __init__(self, cfg, input_channels=1):
                super(CNN, self).__init__()
                self.layers = nn.ModuleList()
                num_conv_layers = cfg.get("num_conv_layers")
                num_filters_l0 = cfg.get("num_filters_l0")
                kernel_size_l0 = cfg.get("kernel_size_l0")
                activation_cnn_str = cfg.get("activation_cnn")
                use_batch_norm = cfg.get("use_batch_norm")

                if activation_cnn_str == "relu":
                    activation_cnn = nn.ReLU()
                elif activation_cnn_str == "tanh":
                    activation_cnn = nn.Tanh()
                else:
                    raise ValueError(f"Unsupported activation function: {activation_cnn_str}")


                # First Convolutional Layer
                self.layers.append(nn.Conv2d(input_channels, num_filters_l0, kernel_size=kernel_size_l0))
                if use_batch_norm:
                    self.layers.append(nn.BatchNorm2d(num_filters_l0))
                self.layers.append(activation_cnn)
                self.layers.append(nn.MaxPool2d(kernel_size=2, stride=2))

                # Additional Convolutional Layers
                in_channels = num_filters_l0
                for i in range(1, num_conv_layers):
                    num_filters = int(num_filters_l0 * (2 ** i))
                    self.layers.append(nn.Conv2d(in_channels, num_filters, kernel_size=3))
                    if use_batch_norm:
                        self.layers.append(nn.BatchNorm2d(num_filters))
                    self.layers.append(activation_cnn)
                    self.layers.append(nn.MaxPool2d(kernel_size=2, stride=2))
                    in_channels = num_filters

                # Calculate the output size of the convolutional layers
                self._to_linear = None
                def _get_linear_size():
                  bs = 1
                  input = torch.randn(bs, input_channels, 28, 28) #Dummy Input Size to derive final size after convolutions
                  x = input
                  for layer in self.layers:
                    x = layer(x)
                  return x.size(1) * x.size(2) * x.size(3)

                self._to_linear = _get_linear_size()
                self.fc = nn.Linear(self._to_linear, 10)  # 10 output classes

            def forward(self, x):
                for layer in self.layers:
                    x = layer(x)

                x = x.reshape(x.size(0), -1)  # Flatten the tensor
                x = self.fc(x)
                return x


        model = CNN(cfg)


    elif model_type == "mlp":
        class MLP(nn.Module):
            def __init__(self, cfg, input_size):
                super(MLP, self).__init__()
                self.layers = nn.ModuleList()
                num_hidden_layers = cfg.get("num_hidden_layers")
                num_units_l0 = cfg.get("num_units_l0")
                activation_mlp_str = cfg.get("activation_mlp")
                dropout_rate = cfg.get("dropout_rate")

                if activation_mlp_str == "relu":
                    activation_mlp = nn.ReLU()
                elif activation_mlp_str == "tanh":
                    activation_mlp = nn.Tanh()
                else:
                    raise ValueError(f"Unsupported activation function: {activation_mlp_str}")

                # First hidden layer
                self.layers.append(nn.Linear(input_size, num_units_l0))
                self.layers.append(activation_mlp)
                self.layers.append(nn.Dropout(dropout_rate))

                # Additional hidden layers
                in_features = num_units_l0
                for _ in range(1, num_hidden_layers):
                    num_units = int(num_units_l0 / 2)
                    self.layers.append(nn.Linear(in_features, num_units))
                    self.layers.append(activation_mlp)
                    self.layers.append(nn.Dropout(dropout_rate))
                    in_features = num_units

                # Output layer
                self.layers.append(nn.Linear(in_features, 10)) # 10 output classes

            def forward(self, x):
                for layer in self.layers:
                    x = layer(x)
                return x

        model = MLP(cfg, input_size=n_features)

    else:
        raise ValueError(f"Unsupported model type: {model_type}")

    # Define optimizer
    learning_rate = cfg.get("learning_rate")
    optimizer_name = cfg.get("optimizer")
    if optimizer_name == "adam":
        optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    elif optimizer_name == "sgd":
        optimizer = optim.SGD(model.parameters(), lr=learning_rate)
    else:
        raise ValueError(f"Unsupported optimizer: {optimizer_name}")

    # Define loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    num_epochs = 10
    total_loss = 0.0
    for epoch in range(num_epochs):
        epoch_loss = 0.0
        for i, (inputs, labels) in enumerate(train_loader):
            # Zero the parameter gradients
            optimizer.zero_grad()

            # Forward pass
            outputs = model(inputs)
            loss = criterion(outputs, labels)

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_epoch_loss = epoch_loss / len(train_loader)
        total_loss += avg_epoch_loss

    avg_loss = total_loss / num_epochs

    return float(avg_loss)

--------------------------------------------------------------------------------
[2025-06-07 11:35:04] [Metadata: {'component': 'train_function', 'status': 'success', 'loss': 2.3104146492150095}] Training executed successfully, loss: 2.3104146492150095
--------------------------------------------------------------------------------
