[2025-06-15 18:27:37] [Metadata: {'component': 'config'}] ```python
from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, Constant
from ConfigSpace.conditions import InCondition
from ConfigSpace.hyperparameters import UnParametrizedHyperparameter

def get_configspace() -> ConfigurationSpace:
    """
    Defines the configuration space for a CNN model for image classification.
    This configuration space is tailored for the image dataset described, considering
    the input format requirements, data processing steps, and framework-specific needs.
    """
    cs = ConfigurationSpace()

    # Define core hyperparameters
    batch_size = CategoricalHyperparameter(
        "batch_size", choices=[32, 64, 128, 256], default_value=64,
        doc="The batch size for training the neural network. Larger batch sizes can lead to more stable training, "
            "while smaller batch sizes might offer better generalization."
    )
    cs.add_hyperparameter(batch_size)

    epochs = UniformIntegerHyperparameter(
        "epochs", lower=5, upper=50, default_value=10,
        doc="The number of epochs to train the neural network. One epoch is a complete pass through the training data."
    )
    cs.add_hyperparameter(epochs)

    optimizer = CategoricalHyperparameter(
        "optimizer", choices=["Adam", "SGD", "RMSprop"], default_value="Adam",
        doc="The optimization algorithm used for training. Adam is generally a good default choice, while SGD and "
            "RMSprop may be suitable for specific cases."
    )
    cs.add_hyperparameter(optimizer)

    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-5, upper=1e-2, default_value=1e-3, log=True,
        doc="The learning rate for the optimization algorithm. It controls the step size during training. "
            "A smaller learning rate may lead to slower convergence, while a larger learning rate might cause instability."
    )
    cs.add_hyperparameter(learning_rate)

    # Add hyperparameters specific to the chosen optimizer
    beta1 = UniformFloatHyperparameter(
        "beta1", lower=0.8, upper=0.999, default_value=0.9,
        doc="Beta1 parameter for Adam optimizer. Controls the exponential decay rate for the first moment estimates."
    )
    beta2 = UniformFloatHyperparameter(
        "beta2", lower=0.9, upper=0.9999, default_value=0.999,
        doc="Beta2 parameter for Adam optimizer. Controls the exponential decay rate for the second moment estimates."
    )
    cs.add_hyperparameter(beta1)
    cs.add_hyperparameter(beta2)

    momentum = UniformFloatHyperparameter(
        "momentum", lower=0.0, upper=0.99, default_value=0.9,
        doc="Momentum parameter for SGD optimizer. Helps accelerate gradient descent in the relevant direction and "
            "dampens oscillations."
    )
    cs.add_hyperparameter(momentum)
    
    rho = UniformFloatHyperparameter(
        "rho", lower=0.0, upper=0.99, default_value=0.9,
        doc="Rho parameter for RMSprop optimizer. Helps to average the square of the gradients for adaptive learning."
    )
    cs.add_hyperparameter(rho)
    
    # Add conditions to only use the relevant hyperparameters for each optimizer
    cs.add_condition(InCondition(child=beta1, parent=optimizer, values=["Adam"]))
    cs.add_condition(InCondition(child=beta2, parent=optimizer, values=["Adam"]))
    cs.add_condition(InCondition(child=momentum, parent=optimizer, values=["SGD"]))
    cs.add_condition(InCondition(child=rho, parent=optimizer, values=["RMSprop"]))

    # CNN Architecture Parameters
    num_conv_layers = UniformIntegerHyperparameter(
        "num_conv_layers", lower=1, upper=5, default_value=3,
        doc="The number of convolutional layers in the CNN architecture. More layers can capture more complex features, "
            "but can also increase the risk of overfitting."
    )
    cs.add_hyperparameter(num_conv_layers)

    num_filters_layer_1 = CategoricalHyperparameter(
        "num_filters_layer_1", choices=[32, 64, 128], default_value=64,
        doc="The number of filters in the first convolutional layer. More filters can capture more diverse features."
    )
    cs.add_hyperparameter(num_filters_layer_1)

    kernel_size = CategoricalHyperparameter(
        "kernel_size", choices=[3, 5, 7], default_value=3,
        doc="The size of the convolutional kernel. Smaller kernels can capture finer details, while larger kernels "
            "can capture broader features."
    )
    cs.add_hyperparameter(kernel_size)

    use_pooling = CategoricalHyperparameter(
        "use_pooling", choices=[True, False], default_value=True,
        doc="Whether to use max-pooling layers after each convolutional layer. Pooling helps reduce the spatial "
            "dimensions and can improve robustness to variations."
    )
    cs.add_hyperparameter(use_pooling)

    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate", lower=0.0, upper=0.5, default_value=0.25,
        doc="The dropout rate for regularization. Dropout randomly drops neurons during training to prevent overfitting."
    )
    cs.add_hyperparameter(dropout_rate)

    # Dense layer parameters
    num_dense_layers = UniformIntegerHyperparameter(
        "num_dense_layers", lower=1, upper=3, default_value=1,
        doc="The number of dense layers in the network."
    )
    cs.add_hyperparameter(num_dense_layers)

    num_units_dense_1 = CategoricalHyperparameter(
        "num_units_dense_1", choices=[64, 128, 256, 512], default_value=256,
        doc="The number of units in the first dense layer."
    )
    cs.add_hyperparameter(num_units_dense_1)
    
    use_batch_normalization = CategoricalHyperparameter(
        "use_batch_normalization", choices=[True, False], default_value=True,
        doc="Whether to use batch normalization. Helps with faster convergence."
    )
    cs.add_hyperparameter(use_batch_normalization)

    # Data Normalization
    normalization_strategy = CategoricalHyperparameter(
        "normalization_strategy", choices=["scale", "standardize", "none"], default_value="scale",
        doc="The data normalization strategy. 'scale' scales pixel values to [0, 1], 'standardize' standardizes to mean=0, std=1."
    )
    cs.add_hyperparameter(normalization_strategy)

    return cs
```
--------------------------------------------------------------------------------
[2025-06-15 18:27:37] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, Constant
from ConfigSpace.conditions import InCondition
from ConfigSpace.hyperparameters import UnParametrizedHyperparameter

def get_configspace() -> ConfigurationSpace:
    """
    Defines the configuration space for a CNN model for image classification.
    This configuration space is tailored for the image dataset described, considering
    the input format requirements, data processing steps, and framework-specific needs.
    """
    cs = ConfigurationSpace()

    # Define core hyperparameters
    batch_size = CategoricalHyperparameter(
        "batch_size", choices=[32, 64, 128, 256], default_value=64,
        doc="The batch size for training the neural network. Larger batch sizes can lead to more stable training, "
            "while smaller batch sizes might offer better generalization."
    )
    cs.add_hyperparameter(batch_size)

    epochs = UniformIntegerHyperparameter(
        "epochs", lower=5, upper=50, default_value=10,
        doc="The number of epochs to train the neural network. One epoch is a complete pass through the training data."
    )
    cs.add_hyperparameter(epochs)

    optimizer = CategoricalHyperparameter(
        "optimizer", choices=["Adam", "SGD", "RMSprop"], default_value="Adam",
        doc="The optimization algorithm used for training. Adam is generally a good default choice, while SGD and "
            "RMSprop may be suitable for specific cases."
    )
    cs.add_hyperparameter(optimizer)

    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-5, upper=1e-2, default_value=1e-3, log=True,
        doc="The learning rate for the optimization algorithm. It controls the step size during training. "
            "A smaller learning rate may lead to slower convergence, while a larger learning rate might cause instability."
    )
    cs.add_hyperparameter(learning_rate)

    # Add hyperparameters specific to the chosen optimizer
    beta1 = UniformFloatHyperparameter(
        "beta1", lower=0.8, upper=0.999, default_value=0.9,
        doc="Beta1 parameter for Adam optimizer. Controls the exponential decay rate for the first moment estimates."
    )
    beta2 = UniformFloatHyperparameter(
        "beta2", lower=0.9, upper=0.9999, default_value=0.999,
        doc="Beta2 parameter for Adam optimizer. Controls the exponential decay rate for the second moment estimates."
    )
    cs.add_hyperparameter(beta1)
    cs.add_hyperparameter(beta2)

    momentum = UniformFloatHyperparameter(
        "momentum", lower=0.0, upper=0.99, default_value=0.9,
        doc="Momentum parameter for SGD optimizer. Helps accelerate gradient descent in the relevant direction and "
            "dampens oscillations."
    )
    cs.add_hyperparameter(momentum)
    
    rho = UniformFloatHyperparameter(
        "rho", lower=0.0, upper=0.99, default_value=0.9,
        doc="Rho parameter for RMSprop optimizer. Helps to average the square of the gradients for adaptive learning."
    )
    cs.add_hyperparameter(rho)
    
    # Add conditions to only use the relevant hyperparameters for each optimizer
    cs.add_condition(InCondition(child=beta1, parent=optimizer, values=["Adam"]))
    cs.add_condition(InCondition(child=beta2, parent=optimizer, values=["Adam"]))
    cs.add_condition(InCondition(child=momentum, parent=optimizer, values=["SGD"]))
    cs.add_condition(InCondition(child=rho, parent=optimizer, values=["RMSprop"]))

    # CNN Architecture Parameters
    num_conv_layers = UniformIntegerHyperparameter(
        "num_conv_layers", lower=1, upper=5, default_value=3,
        doc="The number of convolutional layers in the CNN architecture. More layers can capture more complex features, "
            "but can also increase the risk of overfitting."
    )
    cs.add_hyperparameter(num_conv_layers)

    num_filters_layer_1 = CategoricalHyperparameter(
        "num_filters_layer_1", choices=[32, 64, 128], default_value=64,
        doc="The number of filters in the first convolutional layer. More filters can capture more diverse features."
    )
    cs.add_hyperparameter(num_filters_layer_1)

    kernel_size = CategoricalHyperparameter(
        "kernel_size", choices=[3, 5, 7], default_value=3,
        doc="The size of the convolutional kernel. Smaller kernels can capture finer details, while larger kernels "
            "can capture broader features."
    )
    cs.add_hyperparameter(kernel_size)

    use_pooling = CategoricalHyperparameter(
        "use_pooling", choices=[True, False], default_value=True,
        doc="Whether to use max-pooling layers after each convolutional layer. Pooling helps reduce the spatial "
            "dimensions and can improve robustness to variations."
    )
    cs.add_hyperparameter(use_pooling)

    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate", lower=0.0, upper=0.5, default_value=0.25,
        doc="The dropout rate for regularization. Dropout randomly drops neurons during training to prevent overfitting."
    )
    cs.add_hyperparameter(dropout_rate)

    # Dense layer parameters
    num_dense_layers = UniformIntegerHyperparameter(
        "num_dense_layers", lower=1, upper=3, default_value=1,
        doc="The number of dense layers in the network."
    )
    cs.add_hyperparameter(num_dense_layers)

    num_units_dense_1 = CategoricalHyperparameter(
        "num_units_dense_1", choices=[64, 128, 256, 512], default_value=256,
        doc="The number of units in the first dense layer."
    )
    cs.add_hyperparameter(num_units_dense_1)
    
    use_batch_normalization = CategoricalHyperparameter(
        "use_batch_normalization", choices=[True, False], default_value=True,
        doc="Whether to use batch normalization. Helps with faster convergence."
    )
    cs.add_hyperparameter(use_batch_normalization)

    # Data Normalization
    normalization_strategy = CategoricalHyperparameter(
        "normalization_strategy", choices=["scale", "standardize", "none"], default_value="scale",
        doc="The data normalization strategy. 'scale' scales pixel values to [0, 1], 'standardize' standardizes to mean=0, std=1."
    )
    cs.add_hyperparameter(normalization_strategy)

    return cs

--------------------------------------------------------------------------------
[2025-06-15 18:27:37] [Metadata: {'error_type': 'CONFIG_ERROR'}] [CONFIG_ERROR] Error in config (#1): CategoricalHyperparameter.__init__() got an unexpected keyword argument 'doc'
--------------------------------------------------------------------------------
[2025-06-15 18:27:45] [Metadata: {'component': 'config', 'action': 'fix'}] ```python
from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, Constant
from ConfigSpace.conditions import InCondition
from ConfigSpace.hyperparameters import UnParametrizedHyperparameter

def get_configspace() -> ConfigurationSpace:
    """
    Defines the configuration space for a CNN model for image classification.
    This configuration space is tailored for the image dataset described, considering
    the input format requirements, data processing steps, and framework-specific needs.
    """
    cs = ConfigurationSpace()

    # Define core hyperparameters
    batch_size = CategoricalHyperparameter(
        "batch_size", choices=[32, 64, 128, 256], default_value=64
    )
    cs.add_hyperparameter(batch_size)
    batch_size.doc = "The batch size for training the neural network. Larger batch sizes can lead to more stable training, " \
                      "while smaller batch sizes might offer better generalization."

    epochs = UniformIntegerHyperparameter(
        "epochs", lower=5, upper=50, default_value=10
    )
    cs.add_hyperparameter(epochs)
    epochs.doc = "The number of epochs to train the neural network. One epoch is a complete pass through the training data."

    optimizer = CategoricalHyperparameter(
        "optimizer", choices=["Adam", "SGD", "RMSprop"], default_value="Adam"
    )
    cs.add_hyperparameter(optimizer)
    optimizer.doc = "The optimization algorithm used for training. Adam is generally a good default choice, while SGD and " \
                     "RMSprop may be suitable for specific cases."

    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-5, upper=1e-2, default_value=1e-3, log=True
    )
    cs.add_hyperparameter(learning_rate)
    learning_rate.doc = "The learning rate for the optimization algorithm. It controls the step size during training. " \
                         "A smaller learning rate may lead to slower convergence, while a larger learning rate might cause instability."

    # Add hyperparameters specific to the chosen optimizer
    beta1 = UniformFloatHyperparameter(
        "beta1", lower=0.8, upper=0.999, default_value=0.9
    )
    beta2 = UniformFloatHyperparameter(
        "beta2", lower=0.9, upper=0.9999, default_value=0.999
    )
    cs.add_hyperparameter(beta1)
    cs.add_hyperparameter(beta2)
    beta1.doc = "Beta1 parameter for Adam optimizer. Controls the exponential decay rate for the first moment estimates."
    beta2.doc = "Beta2 parameter for Adam optimizer. Controls the exponential decay rate for the second moment estimates."

    momentum = UniformFloatHyperparameter(
        "momentum", lower=0.0, upper=0.99, default_value=0.9
    )
    cs.add_hyperparameter(momentum)
    momentum.doc = "Momentum parameter for SGD optimizer. Helps accelerate gradient descent in the relevant direction and " \
                   "dampens oscillations."
    
    rho = UniformFloatHyperparameter(
        "rho", lower=0.0, upper=0.99, default_value=0.9
    )
    cs.add_hyperparameter(rho)
    rho.doc = "Rho parameter for RMSprop optimizer. Helps to average the square of the gradients for adaptive learning."
    
    # Add conditions to only use the relevant hyperparameters for each optimizer
    cs.add_condition(InCondition(child=beta1, parent=optimizer, values=["Adam"]))
    cs.add_condition(InCondition(child=beta2, parent=optimizer, values=["Adam"]))
    cs.add_condition(InCondition(child=momentum, parent=optimizer, values=["SGD"]))
    cs.add_condition(InCondition(child=rho, parent=optimizer, values=["RMSprop"]))

    # CNN Architecture Parameters
    num_conv_layers = UniformIntegerHyperparameter(
        "num_conv_layers", lower=1, upper=5, default_value=3
    )
    cs.add_hyperparameter(num_conv_layers)
    num_conv_layers.doc = "The number of convolutional layers in the CNN architecture. More layers can capture more complex features, " \
                           "but can also increase the risk of overfitting."

    num_filters_layer_1 = CategoricalHyperparameter(
        "num_filters_layer_1", choices=[32, 64, 128], default_value=64
    )
    cs.add_hyperparameter(num_filters_layer_1)
    num_filters_layer_1.doc = "The number of filters in the first convolutional layer. More filters can capture more diverse features."

    kernel_size = CategoricalHyperparameter(
        "kernel_size", choices=[3, 5, 7], default_value=3
    )
    cs.add_hyperparameter(kernel_size)
    kernel_size.doc = "The size of the convolutional kernel. Smaller kernels can capture finer details, while larger kernels " \
                      "can capture broader features."

    use_pooling = CategoricalHyperparameter(
        "use_pooling", choices=[True, False], default_value=True
    )
    cs.add_hyperparameter(use_pooling)
    use_pooling.doc = "Whether to use max-pooling layers after each convolutional layer. Pooling helps reduce the spatial " \
                      "dimensions and can improve robustness to variations."

    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate", lower=0.0, upper=0.5, default_value=0.25
    )
    cs.add_hyperparameter(dropout_rate)
    dropout_rate.doc = "The dropout rate for regularization. Dropout randomly drops neurons during training to prevent overfitting."

    # Dense layer parameters
    num_dense_layers = UniformIntegerHyperparameter(
        "num_dense_layers", lower=1, upper=3, default_value=1
    )
    cs.add_hyperparameter(num_dense_layers)
    num_dense_layers.doc = "The number of dense layers in the network."

    num_units_dense_1 = CategoricalHyperparameter(
        "num_units_dense_1", choices=[64, 128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(num_units_dense_1)
    num_units_dense_1.doc = "The number of units in the first dense layer."
    
    use_batch_normalization = CategoricalHyperparameter(
        "use_batch_normalization", choices=[True, False], default_value=True
    )
    cs.add_hyperparameter(use_batch_normalization)
    use_batch_normalization.doc = "Whether to use batch normalization. Helps with faster convergence."

    # Data Normalization
    normalization_strategy = CategoricalHyperparameter(
        "normalization_strategy", choices=["scale", "standardize", "none"], default_value="scale"
    )
    cs.add_hyperparameter(normalization_strategy)
    normalization_strategy.doc = "The data normalization strategy. 'scale' scales pixel values to [0, 1], 'standardize' standardizes to mean=0, std=1."

    return cs
```
--------------------------------------------------------------------------------
[2025-06-15 18:27:45] [Metadata: {'component': 'config', 'action': 'run'}] from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter, Constant
from ConfigSpace.conditions import InCondition
from ConfigSpace.hyperparameters import UnParametrizedHyperparameter

def get_configspace() -> ConfigurationSpace:
    """
    Defines the configuration space for a CNN model for image classification.
    This configuration space is tailored for the image dataset described, considering
    the input format requirements, data processing steps, and framework-specific needs.
    """
    cs = ConfigurationSpace()

    # Define core hyperparameters
    batch_size = CategoricalHyperparameter(
        "batch_size", choices=[32, 64, 128, 256], default_value=64
    )
    cs.add_hyperparameter(batch_size)
    batch_size.doc = "The batch size for training the neural network. Larger batch sizes can lead to more stable training, " \
                      "while smaller batch sizes might offer better generalization."

    epochs = UniformIntegerHyperparameter(
        "epochs", lower=5, upper=50, default_value=10
    )
    cs.add_hyperparameter(epochs)
    epochs.doc = "The number of epochs to train the neural network. One epoch is a complete pass through the training data."

    optimizer = CategoricalHyperparameter(
        "optimizer", choices=["Adam", "SGD", "RMSprop"], default_value="Adam"
    )
    cs.add_hyperparameter(optimizer)
    optimizer.doc = "The optimization algorithm used for training. Adam is generally a good default choice, while SGD and " \
                     "RMSprop may be suitable for specific cases."

    learning_rate = UniformFloatHyperparameter(
        "learning_rate", lower=1e-5, upper=1e-2, default_value=1e-3, log=True
    )
    cs.add_hyperparameter(learning_rate)
    learning_rate.doc = "The learning rate for the optimization algorithm. It controls the step size during training. " \
                         "A smaller learning rate may lead to slower convergence, while a larger learning rate might cause instability."

    # Add hyperparameters specific to the chosen optimizer
    beta1 = UniformFloatHyperparameter(
        "beta1", lower=0.8, upper=0.999, default_value=0.9
    )
    beta2 = UniformFloatHyperparameter(
        "beta2", lower=0.9, upper=0.9999, default_value=0.999
    )
    cs.add_hyperparameter(beta1)
    cs.add_hyperparameter(beta2)
    beta1.doc = "Beta1 parameter for Adam optimizer. Controls the exponential decay rate for the first moment estimates."
    beta2.doc = "Beta2 parameter for Adam optimizer. Controls the exponential decay rate for the second moment estimates."

    momentum = UniformFloatHyperparameter(
        "momentum", lower=0.0, upper=0.99, default_value=0.9
    )
    cs.add_hyperparameter(momentum)
    momentum.doc = "Momentum parameter for SGD optimizer. Helps accelerate gradient descent in the relevant direction and " \
                   "dampens oscillations."
    
    rho = UniformFloatHyperparameter(
        "rho", lower=0.0, upper=0.99, default_value=0.9
    )
    cs.add_hyperparameter(rho)
    rho.doc = "Rho parameter for RMSprop optimizer. Helps to average the square of the gradients for adaptive learning."
    
    # Add conditions to only use the relevant hyperparameters for each optimizer
    cs.add_condition(InCondition(child=beta1, parent=optimizer, values=["Adam"]))
    cs.add_condition(InCondition(child=beta2, parent=optimizer, values=["Adam"]))
    cs.add_condition(InCondition(child=momentum, parent=optimizer, values=["SGD"]))
    cs.add_condition(InCondition(child=rho, parent=optimizer, values=["RMSprop"]))

    # CNN Architecture Parameters
    num_conv_layers = UniformIntegerHyperparameter(
        "num_conv_layers", lower=1, upper=5, default_value=3
    )
    cs.add_hyperparameter(num_conv_layers)
    num_conv_layers.doc = "The number of convolutional layers in the CNN architecture. More layers can capture more complex features, " \
                           "but can also increase the risk of overfitting."

    num_filters_layer_1 = CategoricalHyperparameter(
        "num_filters_layer_1", choices=[32, 64, 128], default_value=64
    )
    cs.add_hyperparameter(num_filters_layer_1)
    num_filters_layer_1.doc = "The number of filters in the first convolutional layer. More filters can capture more diverse features."

    kernel_size = CategoricalHyperparameter(
        "kernel_size", choices=[3, 5, 7], default_value=3
    )
    cs.add_hyperparameter(kernel_size)
    kernel_size.doc = "The size of the convolutional kernel. Smaller kernels can capture finer details, while larger kernels " \
                      "can capture broader features."

    use_pooling = CategoricalHyperparameter(
        "use_pooling", choices=[True, False], default_value=True
    )
    cs.add_hyperparameter(use_pooling)
    use_pooling.doc = "Whether to use max-pooling layers after each convolutional layer. Pooling helps reduce the spatial " \
                      "dimensions and can improve robustness to variations."

    dropout_rate = UniformFloatHyperparameter(
        "dropout_rate", lower=0.0, upper=0.5, default_value=0.25
    )
    cs.add_hyperparameter(dropout_rate)
    dropout_rate.doc = "The dropout rate for regularization. Dropout randomly drops neurons during training to prevent overfitting."

    # Dense layer parameters
    num_dense_layers = UniformIntegerHyperparameter(
        "num_dense_layers", lower=1, upper=3, default_value=1
    )
    cs.add_hyperparameter(num_dense_layers)
    num_dense_layers.doc = "The number of dense layers in the network."

    num_units_dense_1 = CategoricalHyperparameter(
        "num_units_dense_1", choices=[64, 128, 256, 512], default_value=256
    )
    cs.add_hyperparameter(num_units_dense_1)
    num_units_dense_1.doc = "The number of units in the first dense layer."
    
    use_batch_normalization = CategoricalHyperparameter(
        "use_batch_normalization", choices=[True, False], default_value=True
    )
    cs.add_hyperparameter(use_batch_normalization)
    use_batch_normalization.doc = "Whether to use batch normalization. Helps with faster convergence."

    # Data Normalization
    normalization_strategy = CategoricalHyperparameter(
        "normalization_strategy", choices=["scale", "standardize", "none"], default_value="scale"
    )
    cs.add_hyperparameter(normalization_strategy)
    normalization_strategy.doc = "The data normalization strategy. 'scale' scales pixel values to [0, 1], 'standardize' standardizes to mean=0, std=1."

    return cs

--------------------------------------------------------------------------------
[2025-06-15 18:27:45] [Metadata: {'component': 'config', 'status': 'success'}] Configuration space generated successfully
--------------------------------------------------------------------------------
[2025-06-15 18:27:46] [Metadata: {'component': 'scenario'}] ```python
from smac import Scenario
from ConfigSpace import ConfigurationSpace


def generate_scenario(cs: ConfigurationSpace) -> Scenario:
    """
    Generates a SMAC scenario configuration for hyperparameter optimization.

    Args:
        cs (ConfigurationSpace): The configuration space from which to sample configurations.

    Returns:
        Scenario: A SMAC Scenario object configured for the specified optimization task.
    """

    scenario = Scenario(
        configspace=cs,
        name="image_classification_scenario",
        output_directory="automl_results",
        deterministic=False,
        n_trials=10,
        n_workers=1,
    )

    return scenario
```
--------------------------------------------------------------------------------
[2025-06-15 18:27:46] [Metadata: {'component': 'scenario', 'action': 'run'}] from smac import Scenario
from ConfigSpace import ConfigurationSpace


def generate_scenario(cs: ConfigurationSpace) -> Scenario:
    """
    Generates a SMAC scenario configuration for hyperparameter optimization.

    Args:
        cs (ConfigurationSpace): The configuration space from which to sample configurations.

    Returns:
        Scenario: A SMAC Scenario object configured for the specified optimization task.
    """

    scenario = Scenario(
        configspace=cs,
        name="image_classification_scenario",
        output_directory="automl_results",
        deterministic=False,
        n_trials=10,
        n_workers=1,
    )

    return scenario

--------------------------------------------------------------------------------
[2025-06-15 18:27:46] [Metadata: {'component': 'scenario', 'status': 'success'}] Scenario generated successfully
--------------------------------------------------------------------------------
[2025-06-15 18:27:56] [Metadata: {'component': 'train_function'}] ```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from ConfigSpace import Configuration
from typing import Any
import math


class CNN(nn.Module):
    def __init__(self, cfg: Configuration, input_channels: int = 1, num_classes: int = 10):
        super(CNN, self).__init__()

        self.num_conv_layers = cfg.get('num_conv_layers')
        self.num_filters_layer_1 = cfg.get('num_filters_layer_1')
        self.kernel_size = cfg.get('kernel_size')
        self.use_pooling = cfg.get('use_pooling')
        self.dropout_rate = cfg.get('dropout_rate')
        self.num_dense_layers = cfg.get('num_dense_layers')
        self.num_units_dense_1 = cfg.get('num_units_dense_1')
        self.use_batch_normalization = cfg.get('use_batch_normalization')

        layers = []
        in_channels = input_channels
        num_filters = self.num_filters_layer_1

        for i in range(self.num_conv_layers):
            layers.append(nn.Conv2d(in_channels, num_filters, kernel_size=self.kernel_size, padding=self.kernel_size // 2))
            if self.use_batch_normalization:
                layers.append(nn.BatchNorm2d(num_filters))
            layers.append(nn.ReLU())
            if self.use_pooling:
                layers.append(nn.MaxPool2d(2, 2))
            in_channels = num_filters
            num_filters *= 2  # Increase filters with each layer

        self.conv_layers = nn.Sequential(*layers)

        # Dynamically determine the size of the flattened features after conv layers
        self._to_linear = None

        # Dummy forward pass to calculate the output size of conv_layers
        def _determine_flattened_size(m, x):
            with torch.no_grad():
                y = m(x)
                return int(np.prod(y.size()[1:]))

        # Register a hook on the last conv layer if it's not empty
        if layers:
          last_conv_layer = layers[-1]
          if isinstance(last_conv_layer, nn.MaxPool2d):
              # If the last layer is MaxPool2d, register the hook on the previous layer
              last_conv_layer = layers[-2]
          dummy_input = torch.randn(1, input_channels, 28, 28)
          self._to_linear = _determine_flattened_size(self.conv_layers, dummy_input)


        dense_layers = []
        in_features = self._to_linear if self._to_linear is not None else input_channels * 28 * 28  # Handle cases with no conv layers
        num_units = self.num_units_dense_1
        
        for i in range(self.num_dense_layers - 1):
          dense_layers.append(nn.Linear(in_features, num_units))
          dense_layers.append(nn.ReLU())
          dense_layers.append(nn.Dropout(self.dropout_rate))
          in_features = num_units
          num_units //= 2 #reduce units each layer
        
        dense_layers.append(nn.Linear(in_features, num_classes))

        self.dense_layers = nn.Sequential(*dense_layers)
        
    def forward(self, x):
        if self.conv_layers:
            x = self.conv_layers(x)
            x = torch.flatten(x, 1)  # Flatten before dense layers
        else:
            x = torch.flatten(x, 1)

        x = self.dense_layers(x)
        return x


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a CNN model on the given dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dataset dictionary with 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: The training loss.
    """
    torch.manual_seed(seed)

    # Extract hyperparameters
    batch_size = cfg.get('batch_size')
    epochs = cfg.get('epochs')
    learning_rate = cfg.get('learning_rate')
    optimizer_name = cfg.get('optimizer')

    # Data preparation
    X = dataset['X']
    y = dataset['y']

    # Handle different input data shapes
    if len(X.shape) == 2:  # Flattened input
        n_features = X.shape[1]
        height = width = int(math.sqrt(n_features))
        if height * width != n_features:
            raise ValueError("Input features are not a perfect square.")
        X = X.reshape(-1, 1, height, width)  # Reshape to (N, 1, H, W)
    elif len(X.shape) == 3:  # 3D input (N, H, W)
        X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    elif len(X.shape) == 4:
        pass # Assume (N, C, H, W) format for PyTorch

    # Normalize data
    normalization_strategy = cfg.get('normalization_strategy')
    if normalization_strategy == 'scale':
        X = X / 255.0
    elif normalization_strategy == 'standardize':
        mean = X.mean()
        std = X.std()
        X = (X - mean) / (std + 1e-8)  # Add a small constant to avoid division by zero
    # else: 'none' - no normalization
    
    # Convert data to PyTorch tensors
    X = torch.tensor(X, dtype=torch.float32)
    y = torch.tensor(y, dtype=torch.long)

    # Create DataLoader
    dataset = torch.utils.data.TensorDataset(X, y)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Model initialization
    input_channels = X.shape[1]
    num_classes = len(torch.unique(y))
    model = CNN(cfg, input_channels=input_channels, num_classes=num_classes)

    # Optimizer selection
    if optimizer_name == 'Adam':
        beta1 = cfg.get('beta1')
        beta2 = cfg.get('beta2')
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2))
    elif optimizer_name == 'SGD':
        momentum = cfg.get('momentum')
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    elif optimizer_name == 'RMSprop':
        rho = cfg.get('rho')
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, alpha=rho)
    else:
        raise ValueError(f"Unknown optimizer: {optimizer_name}")

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in dataloader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

    # Calculate average loss
    loss = running_loss / len(dataloader)
    return float(loss)
```
--------------------------------------------------------------------------------
[2025-06-15 18:27:56] [Metadata: {'component': 'train_function', 'action': 'run'}] import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from ConfigSpace import Configuration
from typing import Any
import math


class CNN(nn.Module):
    def __init__(self, cfg: Configuration, input_channels: int = 1, num_classes: int = 10):
        super(CNN, self).__init__()

        self.num_conv_layers = cfg.get('num_conv_layers')
        self.num_filters_layer_1 = cfg.get('num_filters_layer_1')
        self.kernel_size = cfg.get('kernel_size')
        self.use_pooling = cfg.get('use_pooling')
        self.dropout_rate = cfg.get('dropout_rate')
        self.num_dense_layers = cfg.get('num_dense_layers')
        self.num_units_dense_1 = cfg.get('num_units_dense_1')
        self.use_batch_normalization = cfg.get('use_batch_normalization')

        layers = []
        in_channels = input_channels
        num_filters = self.num_filters_layer_1

        for i in range(self.num_conv_layers):
            layers.append(nn.Conv2d(in_channels, num_filters, kernel_size=self.kernel_size, padding=self.kernel_size // 2))
            if self.use_batch_normalization:
                layers.append(nn.BatchNorm2d(num_filters))
            layers.append(nn.ReLU())
            if self.use_pooling:
                layers.append(nn.MaxPool2d(2, 2))
            in_channels = num_filters
            num_filters *= 2  # Increase filters with each layer

        self.conv_layers = nn.Sequential(*layers)

        # Dynamically determine the size of the flattened features after conv layers
        self._to_linear = None

        # Dummy forward pass to calculate the output size of conv_layers
        def _determine_flattened_size(m, x):
            with torch.no_grad():
                y = m(x)
                return int(np.prod(y.size()[1:]))

        # Register a hook on the last conv layer if it's not empty
        if layers:
          last_conv_layer = layers[-1]
          if isinstance(last_conv_layer, nn.MaxPool2d):
              # If the last layer is MaxPool2d, register the hook on the previous layer
              last_conv_layer = layers[-2]
          dummy_input = torch.randn(1, input_channels, 28, 28)
          self._to_linear = _determine_flattened_size(self.conv_layers, dummy_input)


        dense_layers = []
        in_features = self._to_linear if self._to_linear is not None else input_channels * 28 * 28  # Handle cases with no conv layers
        num_units = self.num_units_dense_1
        
        for i in range(self.num_dense_layers - 1):
          dense_layers.append(nn.Linear(in_features, num_units))
          dense_layers.append(nn.ReLU())
          dense_layers.append(nn.Dropout(self.dropout_rate))
          in_features = num_units
          num_units //= 2 #reduce units each layer
        
        dense_layers.append(nn.Linear(in_features, num_classes))

        self.dense_layers = nn.Sequential(*dense_layers)
        
    def forward(self, x):
        if self.conv_layers:
            x = self.conv_layers(x)
            x = torch.flatten(x, 1)  # Flatten before dense layers
        else:
            x = torch.flatten(x, 1)

        x = self.dense_layers(x)
        return x


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a CNN model on the given dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dataset dictionary with 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: The training loss.
    """
    torch.manual_seed(seed)

    # Extract hyperparameters
    batch_size = cfg.get('batch_size')
    epochs = cfg.get('epochs')
    learning_rate = cfg.get('learning_rate')
    optimizer_name = cfg.get('optimizer')

    # Data preparation
    X = dataset['X']
    y = dataset['y']

    # Handle different input data shapes
    if len(X.shape) == 2:  # Flattened input
        n_features = X.shape[1]
        height = width = int(math.sqrt(n_features))
        if height * width != n_features:
            raise ValueError("Input features are not a perfect square.")
        X = X.reshape(-1, 1, height, width)  # Reshape to (N, 1, H, W)
    elif len(X.shape) == 3:  # 3D input (N, H, W)
        X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    elif len(X.shape) == 4:
        pass # Assume (N, C, H, W) format for PyTorch

    # Normalize data
    normalization_strategy = cfg.get('normalization_strategy')
    if normalization_strategy == 'scale':
        X = X / 255.0
    elif normalization_strategy == 'standardize':
        mean = X.mean()
        std = X.std()
        X = (X - mean) / (std + 1e-8)  # Add a small constant to avoid division by zero
    # else: 'none' - no normalization
    
    # Convert data to PyTorch tensors
    X = torch.tensor(X, dtype=torch.float32)
    y = torch.tensor(y, dtype=torch.long)

    # Create DataLoader
    dataset = torch.utils.data.TensorDataset(X, y)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Model initialization
    input_channels = X.shape[1]
    num_classes = len(torch.unique(y))
    model = CNN(cfg, input_channels=input_channels, num_classes=num_classes)

    # Optimizer selection
    if optimizer_name == 'Adam':
        beta1 = cfg.get('beta1')
        beta2 = cfg.get('beta2')
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2))
    elif optimizer_name == 'SGD':
        momentum = cfg.get('momentum')
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    elif optimizer_name == 'RMSprop':
        rho = cfg.get('rho')
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, alpha=rho)
    else:
        raise ValueError(f"Unknown optimizer: {optimizer_name}")

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in dataloader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

    # Calculate average loss
    loss = running_loss / len(dataloader)
    return float(loss)

--------------------------------------------------------------------------------
[2025-06-15 18:27:56] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#1): 'DataFrame' object has no attribute 'reshape'
--------------------------------------------------------------------------------
[2025-06-15 18:28:05] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from ConfigSpace import Configuration
from typing import Any
import math


class CNN(nn.Module):
    def __init__(self, cfg: Configuration, input_channels: int = 1, num_classes: int = 10):
        super(CNN, self).__init__()

        self.num_conv_layers = cfg.get('num_conv_layers')
        self.num_filters_layer_1 = cfg.get('num_filters_layer_1')
        self.kernel_size = cfg.get('kernel_size')
        self.use_pooling = cfg.get('use_pooling')
        self.dropout_rate = cfg.get('dropout_rate')
        self.num_dense_layers = cfg.get('num_dense_layers')
        self.num_units_dense_1 = cfg.get('num_units_dense_1')
        self.use_batch_normalization = cfg.get('use_batch_normalization')

        layers = []
        in_channels = input_channels
        num_filters = self.num_filters_layer_1

        for i in range(self.num_conv_layers):
            layers.append(nn.Conv2d(in_channels, num_filters, kernel_size=self.kernel_size, padding=self.kernel_size // 2))
            if self.use_batch_normalization:
                layers.append(nn.BatchNorm2d(num_filters))
            layers.append(nn.ReLU())
            if self.use_pooling:
                layers.append(nn.MaxPool2d(2, 2))
            in_channels = num_filters
            num_filters *= 2  # Increase filters with each layer

        self.conv_layers = nn.Sequential(*layers)

        # Dynamically determine the size of the flattened features after conv layers
        self._to_linear = None

        # Dummy forward pass to calculate the output size of conv_layers
        def _determine_flattened_size(m, x):
            with torch.no_grad():
                y = m(x)
                return int(np.prod(y.size()[1:]))

        # Register a hook on the last conv layer if it's not empty
        if layers:
          dummy_input = torch.randn(1, input_channels, 28, 28)
          self._to_linear = _determine_flattened_size(self.conv_layers, dummy_input)


        dense_layers = []
        in_features = self._to_linear if self._to_linear is not None else input_channels * 28 * 28  # Handle cases with no conv layers
        num_units = self.num_units_dense_1
        
        for i in range(self.num_dense_layers - 1):
          dense_layers.append(nn.Linear(in_features, num_units))
          dense_layers.append(nn.ReLU())
          dense_layers.append(nn.Dropout(self.dropout_rate))
          in_features = num_units
          num_units //= 2 #reduce units each layer
        
        dense_layers.append(nn.Linear(in_features, num_classes))

        self.dense_layers = nn.Sequential(*dense_layers)
        
    def forward(self, x):
        if self.conv_layers:
            x = self.conv_layers(x)
            x = torch.flatten(x, 1)  # Flatten before dense layers
        else:
            x = torch.flatten(x, 1)

        x = self.dense_layers(x)
        return x


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a CNN model on the given dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dataset dictionary with 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: The training loss.
    """
    torch.manual_seed(seed)

    # Extract hyperparameters
    batch_size = cfg.get('batch_size')
    epochs = cfg.get('epochs')
    learning_rate = cfg.get('learning_rate')
    optimizer_name = cfg.get('optimizer')

    # Data preparation
    X = dataset['X']
    y = dataset['y']

    # Handle different input data shapes
    if isinstance(X, np.ndarray):
        if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)  # Reshape to (N, 1, H, W)
        elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    elif isinstance(X, torch.Tensor):
        if len(X.shape) == 2:
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)
        elif len(X.shape) == 3:
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])

    # Normalize data
    normalization_strategy = cfg.get('normalization_strategy')
    if normalization_strategy == 'scale':
        if isinstance(X, np.ndarray):
            X = X / 255.0
        elif isinstance(X, torch.Tensor):
            X = X / 255.0
    elif normalization_strategy == 'standardize':
        if isinstance(X, np.ndarray):
            mean = X.mean()
            std = X.std()
            X = (X - mean) / (std + 1e-8)  # Add a small constant to avoid division by zero
        elif isinstance(X, torch.Tensor):
            mean = X.float().mean()
            std = X.float().std()
            X = (X - mean) / (std + 1e-8)
    # else: 'none' - no normalization
    
    # Convert data to PyTorch tensors
    if isinstance(X, np.ndarray):
        X = torch.tensor(X, dtype=torch.float32)
        y = torch.tensor(y, dtype=torch.long)
    elif not isinstance(X, torch.Tensor):
        raise TypeError("X must be a numpy array or a torch tensor.")
    elif not isinstance(y, torch.Tensor):
        y = torch.tensor(y, dtype=torch.long)


    # Create DataLoader
    dataset = torch.utils.data.TensorDataset(X, y)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Model initialization
    input_channels = X.shape[1]
    num_classes = len(torch.unique(y))
    model = CNN(cfg, input_channels=input_channels, num_classes=num_classes)

    # Optimizer selection
    if optimizer_name == 'Adam':
        beta1 = cfg.get('beta1')
        beta2 = cfg.get('beta2')
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2))
    elif optimizer_name == 'SGD':
        momentum = cfg.get('momentum')
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    elif optimizer_name == 'RMSprop':
        rho = cfg.get('rho')
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, alpha=rho)
    else:
        raise ValueError(f"Unknown optimizer: {{optimizer_name}}")

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in dataloader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

    # Calculate average loss
    loss = running_loss / len(dataloader)
    return float(loss)
```
--------------------------------------------------------------------------------
[2025-06-15 18:28:05] [Metadata: {'component': 'train_function', 'action': 'run'}] import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from ConfigSpace import Configuration
from typing import Any
import math


class CNN(nn.Module):
    def __init__(self, cfg: Configuration, input_channels: int = 1, num_classes: int = 10):
        super(CNN, self).__init__()

        self.num_conv_layers = cfg.get('num_conv_layers')
        self.num_filters_layer_1 = cfg.get('num_filters_layer_1')
        self.kernel_size = cfg.get('kernel_size')
        self.use_pooling = cfg.get('use_pooling')
        self.dropout_rate = cfg.get('dropout_rate')
        self.num_dense_layers = cfg.get('num_dense_layers')
        self.num_units_dense_1 = cfg.get('num_units_dense_1')
        self.use_batch_normalization = cfg.get('use_batch_normalization')

        layers = []
        in_channels = input_channels
        num_filters = self.num_filters_layer_1

        for i in range(self.num_conv_layers):
            layers.append(nn.Conv2d(in_channels, num_filters, kernel_size=self.kernel_size, padding=self.kernel_size // 2))
            if self.use_batch_normalization:
                layers.append(nn.BatchNorm2d(num_filters))
            layers.append(nn.ReLU())
            if self.use_pooling:
                layers.append(nn.MaxPool2d(2, 2))
            in_channels = num_filters
            num_filters *= 2  # Increase filters with each layer

        self.conv_layers = nn.Sequential(*layers)

        # Dynamically determine the size of the flattened features after conv layers
        self._to_linear = None

        # Dummy forward pass to calculate the output size of conv_layers
        def _determine_flattened_size(m, x):
            with torch.no_grad():
                y = m(x)
                return int(np.prod(y.size()[1:]))

        # Register a hook on the last conv layer if it's not empty
        if layers:
          dummy_input = torch.randn(1, input_channels, 28, 28)
          self._to_linear = _determine_flattened_size(self.conv_layers, dummy_input)


        dense_layers = []
        in_features = self._to_linear if self._to_linear is not None else input_channels * 28 * 28  # Handle cases with no conv layers
        num_units = self.num_units_dense_1
        
        for i in range(self.num_dense_layers - 1):
          dense_layers.append(nn.Linear(in_features, num_units))
          dense_layers.append(nn.ReLU())
          dense_layers.append(nn.Dropout(self.dropout_rate))
          in_features = num_units
          num_units //= 2 #reduce units each layer
        
        dense_layers.append(nn.Linear(in_features, num_classes))

        self.dense_layers = nn.Sequential(*dense_layers)
        
    def forward(self, x):
        if self.conv_layers:
            x = self.conv_layers(x)
            x = torch.flatten(x, 1)  # Flatten before dense layers
        else:
            x = torch.flatten(x, 1)

        x = self.dense_layers(x)
        return x


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a CNN model on the given dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dataset dictionary with 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: The training loss.
    """
    torch.manual_seed(seed)

    # Extract hyperparameters
    batch_size = cfg.get('batch_size')
    epochs = cfg.get('epochs')
    learning_rate = cfg.get('learning_rate')
    optimizer_name = cfg.get('optimizer')

    # Data preparation
    X = dataset['X']
    y = dataset['y']

    # Handle different input data shapes
    if isinstance(X, np.ndarray):
        if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)  # Reshape to (N, 1, H, W)
        elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    elif isinstance(X, torch.Tensor):
        if len(X.shape) == 2:
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)
        elif len(X.shape) == 3:
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])

    # Normalize data
    normalization_strategy = cfg.get('normalization_strategy')
    if normalization_strategy == 'scale':
        if isinstance(X, np.ndarray):
            X = X / 255.0
        elif isinstance(X, torch.Tensor):
            X = X / 255.0
    elif normalization_strategy == 'standardize':
        if isinstance(X, np.ndarray):
            mean = X.mean()
            std = X.std()
            X = (X - mean) / (std + 1e-8)  # Add a small constant to avoid division by zero
        elif isinstance(X, torch.Tensor):
            mean = X.float().mean()
            std = X.float().std()
            X = (X - mean) / (std + 1e-8)
    # else: 'none' - no normalization
    
    # Convert data to PyTorch tensors
    if isinstance(X, np.ndarray):
        X = torch.tensor(X, dtype=torch.float32)
        y = torch.tensor(y, dtype=torch.long)
    elif not isinstance(X, torch.Tensor):
        raise TypeError("X must be a numpy array or a torch tensor.")
    elif not isinstance(y, torch.Tensor):
        y = torch.tensor(y, dtype=torch.long)


    # Create DataLoader
    dataset = torch.utils.data.TensorDataset(X, y)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Model initialization
    input_channels = X.shape[1]
    num_classes = len(torch.unique(y))
    model = CNN(cfg, input_channels=input_channels, num_classes=num_classes)

    # Optimizer selection
    if optimizer_name == 'Adam':
        beta1 = cfg.get('beta1')
        beta2 = cfg.get('beta2')
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2))
    elif optimizer_name == 'SGD':
        momentum = cfg.get('momentum')
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    elif optimizer_name == 'RMSprop':
        rho = cfg.get('rho')
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, alpha=rho)
    else:
        raise ValueError(f"Unknown optimizer: {{optimizer_name}}")

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in dataloader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

    # Calculate average loss
    loss = running_loss / len(dataloader)
    return float(loss)

--------------------------------------------------------------------------------
[2025-06-15 18:28:05] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#2): X must be a numpy array or a torch tensor.
--------------------------------------------------------------------------------
[2025-06-15 18:28:14] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from ConfigSpace import Configuration
from typing import Any
import math


class CNN(nn.Module):
    def __init__(self, cfg: Configuration, input_channels: int = 1, num_classes: int = 10):
        super(CNN, self).__init__()

        self.num_conv_layers = cfg.get('num_conv_layers')
        self.num_filters_layer_1 = cfg.get('num_filters_layer_1')
        self.kernel_size = cfg.get('kernel_size')
        self.use_pooling = cfg.get('use_pooling')
        self.dropout_rate = cfg.get('dropout_rate')
        self.num_dense_layers = cfg.get('num_dense_layers')
        self.num_units_dense_1 = cfg.get('num_units_dense_1')
        self.use_batch_normalization = cfg.get('use_batch_normalization')

        layers = []
        in_channels = input_channels
        num_filters = self.num_filters_layer_1

        for i in range(self.num_conv_layers):
            layers.append(nn.Conv2d(in_channels, num_filters, kernel_size=self.kernel_size, padding=self.kernel_size // 2))
            if self.use_batch_normalization:
                layers.append(nn.BatchNorm2d(num_filters))
            layers.append(nn.ReLU())
            if self.use_pooling:
                layers.append(nn.MaxPool2d(2, 2))
            in_channels = num_filters
            num_filters *= 2  # Increase filters with each layer

        self.conv_layers = nn.Sequential(*layers)

        # Dynamically determine the size of the flattened features after conv layers
        self._to_linear = None

        # Dummy forward pass to calculate the output size of conv_layers
        def _determine_flattened_size(m, x):
            with torch.no_grad():
                y = m(x)
                return int(np.prod(y.size()[1:]))

        # Register a hook on the last conv layer if it's not empty
        if layers:
          dummy_input = torch.randn(1, input_channels, 28, 28)
          self._to_linear = _determine_flattened_size(self.conv_layers, dummy_input)


        dense_layers = []
        in_features = self._to_linear if self._to_linear is not None else input_channels * 28 * 28  # Handle cases with no conv layers
        num_units = self.num_units_dense_1
        
        for i in range(self.num_dense_layers - 1):
          dense_layers.append(nn.Linear(in_features, num_units))
          dense_layers.append(nn.ReLU())
          dense_layers.append(nn.Dropout(self.dropout_rate))
          in_features = num_units
          num_units //= 2 #reduce units each layer
        
        dense_layers.append(nn.Linear(in_features, num_classes))

        self.dense_layers = nn.Sequential(*dense_layers)
        
    def forward(self, x):
        if self.conv_layers:
            x = self.conv_layers(x)
            x = torch.flatten(x, 1)  # Flatten before dense layers
        else:
            x = torch.flatten(x, 1)

        x = self.dense_layers(x)
        return x


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a CNN model on the given dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dataset dictionary with 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: The training loss.
    """
    torch.manual_seed(seed)

    # Extract hyperparameters
    batch_size = cfg.get('batch_size')
    epochs = cfg.get('epochs')
    learning_rate = cfg.get('learning_rate')
    optimizer_name = cfg.get('optimizer')

    # Data preparation
    X = dataset['X']
    y = dataset['y']

    # Handle different input data shapes
    if isinstance(X, np.ndarray):
        if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)  # Reshape to (N, 1, H, W)
        elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    elif isinstance(X, torch.Tensor):
        if len(X.shape) == 2:
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)
        elif len(X.shape) == 3:
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])
    else:
        raise TypeError("X must be a numpy array or a torch tensor.")

    # Normalize data
    normalization_strategy = cfg.get('normalization_strategy')
    if normalization_strategy == 'scale':
        if isinstance(X, np.ndarray):
            X = X / 255.0
        elif isinstance(X, torch.Tensor):
            X = X / 255.0
    elif normalization_strategy == 'standardize':
        if isinstance(X, np.ndarray):
            mean = X.mean()
            std = X.std()
            X = (X - mean) / (std + 1e-8)  # Add a small constant to avoid division by zero
        elif isinstance(X, torch.Tensor):
            mean = X.float().mean()
            std = X.float().std()
            X = (X - mean) / (std + 1e-8)
    # else: 'none' - no normalization
    
    # Convert data to PyTorch tensors
    if isinstance(X, np.ndarray):
        X = torch.tensor(X, dtype=torch.float32)
        y = torch.tensor(y, dtype=torch.long)
    elif not isinstance(y, np.ndarray) and not isinstance(y, torch.Tensor):
        raise TypeError("y must be a numpy array or a torch tensor.")
    elif isinstance(y, np.ndarray):
        y = torch.tensor(y, dtype=torch.long)


    # Create DataLoader
    dataset = torch.utils.data.TensorDataset(X, y)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Model initialization
    input_channels = X.shape[1]
    num_classes = len(torch.unique(y))
    model = CNN(cfg, input_channels=input_channels, num_classes=num_classes)

    # Optimizer selection
    if optimizer_name == 'Adam':
        beta1 = cfg.get('beta1')
        beta2 = cfg.get('beta2')
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2))
    elif optimizer_name == 'SGD':
        momentum = cfg.get('momentum')
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    elif optimizer_name == 'RMSprop':
        rho = cfg.get('rho')
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, alpha=rho)
    else:
        raise ValueError(f"Unknown optimizer: {{{{optimizer_name}}}}")

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in dataloader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

    # Calculate average loss
    loss = running_loss / len(dataloader)
    return float(loss)
```
--------------------------------------------------------------------------------
[2025-06-15 18:28:14] [Metadata: {'component': 'train_function', 'action': 'run'}] import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from ConfigSpace import Configuration
from typing import Any
import math


class CNN(nn.Module):
    def __init__(self, cfg: Configuration, input_channels: int = 1, num_classes: int = 10):
        super(CNN, self).__init__()

        self.num_conv_layers = cfg.get('num_conv_layers')
        self.num_filters_layer_1 = cfg.get('num_filters_layer_1')
        self.kernel_size = cfg.get('kernel_size')
        self.use_pooling = cfg.get('use_pooling')
        self.dropout_rate = cfg.get('dropout_rate')
        self.num_dense_layers = cfg.get('num_dense_layers')
        self.num_units_dense_1 = cfg.get('num_units_dense_1')
        self.use_batch_normalization = cfg.get('use_batch_normalization')

        layers = []
        in_channels = input_channels
        num_filters = self.num_filters_layer_1

        for i in range(self.num_conv_layers):
            layers.append(nn.Conv2d(in_channels, num_filters, kernel_size=self.kernel_size, padding=self.kernel_size // 2))
            if self.use_batch_normalization:
                layers.append(nn.BatchNorm2d(num_filters))
            layers.append(nn.ReLU())
            if self.use_pooling:
                layers.append(nn.MaxPool2d(2, 2))
            in_channels = num_filters
            num_filters *= 2  # Increase filters with each layer

        self.conv_layers = nn.Sequential(*layers)

        # Dynamically determine the size of the flattened features after conv layers
        self._to_linear = None

        # Dummy forward pass to calculate the output size of conv_layers
        def _determine_flattened_size(m, x):
            with torch.no_grad():
                y = m(x)
                return int(np.prod(y.size()[1:]))

        # Register a hook on the last conv layer if it's not empty
        if layers:
          dummy_input = torch.randn(1, input_channels, 28, 28)
          self._to_linear = _determine_flattened_size(self.conv_layers, dummy_input)


        dense_layers = []
        in_features = self._to_linear if self._to_linear is not None else input_channels * 28 * 28  # Handle cases with no conv layers
        num_units = self.num_units_dense_1
        
        for i in range(self.num_dense_layers - 1):
          dense_layers.append(nn.Linear(in_features, num_units))
          dense_layers.append(nn.ReLU())
          dense_layers.append(nn.Dropout(self.dropout_rate))
          in_features = num_units
          num_units //= 2 #reduce units each layer
        
        dense_layers.append(nn.Linear(in_features, num_classes))

        self.dense_layers = nn.Sequential(*dense_layers)
        
    def forward(self, x):
        if self.conv_layers:
            x = self.conv_layers(x)
            x = torch.flatten(x, 1)  # Flatten before dense layers
        else:
            x = torch.flatten(x, 1)

        x = self.dense_layers(x)
        return x


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a CNN model on the given dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dataset dictionary with 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: The training loss.
    """
    torch.manual_seed(seed)

    # Extract hyperparameters
    batch_size = cfg.get('batch_size')
    epochs = cfg.get('epochs')
    learning_rate = cfg.get('learning_rate')
    optimizer_name = cfg.get('optimizer')

    # Data preparation
    X = dataset['X']
    y = dataset['y']

    # Handle different input data shapes
    if isinstance(X, np.ndarray):
        if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)  # Reshape to (N, 1, H, W)
        elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    elif isinstance(X, torch.Tensor):
        if len(X.shape) == 2:
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)
        elif len(X.shape) == 3:
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])
    else:
        raise TypeError("X must be a numpy array or a torch tensor.")

    # Normalize data
    normalization_strategy = cfg.get('normalization_strategy')
    if normalization_strategy == 'scale':
        if isinstance(X, np.ndarray):
            X = X / 255.0
        elif isinstance(X, torch.Tensor):
            X = X / 255.0
    elif normalization_strategy == 'standardize':
        if isinstance(X, np.ndarray):
            mean = X.mean()
            std = X.std()
            X = (X - mean) / (std + 1e-8)  # Add a small constant to avoid division by zero
        elif isinstance(X, torch.Tensor):
            mean = X.float().mean()
            std = X.float().std()
            X = (X - mean) / (std + 1e-8)
    # else: 'none' - no normalization
    
    # Convert data to PyTorch tensors
    if isinstance(X, np.ndarray):
        X = torch.tensor(X, dtype=torch.float32)
        y = torch.tensor(y, dtype=torch.long)
    elif not isinstance(y, np.ndarray) and not isinstance(y, torch.Tensor):
        raise TypeError("y must be a numpy array or a torch tensor.")
    elif isinstance(y, np.ndarray):
        y = torch.tensor(y, dtype=torch.long)


    # Create DataLoader
    dataset = torch.utils.data.TensorDataset(X, y)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Model initialization
    input_channels = X.shape[1]
    num_classes = len(torch.unique(y))
    model = CNN(cfg, input_channels=input_channels, num_classes=num_classes)

    # Optimizer selection
    if optimizer_name == 'Adam':
        beta1 = cfg.get('beta1')
        beta2 = cfg.get('beta2')
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2))
    elif optimizer_name == 'SGD':
        momentum = cfg.get('momentum')
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    elif optimizer_name == 'RMSprop':
        rho = cfg.get('rho')
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, alpha=rho)
    else:
        raise ValueError(f"Unknown optimizer: {{{{optimizer_name}}}}")

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in dataloader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

    # Calculate average loss
    loss = running_loss / len(dataloader)
    return float(loss)

--------------------------------------------------------------------------------
[2025-06-15 18:28:14] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#3): X must be a numpy array or a torch tensor.
--------------------------------------------------------------------------------
[2025-06-15 18:28:23] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from ConfigSpace import Configuration
from typing import Any
import math


class CNN(nn.Module):
    def __init__(self, cfg: Configuration, input_channels: int = 1, num_classes: int = 10):
        super(CNN, self).__init__()

        self.num_conv_layers = cfg.get('num_conv_layers')
        self.num_filters_layer_1 = cfg.get('num_filters_layer_1')
        self.kernel_size = cfg.get('kernel_size')
        self.use_pooling = cfg.get('use_pooling')
        self.dropout_rate = cfg.get('dropout_rate')
        self.num_dense_layers = cfg.get('num_dense_layers')
        self.num_units_dense_1 = cfg.get('num_units_dense_1')
        self.use_batch_normalization = cfg.get('use_batch_normalization')

        layers = []
        in_channels = input_channels
        num_filters = self.num_filters_layer_1

        for i in range(self.num_conv_layers):
            layers.append(nn.Conv2d(in_channels, num_filters, kernel_size=self.kernel_size, padding=self.kernel_size // 2))
            if self.use_batch_normalization:
                layers.append(nn.BatchNorm2d(num_filters))
            layers.append(nn.ReLU())
            if self.use_pooling:
                layers.append(nn.MaxPool2d(2, 2))
            in_channels = num_filters
            num_filters *= 2  # Increase filters with each layer

        self.conv_layers = nn.Sequential(*layers)

        # Dynamically determine the size of the flattened features after conv layers
        self._to_linear = None

        # Dummy forward pass to calculate the output size of conv_layers
        def _determine_flattened_size(m, x):
            with torch.no_grad():
                y = m(x)
                return int(np.prod(y.size()[1:]))

        # Register a hook on the last conv layer if it's not empty
        if layers:
          dummy_input = torch.randn(1, input_channels, 28, 28)
          self._to_linear = _determine_flattened_size(self.conv_layers, dummy_input)


        dense_layers = []
        in_features = self._to_linear if self._to_linear is not None else input_channels * 28 * 28  # Handle cases with no conv layers
        num_units = self.num_units_dense_1
        
        for i in range(self.num_dense_layers - 1):
          dense_layers.append(nn.Linear(in_features, num_units))
          dense_layers.append(nn.ReLU())
          dense_layers.append(nn.Dropout(self.dropout_rate))
          in_features = num_units
          num_units //= 2 #reduce units each layer
        
        dense_layers.append(nn.Linear(in_features, num_classes))

        self.dense_layers = nn.Sequential(*dense_layers)
        
    def forward(self, x):
        if self.conv_layers:
            x = self.conv_layers(x)
            x = torch.flatten(x, 1)  # Flatten before dense layers
        else:
            x = torch.flatten(x, 1)

        x = self.dense_layers(x)
        return x


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a CNN model on the given dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dataset dictionary with 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: The training loss.
    """
    torch.manual_seed(seed)

    # Extract hyperparameters
    batch_size = cfg.get('batch_size')
    epochs = cfg.get('epochs')
    learning_rate = cfg.get('learning_rate')
    optimizer_name = cfg.get('optimizer')

    # Data preparation
    X = dataset['X']
    y = dataset['y']

    # Handle different input data shapes
    if isinstance(X, np.ndarray):
        if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)  # Reshape to (N, 1, H, W)
        elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    elif isinstance(X, torch.Tensor):
        if len(X.shape) == 2:
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)
        elif len(X.shape) == 3:
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])
    else:
        raise TypeError("X must be a numpy array or a torch tensor.")

    # Normalize data
    normalization_strategy = cfg.get('normalization_strategy')
    if normalization_strategy == 'scale':
        if isinstance(X, np.ndarray):
            X = X / 255.0
        elif isinstance(X, torch.Tensor):
            X = X / 255.0
    elif normalization_strategy == 'standardize':
        if isinstance(X, np.ndarray):
            mean = X.mean()
            std = X.std()
            X = (X - mean) / (std + 1e-8)  # Add a small constant to avoid division by zero
        elif isinstance(X, torch.Tensor):
            mean = X.float().mean()
            std = X.float().std()
            X = (X - mean) / (std + 1e-8)
    # else: 'none' - no normalization
    
    # Convert data to PyTorch tensors
    if isinstance(X, np.ndarray):
        X = torch.tensor(X, dtype=torch.float32)
    elif not isinstance(X, torch.Tensor):
        raise TypeError("X must be a numpy array or a torch tensor.")
        
    if isinstance(y, np.ndarray):
        y = torch.tensor(y, dtype=torch.long)
    elif not isinstance(y, torch.Tensor):
        raise TypeError("y must be a numpy array or a torch tensor.")

    # Create DataLoader
    dataset = torch.utils.data.TensorDataset(X, y)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Model initialization
    input_channels = X.shape[1]
    num_classes = len(torch.unique(y))
    model = CNN(cfg, input_channels=input_channels, num_classes=num_classes)

    # Optimizer selection
    if optimizer_name == 'Adam':
        beta1 = cfg.get('beta1')
        beta2 = cfg.get('beta2')
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2))
    elif optimizer_name == 'SGD':
        momentum = cfg.get('momentum')
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    elif optimizer_name == 'RMSprop':
        rho = cfg.get('rho')
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, alpha=rho)
    else:
        raise ValueError(f"Unknown optimizer: {{{{{{{{optimizer_name}}}}}}}}")

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in dataloader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

    # Calculate average loss
    loss = running_loss / len(dataloader)
    return float(loss)
```
--------------------------------------------------------------------------------
[2025-06-15 18:28:23] [Metadata: {'component': 'train_function', 'action': 'run'}] import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from ConfigSpace import Configuration
from typing import Any
import math


class CNN(nn.Module):
    def __init__(self, cfg: Configuration, input_channels: int = 1, num_classes: int = 10):
        super(CNN, self).__init__()

        self.num_conv_layers = cfg.get('num_conv_layers')
        self.num_filters_layer_1 = cfg.get('num_filters_layer_1')
        self.kernel_size = cfg.get('kernel_size')
        self.use_pooling = cfg.get('use_pooling')
        self.dropout_rate = cfg.get('dropout_rate')
        self.num_dense_layers = cfg.get('num_dense_layers')
        self.num_units_dense_1 = cfg.get('num_units_dense_1')
        self.use_batch_normalization = cfg.get('use_batch_normalization')

        layers = []
        in_channels = input_channels
        num_filters = self.num_filters_layer_1

        for i in range(self.num_conv_layers):
            layers.append(nn.Conv2d(in_channels, num_filters, kernel_size=self.kernel_size, padding=self.kernel_size // 2))
            if self.use_batch_normalization:
                layers.append(nn.BatchNorm2d(num_filters))
            layers.append(nn.ReLU())
            if self.use_pooling:
                layers.append(nn.MaxPool2d(2, 2))
            in_channels = num_filters
            num_filters *= 2  # Increase filters with each layer

        self.conv_layers = nn.Sequential(*layers)

        # Dynamically determine the size of the flattened features after conv layers
        self._to_linear = None

        # Dummy forward pass to calculate the output size of conv_layers
        def _determine_flattened_size(m, x):
            with torch.no_grad():
                y = m(x)
                return int(np.prod(y.size()[1:]))

        # Register a hook on the last conv layer if it's not empty
        if layers:
          dummy_input = torch.randn(1, input_channels, 28, 28)
          self._to_linear = _determine_flattened_size(self.conv_layers, dummy_input)


        dense_layers = []
        in_features = self._to_linear if self._to_linear is not None else input_channels * 28 * 28  # Handle cases with no conv layers
        num_units = self.num_units_dense_1
        
        for i in range(self.num_dense_layers - 1):
          dense_layers.append(nn.Linear(in_features, num_units))
          dense_layers.append(nn.ReLU())
          dense_layers.append(nn.Dropout(self.dropout_rate))
          in_features = num_units
          num_units //= 2 #reduce units each layer
        
        dense_layers.append(nn.Linear(in_features, num_classes))

        self.dense_layers = nn.Sequential(*dense_layers)
        
    def forward(self, x):
        if self.conv_layers:
            x = self.conv_layers(x)
            x = torch.flatten(x, 1)  # Flatten before dense layers
        else:
            x = torch.flatten(x, 1)

        x = self.dense_layers(x)
        return x


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a CNN model on the given dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dataset dictionary with 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: The training loss.
    """
    torch.manual_seed(seed)

    # Extract hyperparameters
    batch_size = cfg.get('batch_size')
    epochs = cfg.get('epochs')
    learning_rate = cfg.get('learning_rate')
    optimizer_name = cfg.get('optimizer')

    # Data preparation
    X = dataset['X']
    y = dataset['y']

    # Handle different input data shapes
    if isinstance(X, np.ndarray):
        if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)  # Reshape to (N, 1, H, W)
        elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    elif isinstance(X, torch.Tensor):
        if len(X.shape) == 2:
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)
        elif len(X.shape) == 3:
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])
    else:
        raise TypeError("X must be a numpy array or a torch tensor.")

    # Normalize data
    normalization_strategy = cfg.get('normalization_strategy')
    if normalization_strategy == 'scale':
        if isinstance(X, np.ndarray):
            X = X / 255.0
        elif isinstance(X, torch.Tensor):
            X = X / 255.0
    elif normalization_strategy == 'standardize':
        if isinstance(X, np.ndarray):
            mean = X.mean()
            std = X.std()
            X = (X - mean) / (std + 1e-8)  # Add a small constant to avoid division by zero
        elif isinstance(X, torch.Tensor):
            mean = X.float().mean()
            std = X.float().std()
            X = (X - mean) / (std + 1e-8)
    # else: 'none' - no normalization
    
    # Convert data to PyTorch tensors
    if isinstance(X, np.ndarray):
        X = torch.tensor(X, dtype=torch.float32)
    elif not isinstance(X, torch.Tensor):
        raise TypeError("X must be a numpy array or a torch tensor.")
        
    if isinstance(y, np.ndarray):
        y = torch.tensor(y, dtype=torch.long)
    elif not isinstance(y, torch.Tensor):
        raise TypeError("y must be a numpy array or a torch tensor.")

    # Create DataLoader
    dataset = torch.utils.data.TensorDataset(X, y)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Model initialization
    input_channels = X.shape[1]
    num_classes = len(torch.unique(y))
    model = CNN(cfg, input_channels=input_channels, num_classes=num_classes)

    # Optimizer selection
    if optimizer_name == 'Adam':
        beta1 = cfg.get('beta1')
        beta2 = cfg.get('beta2')
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2))
    elif optimizer_name == 'SGD':
        momentum = cfg.get('momentum')
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    elif optimizer_name == 'RMSprop':
        rho = cfg.get('rho')
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, alpha=rho)
    else:
        raise ValueError(f"Unknown optimizer: {{{{{{{{optimizer_name}}}}}}}}")

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in dataloader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

    # Calculate average loss
    loss = running_loss / len(dataloader)
    return float(loss)

--------------------------------------------------------------------------------
[2025-06-15 18:28:23] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#4): X must be a numpy array or a torch tensor.
--------------------------------------------------------------------------------
[2025-06-15 18:28:34] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from ConfigSpace import Configuration
from typing import Any
import math
import pandas as pd


class CNN(nn.Module):
    def __init__(self, cfg: Configuration, input_channels: int = 1, num_classes: int = 10):
        super(CNN, self).__init__()

        self.num_conv_layers = cfg.get('num_conv_layers')
        self.num_filters_layer_1 = cfg.get('num_filters_layer_1')
        self.kernel_size = cfg.get('kernel_size')
        self.use_pooling = cfg.get('use_pooling')
        self.dropout_rate = cfg.get('dropout_rate')
        self.num_dense_layers = cfg.get('num_dense_layers')
        self.num_units_dense_1 = cfg.get('num_units_dense_1')
        self.use_batch_normalization = cfg.get('use_batch_normalization')

        layers = []
        in_channels = input_channels
        num_filters = self.num_filters_layer_1

        for i in range(self.num_conv_layers):
            layers.append(nn.Conv2d(in_channels, num_filters, kernel_size=self.kernel_size, padding=self.kernel_size // 2))
            if self.use_batch_normalization:
                layers.append(nn.BatchNorm2d(num_filters))
            layers.append(nn.ReLU())
            if self.use_pooling:
                layers.append(nn.MaxPool2d(2, 2))
            in_channels = num_filters
            num_filters *= 2  # Increase filters with each layer

        self.conv_layers = nn.Sequential(*layers)

        # Dynamically determine the size of the flattened features after conv layers
        self._to_linear = None

        # Dummy forward pass to calculate the output size of conv_layers
        def _determine_flattened_size(m, x):
            with torch.no_grad():
                y = m(x)
                return int(np.prod(y.size()[1:]))

        # Register a hook on the last conv layer if it's not empty
        if layers:
          dummy_input = torch.randn(1, input_channels, 28, 28)
          self._to_linear = _determine_flattened_size(self.conv_layers, dummy_input)


        dense_layers = []
        in_features = self._to_linear if self._to_linear is not None else input_channels * 28 * 28  # Handle cases with no conv layers
        num_units = self.num_units_dense_1
        
        for i in range(self.num_dense_layers - 1):
          dense_layers.append(nn.Linear(in_features, num_units))
          dense_layers.append(nn.ReLU())
          dense_layers.append(nn.Dropout(self.dropout_rate))
          in_features = num_units
          num_units //= 2 #reduce units each layer
        
        dense_layers.append(nn.Linear(in_features, num_classes))

        self.dense_layers = nn.Sequential(*dense_layers)
        
    def forward(self, x):
        if self.conv_layers:
            x = self.conv_layers(x)
            x = torch.flatten(x, 1)  # Flatten before dense layers
        else:
            x = torch.flatten(x, 1)

        x = self.dense_layers(x)
        return x


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a CNN model on the given dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dataset dictionary with 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: The training loss.
    """
    torch.manual_seed(seed)

    # Extract hyperparameters
    batch_size = cfg.get('batch_size')
    epochs = cfg.get('epochs')
    learning_rate = cfg.get('learning_rate')
    optimizer_name = cfg.get('optimizer')

    # Data preparation
    X = dataset['X']
    y = dataset['y']

    # Handle different input data shapes
    if isinstance(X, np.ndarray):
        if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)  # Reshape to (N, 1, H, W)
        elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    elif isinstance(X, torch.Tensor):
        if len(X.shape) == 2:
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)
        elif len(X.shape) == 3:
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])
    elif isinstance(X, pd.DataFrame):
         X = X.values
         if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)  # Reshape to (N, 1, H, W)
         elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    else:
        raise TypeError("X must be a numpy array or a torch tensor.")

    # Normalize data
    normalization_strategy = cfg.get('normalization_strategy')
    if normalization_strategy == 'scale':
        if isinstance(X, np.ndarray):
            X = X / 255.0
        elif isinstance(X, torch.Tensor):
            X = X / 255.0
        else:
            X = X / 255.0
    elif normalization_strategy == 'standardize':
        if isinstance(X, np.ndarray):
            mean = X.mean()
            std = X.std()
            X = (X - mean) / (std + 1e-8)  # Add a small constant to avoid division by zero
        elif isinstance(X, torch.Tensor):
            mean = X.float().mean()
            std = X.float().std()
            X = (X - mean) / (std + 1e-8)
        else:
            mean = X.mean()
            std = X.std()
            X = (X - mean) / (std + 1e-8)
    # else: 'none' - no normalization
    
    # Convert data to PyTorch tensors
    if isinstance(X, np.ndarray):
        X = torch.tensor(X, dtype=torch.float32)
    elif not isinstance(X, torch.Tensor):
        X = torch.tensor(X, dtype=torch.float32)
        
    if isinstance(y, np.ndarray):
        y = torch.tensor(y, dtype=torch.long)
    elif not isinstance(y, torch.Tensor):
        y = torch.tensor(y, dtype=torch.long)

    # Create DataLoader
    dataset = torch.utils.data.TensorDataset(X, y)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Model initialization
    input_channels = X.shape[1]
    num_classes = len(torch.unique(y))
    model = CNN(cfg, input_channels=input_channels, num_classes=num_classes)

    # Optimizer selection
    if optimizer_name == 'Adam':
        beta1 = cfg.get('beta1')
        beta2 = cfg.get('beta2')
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2))
    elif optimizer_name == 'SGD':
        momentum = cfg.get('momentum')
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    elif optimizer_name == 'RMSprop':
        rho = cfg.get('rho')
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, alpha=rho)
    else:
        raise ValueError(f"Unknown optimizer: {{{{{{{{{{{{{{{{optimizer_name}}}}}}}}}}}}}}}}")

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in dataloader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

    # Calculate average loss
    loss = running_loss / len(dataloader)
    return float(loss)
```
--------------------------------------------------------------------------------
[2025-06-15 18:28:34] [Metadata: {'component': 'train_function', 'action': 'run'}] import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from ConfigSpace import Configuration
from typing import Any
import math
import pandas as pd


class CNN(nn.Module):
    def __init__(self, cfg: Configuration, input_channels: int = 1, num_classes: int = 10):
        super(CNN, self).__init__()

        self.num_conv_layers = cfg.get('num_conv_layers')
        self.num_filters_layer_1 = cfg.get('num_filters_layer_1')
        self.kernel_size = cfg.get('kernel_size')
        self.use_pooling = cfg.get('use_pooling')
        self.dropout_rate = cfg.get('dropout_rate')
        self.num_dense_layers = cfg.get('num_dense_layers')
        self.num_units_dense_1 = cfg.get('num_units_dense_1')
        self.use_batch_normalization = cfg.get('use_batch_normalization')

        layers = []
        in_channels = input_channels
        num_filters = self.num_filters_layer_1

        for i in range(self.num_conv_layers):
            layers.append(nn.Conv2d(in_channels, num_filters, kernel_size=self.kernel_size, padding=self.kernel_size // 2))
            if self.use_batch_normalization:
                layers.append(nn.BatchNorm2d(num_filters))
            layers.append(nn.ReLU())
            if self.use_pooling:
                layers.append(nn.MaxPool2d(2, 2))
            in_channels = num_filters
            num_filters *= 2  # Increase filters with each layer

        self.conv_layers = nn.Sequential(*layers)

        # Dynamically determine the size of the flattened features after conv layers
        self._to_linear = None

        # Dummy forward pass to calculate the output size of conv_layers
        def _determine_flattened_size(m, x):
            with torch.no_grad():
                y = m(x)
                return int(np.prod(y.size()[1:]))

        # Register a hook on the last conv layer if it's not empty
        if layers:
          dummy_input = torch.randn(1, input_channels, 28, 28)
          self._to_linear = _determine_flattened_size(self.conv_layers, dummy_input)


        dense_layers = []
        in_features = self._to_linear if self._to_linear is not None else input_channels * 28 * 28  # Handle cases with no conv layers
        num_units = self.num_units_dense_1
        
        for i in range(self.num_dense_layers - 1):
          dense_layers.append(nn.Linear(in_features, num_units))
          dense_layers.append(nn.ReLU())
          dense_layers.append(nn.Dropout(self.dropout_rate))
          in_features = num_units
          num_units //= 2 #reduce units each layer
        
        dense_layers.append(nn.Linear(in_features, num_classes))

        self.dense_layers = nn.Sequential(*dense_layers)
        
    def forward(self, x):
        if self.conv_layers:
            x = self.conv_layers(x)
            x = torch.flatten(x, 1)  # Flatten before dense layers
        else:
            x = torch.flatten(x, 1)

        x = self.dense_layers(x)
        return x


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a CNN model on the given dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dataset dictionary with 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: The training loss.
    """
    torch.manual_seed(seed)

    # Extract hyperparameters
    batch_size = cfg.get('batch_size')
    epochs = cfg.get('epochs')
    learning_rate = cfg.get('learning_rate')
    optimizer_name = cfg.get('optimizer')

    # Data preparation
    X = dataset['X']
    y = dataset['y']

    # Handle different input data shapes
    if isinstance(X, np.ndarray):
        if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)  # Reshape to (N, 1, H, W)
        elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    elif isinstance(X, torch.Tensor):
        if len(X.shape) == 2:
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)
        elif len(X.shape) == 3:
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])
    elif isinstance(X, pd.DataFrame):
         X = X.values
         if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)  # Reshape to (N, 1, H, W)
         elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    else:
        raise TypeError("X must be a numpy array or a torch tensor.")

    # Normalize data
    normalization_strategy = cfg.get('normalization_strategy')
    if normalization_strategy == 'scale':
        if isinstance(X, np.ndarray):
            X = X / 255.0
        elif isinstance(X, torch.Tensor):
            X = X / 255.0
        else:
            X = X / 255.0
    elif normalization_strategy == 'standardize':
        if isinstance(X, np.ndarray):
            mean = X.mean()
            std = X.std()
            X = (X - mean) / (std + 1e-8)  # Add a small constant to avoid division by zero
        elif isinstance(X, torch.Tensor):
            mean = X.float().mean()
            std = X.float().std()
            X = (X - mean) / (std + 1e-8)
        else:
            mean = X.mean()
            std = X.std()
            X = (X - mean) / (std + 1e-8)
    # else: 'none' - no normalization
    
    # Convert data to PyTorch tensors
    if isinstance(X, np.ndarray):
        X = torch.tensor(X, dtype=torch.float32)
    elif not isinstance(X, torch.Tensor):
        X = torch.tensor(X, dtype=torch.float32)
        
    if isinstance(y, np.ndarray):
        y = torch.tensor(y, dtype=torch.long)
    elif not isinstance(y, torch.Tensor):
        y = torch.tensor(y, dtype=torch.long)

    # Create DataLoader
    dataset = torch.utils.data.TensorDataset(X, y)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Model initialization
    input_channels = X.shape[1]
    num_classes = len(torch.unique(y))
    model = CNN(cfg, input_channels=input_channels, num_classes=num_classes)

    # Optimizer selection
    if optimizer_name == 'Adam':
        beta1 = cfg.get('beta1')
        beta2 = cfg.get('beta2')
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2))
    elif optimizer_name == 'SGD':
        momentum = cfg.get('momentum')
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    elif optimizer_name == 'RMSprop':
        rho = cfg.get('rho')
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, alpha=rho)
    else:
        raise ValueError(f"Unknown optimizer: {{{{{{{{{{{{{{{{optimizer_name}}}}}}}}}}}}}}}}")

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in dataloader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

    # Calculate average loss
    loss = running_loss / len(dataloader)
    return float(loss)

--------------------------------------------------------------------------------
[2025-06-15 18:28:34] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#5): batch_size should be a positive integer value, but got batch_size=32
--------------------------------------------------------------------------------
[2025-06-15 18:28:43] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from ConfigSpace import Configuration
from typing import Any
import math
import pandas as pd


class CNN(nn.Module):
    def __init__(self, cfg: Configuration, input_channels: int = 1, num_classes: int = 10):
        super(CNN, self).__init__()

        self.num_conv_layers = cfg.get('num_conv_layers')
        self.num_filters_layer_1 = cfg.get('num_filters_layer_1')
        self.kernel_size = cfg.get('kernel_size')
        self.use_pooling = cfg.get('use_pooling')
        self.dropout_rate = cfg.get('dropout_rate')
        self.num_dense_layers = cfg.get('num_dense_layers')
        self.num_units_dense_1 = cfg.get('num_units_dense_1')
        self.use_batch_normalization = cfg.get('use_batch_normalization')

        layers = []
        in_channels = input_channels
        num_filters = self.num_filters_layer_1

        for i in range(self.num_conv_layers):
            layers.append(nn.Conv2d(in_channels, num_filters, kernel_size=self.kernel_size, padding=self.kernel_size // 2))
            if self.use_batch_normalization:
                layers.append(nn.BatchNorm2d(num_filters))
            layers.append(nn.ReLU())
            if self.use_pooling:
                layers.append(nn.MaxPool2d(2, 2))
            in_channels = num_filters
            num_filters *= 2  # Increase filters with each layer

        self.conv_layers = nn.Sequential(*layers)

        # Dynamically determine the size of the flattened features after conv layers
        self._to_linear = None

        # Dummy forward pass to calculate the output size of conv_layers
        def _determine_flattened_size(m, x):
            with torch.no_grad():
                y = m(x)
                return int(np.prod(y.size()[1:]))

        # Register a hook on the last conv layer if it's not empty
        if layers:
          dummy_input = torch.randn(1, input_channels, 28, 28)
          self._to_linear = _determine_flattened_size(self.conv_layers, dummy_input)


        dense_layers = []
        in_features = self._to_linear if self._to_linear is not None else input_channels * 28 * 28  # Handle cases with no conv layers
        num_units = self.num_units_dense_1
        
        for i in range(self.num_dense_layers - 1):
          dense_layers.append(nn.Linear(in_features, num_units))
          dense_layers.append(nn.ReLU())
          dense_layers.append(nn.Dropout(self.dropout_rate))
          in_features = num_units
          num_units //= 2 #reduce units each layer
        
        dense_layers.append(nn.Linear(in_features, num_classes))

        self.dense_layers = nn.Sequential(*dense_layers)
        
    def forward(self, x):
        if self.conv_layers:
            x = self.conv_layers(x)
            x = torch.flatten(x, 1)  # Flatten before dense layers
        else:
            x = torch.flatten(x, 1)

        x = self.dense_layers(x)
        return x


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a CNN model on the given dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dataset dictionary with 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: The training loss.
    """
    torch.manual_seed(seed)

    # Extract hyperparameters
    batch_size = cfg.get('batch_size')
    epochs = cfg.get('epochs')
    learning_rate = cfg.get('learning_rate')
    optimizer_name = cfg.get('optimizer')

    if not isinstance(batch_size, int) or batch_size <= 0:
        raise ValueError(f"batch_size should be a positive integer value, but got batch_size={batch_size}")

    # Data preparation
    X = dataset['X']
    y = dataset['y']

    # Handle different input data shapes
    if isinstance(X, np.ndarray):
        if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)  # Reshape to (N, 1, H, W)
        elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    elif isinstance(X, torch.Tensor):
        if len(X.shape) == 2:
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)
        elif len(X.shape) == 3:
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])
    elif isinstance(X, pd.DataFrame):
        X = X.values
        if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)  # Reshape to (N, 1, H, W)
        elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    else:
        raise TypeError("X must be a numpy array, torch tensor, or Pandas DataFrame.")

    # Normalize data
    normalization_strategy = cfg.get('normalization_strategy')
    if normalization_strategy == 'scale':
        X = X / 255.0
    elif normalization_strategy == 'standardize':
        mean = X.mean()
        std = X.std()
        X = (X - mean) / (std + 1e-8)  # Add a small constant to avoid division by zero
    # else: 'none' - no normalization
    
    # Convert data to PyTorch tensors
    if not isinstance(X, torch.Tensor):
        X = torch.tensor(X, dtype=torch.float32)
        
    if not isinstance(y, torch.Tensor):
        y = torch.tensor(y, dtype=torch.long)

    # Create DataLoader
    dataset = torch.utils.data.TensorDataset(X, y)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Model initialization
    input_channels = X.shape[1]
    num_classes = len(torch.unique(y))
    model = CNN(cfg, input_channels=input_channels, num_classes=num_classes)

    # Optimizer selection
    if optimizer_name == 'Adam':
        beta1 = cfg.get('beta1')
        beta2 = cfg.get('beta2')
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2))
    elif optimizer_name == 'SGD':
        momentum = cfg.get('momentum')
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    elif optimizer_name == 'RMSprop':
        rho = cfg.get('rho')
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, alpha=rho)
    else:
        raise ValueError(f"Unknown optimizer: {optimizer_name}")

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in dataloader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

    # Calculate average loss
    loss = running_loss / len(dataloader)
    return float(loss)
```
--------------------------------------------------------------------------------
[2025-06-15 18:28:43] [Metadata: {'component': 'train_function', 'action': 'run'}] import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from ConfigSpace import Configuration
from typing import Any
import math
import pandas as pd


class CNN(nn.Module):
    def __init__(self, cfg: Configuration, input_channels: int = 1, num_classes: int = 10):
        super(CNN, self).__init__()

        self.num_conv_layers = cfg.get('num_conv_layers')
        self.num_filters_layer_1 = cfg.get('num_filters_layer_1')
        self.kernel_size = cfg.get('kernel_size')
        self.use_pooling = cfg.get('use_pooling')
        self.dropout_rate = cfg.get('dropout_rate')
        self.num_dense_layers = cfg.get('num_dense_layers')
        self.num_units_dense_1 = cfg.get('num_units_dense_1')
        self.use_batch_normalization = cfg.get('use_batch_normalization')

        layers = []
        in_channels = input_channels
        num_filters = self.num_filters_layer_1

        for i in range(self.num_conv_layers):
            layers.append(nn.Conv2d(in_channels, num_filters, kernel_size=self.kernel_size, padding=self.kernel_size // 2))
            if self.use_batch_normalization:
                layers.append(nn.BatchNorm2d(num_filters))
            layers.append(nn.ReLU())
            if self.use_pooling:
                layers.append(nn.MaxPool2d(2, 2))
            in_channels = num_filters
            num_filters *= 2  # Increase filters with each layer

        self.conv_layers = nn.Sequential(*layers)

        # Dynamically determine the size of the flattened features after conv layers
        self._to_linear = None

        # Dummy forward pass to calculate the output size of conv_layers
        def _determine_flattened_size(m, x):
            with torch.no_grad():
                y = m(x)
                return int(np.prod(y.size()[1:]))

        # Register a hook on the last conv layer if it's not empty
        if layers:
          dummy_input = torch.randn(1, input_channels, 28, 28)
          self._to_linear = _determine_flattened_size(self.conv_layers, dummy_input)


        dense_layers = []
        in_features = self._to_linear if self._to_linear is not None else input_channels * 28 * 28  # Handle cases with no conv layers
        num_units = self.num_units_dense_1
        
        for i in range(self.num_dense_layers - 1):
          dense_layers.append(nn.Linear(in_features, num_units))
          dense_layers.append(nn.ReLU())
          dense_layers.append(nn.Dropout(self.dropout_rate))
          in_features = num_units
          num_units //= 2 #reduce units each layer
        
        dense_layers.append(nn.Linear(in_features, num_classes))

        self.dense_layers = nn.Sequential(*dense_layers)
        
    def forward(self, x):
        if self.conv_layers:
            x = self.conv_layers(x)
            x = torch.flatten(x, 1)  # Flatten before dense layers
        else:
            x = torch.flatten(x, 1)

        x = self.dense_layers(x)
        return x


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a CNN model on the given dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dataset dictionary with 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: The training loss.
    """
    torch.manual_seed(seed)

    # Extract hyperparameters
    batch_size = cfg.get('batch_size')
    epochs = cfg.get('epochs')
    learning_rate = cfg.get('learning_rate')
    optimizer_name = cfg.get('optimizer')

    if not isinstance(batch_size, int) or batch_size <= 0:
        raise ValueError(f"batch_size should be a positive integer value, but got batch_size={batch_size}")

    # Data preparation
    X = dataset['X']
    y = dataset['y']

    # Handle different input data shapes
    if isinstance(X, np.ndarray):
        if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)  # Reshape to (N, 1, H, W)
        elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    elif isinstance(X, torch.Tensor):
        if len(X.shape) == 2:
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)
        elif len(X.shape) == 3:
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])
    elif isinstance(X, pd.DataFrame):
        X = X.values
        if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)  # Reshape to (N, 1, H, W)
        elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    else:
        raise TypeError("X must be a numpy array, torch tensor, or Pandas DataFrame.")

    # Normalize data
    normalization_strategy = cfg.get('normalization_strategy')
    if normalization_strategy == 'scale':
        X = X / 255.0
    elif normalization_strategy == 'standardize':
        mean = X.mean()
        std = X.std()
        X = (X - mean) / (std + 1e-8)  # Add a small constant to avoid division by zero
    # else: 'none' - no normalization
    
    # Convert data to PyTorch tensors
    if not isinstance(X, torch.Tensor):
        X = torch.tensor(X, dtype=torch.float32)
        
    if not isinstance(y, torch.Tensor):
        y = torch.tensor(y, dtype=torch.long)

    # Create DataLoader
    dataset = torch.utils.data.TensorDataset(X, y)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Model initialization
    input_channels = X.shape[1]
    num_classes = len(torch.unique(y))
    model = CNN(cfg, input_channels=input_channels, num_classes=num_classes)

    # Optimizer selection
    if optimizer_name == 'Adam':
        beta1 = cfg.get('beta1')
        beta2 = cfg.get('beta2')
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2))
    elif optimizer_name == 'SGD':
        momentum = cfg.get('momentum')
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    elif optimizer_name == 'RMSprop':
        rho = cfg.get('rho')
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, alpha=rho)
    else:
        raise ValueError(f"Unknown optimizer: {optimizer_name}")

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in dataloader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

    # Calculate average loss
    loss = running_loss / len(dataloader)
    return float(loss)

--------------------------------------------------------------------------------
[2025-06-15 18:28:43] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#6): batch_size should be a positive integer value, but got batch_size=64
--------------------------------------------------------------------------------
[2025-06-15 18:28:54] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from ConfigSpace import Configuration
from typing import Any
import math
import pandas as pd


class CNN(nn.Module):
    def __init__(self, cfg: Configuration, input_channels: int = 1, num_classes: int = 10):
        super(CNN, self).__init__()

        self.num_conv_layers = cfg.get('num_conv_layers')
        self.num_filters_layer_1 = cfg.get('num_filters_layer_1')
        self.kernel_size = cfg.get('kernel_size')
        self.use_pooling = cfg.get('use_pooling')
        self.dropout_rate = cfg.get('dropout_rate')
        self.num_dense_layers = cfg.get('num_dense_layers')
        self.num_units_dense_1 = cfg.get('num_units_dense_1')
        self.use_batch_normalization = cfg.get('use_batch_normalization')

        layers = []
        in_channels = input_channels
        num_filters = self.num_filters_layer_1

        for i in range(self.num_conv_layers):
            layers.append(nn.Conv2d(in_channels, num_filters, kernel_size=self.kernel_size, padding=self.kernel_size // 2))
            if self.use_batch_normalization:
                layers.append(nn.BatchNorm2d(num_filters))
            layers.append(nn.ReLU())
            if self.use_pooling:
                layers.append(nn.MaxPool2d(2, 2))
            in_channels = num_filters
            num_filters *= 2  # Increase filters with each layer

        self.conv_layers = nn.Sequential(*layers)

        # Dynamically determine the size of the flattened features after conv layers
        self._to_linear = None

        # Dummy forward pass to calculate the output size of conv_layers
        def _determine_flattened_size(m, x):
            with torch.no_grad():
                y = m(x)
                return int(np.prod(y.size()[1:]))

        # Register a hook on the last conv layer if it's not empty
        if layers:
          dummy_input = torch.randn(1, input_channels, 28, 28)
          self._to_linear = _determine_flattened_size(self.conv_layers, dummy_input)


        dense_layers = []
        in_features = self._to_linear if self._to_linear is not None else input_channels * 28 * 28  # Handle cases with no conv layers
        num_units = self.num_units_dense_1
        
        for i in range(self.num_dense_layers - 1):
          dense_layers.append(nn.Linear(in_features, num_units))
          dense_layers.append(nn.ReLU())
          dense_layers.append(nn.Dropout(self.dropout_rate))
          in_features = num_units
          num_units //= 2 #reduce units each layer
        
        dense_layers.append(nn.Linear(in_features, num_classes))

        self.dense_layers = nn.Sequential(*dense_layers)
        
    def forward(self, x):
        if self.conv_layers:
            x = self.conv_layers(x)
            x = torch.flatten(x, 1)  # Flatten before dense layers
        else:
            x = torch.flatten(x, 1)

        x = self.dense_layers(x)
        return x


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a CNN model on the given dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dataset dictionary with 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: The training loss.
    """
    torch.manual_seed(seed)

    # Extract hyperparameters
    batch_size = cfg.get('batch_size')
    epochs = cfg.get('epochs')
    learning_rate = cfg.get('learning_rate')
    optimizer_name = cfg.get('optimizer')

    if not isinstance(batch_size, int) or batch_size <= 0:
        raise ValueError(f"batch_size should be a positive integer value, but got batch_size={batch_size}")

    # Data preparation
    X = dataset['X']
    y = dataset['y']

    # Handle different input data shapes
    if isinstance(X, np.ndarray):
        X = np.array(X)  # Ensure X is a NumPy array
        if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)  # Reshape to (N, 1, H, W)
        elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    elif isinstance(X, torch.Tensor):
        if len(X.shape) == 2:
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)
        elif len(X.shape) == 3:
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])
    elif isinstance(X, pd.DataFrame):
        X = X.values
        X = np.array(X) #convert dataframe to numpy
        if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)  # Reshape to (N, 1, H, W)
        elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    else:
        raise TypeError("X must be a numpy array, torch tensor, or Pandas DataFrame.")

    # Normalize data
    normalization_strategy = cfg.get('normalization_strategy')
    if normalization_strategy == 'scale':
        X = X / 255.0
    elif normalization_strategy == 'standardize':
        mean = X.mean()
        std = X.std()
        X = (X - mean) / (std + 1e-8)  # Add a small constant to avoid division by zero
    # else: 'none' - no normalization
    
    # Convert data to PyTorch tensors
    if not isinstance(X, torch.Tensor):
        X = torch.tensor(X, dtype=torch.float32)
        
    if not isinstance(y, torch.Tensor):
        y = torch.tensor(y, dtype=torch.long)

    # Create DataLoader
    dataset = torch.utils.data.TensorDataset(X, y)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Model initialization
    input_channels = X.shape[1]
    num_classes = len(torch.unique(y))
    model = CNN(cfg, input_channels=input_channels, num_classes=num_classes)

    # Optimizer selection
    if optimizer_name == 'Adam':
        beta1 = cfg.get('beta1')
        beta2 = cfg.get('beta2')
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2))
    elif optimizer_name == 'SGD':
        momentum = cfg.get('momentum')
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    elif optimizer_name == 'RMSprop':
        rho = cfg.get('rho')
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, alpha=rho)
    else:
        raise ValueError(f"Unknown optimizer: {optimizer_name}")

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in dataloader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

    # Calculate average loss
    loss = running_loss / len(dataloader)
    return float(loss)
```
--------------------------------------------------------------------------------
[2025-06-15 18:28:54] [Metadata: {'component': 'train_function', 'action': 'run'}] import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from ConfigSpace import Configuration
from typing import Any
import math
import pandas as pd


class CNN(nn.Module):
    def __init__(self, cfg: Configuration, input_channels: int = 1, num_classes: int = 10):
        super(CNN, self).__init__()

        self.num_conv_layers = cfg.get('num_conv_layers')
        self.num_filters_layer_1 = cfg.get('num_filters_layer_1')
        self.kernel_size = cfg.get('kernel_size')
        self.use_pooling = cfg.get('use_pooling')
        self.dropout_rate = cfg.get('dropout_rate')
        self.num_dense_layers = cfg.get('num_dense_layers')
        self.num_units_dense_1 = cfg.get('num_units_dense_1')
        self.use_batch_normalization = cfg.get('use_batch_normalization')

        layers = []
        in_channels = input_channels
        num_filters = self.num_filters_layer_1

        for i in range(self.num_conv_layers):
            layers.append(nn.Conv2d(in_channels, num_filters, kernel_size=self.kernel_size, padding=self.kernel_size // 2))
            if self.use_batch_normalization:
                layers.append(nn.BatchNorm2d(num_filters))
            layers.append(nn.ReLU())
            if self.use_pooling:
                layers.append(nn.MaxPool2d(2, 2))
            in_channels = num_filters
            num_filters *= 2  # Increase filters with each layer

        self.conv_layers = nn.Sequential(*layers)

        # Dynamically determine the size of the flattened features after conv layers
        self._to_linear = None

        # Dummy forward pass to calculate the output size of conv_layers
        def _determine_flattened_size(m, x):
            with torch.no_grad():
                y = m(x)
                return int(np.prod(y.size()[1:]))

        # Register a hook on the last conv layer if it's not empty
        if layers:
          dummy_input = torch.randn(1, input_channels, 28, 28)
          self._to_linear = _determine_flattened_size(self.conv_layers, dummy_input)


        dense_layers = []
        in_features = self._to_linear if self._to_linear is not None else input_channels * 28 * 28  # Handle cases with no conv layers
        num_units = self.num_units_dense_1
        
        for i in range(self.num_dense_layers - 1):
          dense_layers.append(nn.Linear(in_features, num_units))
          dense_layers.append(nn.ReLU())
          dense_layers.append(nn.Dropout(self.dropout_rate))
          in_features = num_units
          num_units //= 2 #reduce units each layer
        
        dense_layers.append(nn.Linear(in_features, num_classes))

        self.dense_layers = nn.Sequential(*dense_layers)
        
    def forward(self, x):
        if self.conv_layers:
            x = self.conv_layers(x)
            x = torch.flatten(x, 1)  # Flatten before dense layers
        else:
            x = torch.flatten(x, 1)

        x = self.dense_layers(x)
        return x


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a CNN model on the given dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dataset dictionary with 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: The training loss.
    """
    torch.manual_seed(seed)

    # Extract hyperparameters
    batch_size = cfg.get('batch_size')
    epochs = cfg.get('epochs')
    learning_rate = cfg.get('learning_rate')
    optimizer_name = cfg.get('optimizer')

    if not isinstance(batch_size, int) or batch_size <= 0:
        raise ValueError(f"batch_size should be a positive integer value, but got batch_size={batch_size}")

    # Data preparation
    X = dataset['X']
    y = dataset['y']

    # Handle different input data shapes
    if isinstance(X, np.ndarray):
        X = np.array(X)  # Ensure X is a NumPy array
        if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)  # Reshape to (N, 1, H, W)
        elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    elif isinstance(X, torch.Tensor):
        if len(X.shape) == 2:
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)
        elif len(X.shape) == 3:
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])
    elif isinstance(X, pd.DataFrame):
        X = X.values
        X = np.array(X) #convert dataframe to numpy
        if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            height = width = int(math.sqrt(n_features))
            if height * width != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, height, width)  # Reshape to (N, 1, H, W)
        elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    else:
        raise TypeError("X must be a numpy array, torch tensor, or Pandas DataFrame.")

    # Normalize data
    normalization_strategy = cfg.get('normalization_strategy')
    if normalization_strategy == 'scale':
        X = X / 255.0
    elif normalization_strategy == 'standardize':
        mean = X.mean()
        std = X.std()
        X = (X - mean) / (std + 1e-8)  # Add a small constant to avoid division by zero
    # else: 'none' - no normalization
    
    # Convert data to PyTorch tensors
    if not isinstance(X, torch.Tensor):
        X = torch.tensor(X, dtype=torch.float32)
        
    if not isinstance(y, torch.Tensor):
        y = torch.tensor(y, dtype=torch.long)

    # Create DataLoader
    dataset = torch.utils.data.TensorDataset(X, y)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Model initialization
    input_channels = X.shape[1]
    num_classes = len(torch.unique(y))
    model = CNN(cfg, input_channels=input_channels, num_classes=num_classes)

    # Optimizer selection
    if optimizer_name == 'Adam':
        beta1 = cfg.get('beta1')
        beta2 = cfg.get('beta2')
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2))
    elif optimizer_name == 'SGD':
        momentum = cfg.get('momentum')
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    elif optimizer_name == 'RMSprop':
        rho = cfg.get('rho')
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, alpha=rho)
    else:
        raise ValueError(f"Unknown optimizer: {optimizer_name}")

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in dataloader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

    # Calculate average loss
    loss = running_loss / len(dataloader)
    return float(loss)

--------------------------------------------------------------------------------
[2025-06-15 18:28:54] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#7): batch_size should be a positive integer value, but got batch_size=128
--------------------------------------------------------------------------------
[2025-06-15 18:29:03] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from ConfigSpace import Configuration
from typing import Any
import math
import pandas as pd


class CNN(nn.Module):
    def __init__(self, cfg: Configuration, input_channels: int = 1, num_classes: int = 10):
        super(CNN, self).__init__()

        self.num_conv_layers = cfg.get('num_conv_layers')
        self.num_filters_layer_1 = cfg.get('num_filters_layer_1')
        self.kernel_size = cfg.get('kernel_size')
        self.use_pooling = cfg.get('use_pooling')
        self.dropout_rate = cfg.get('dropout_rate')
        self.num_dense_layers = cfg.get('num_dense_layers')
        self.num_units_dense_1 = cfg.get('num_units_dense_1')
        self.use_batch_normalization = cfg.get('use_batch_normalization')

        layers = []
        in_channels = input_channels
        num_filters = self.num_filters_layer_1

        for i in range(self.num_conv_layers):
            layers.append(nn.Conv2d(in_channels, num_filters, kernel_size=self.kernel_size, padding=self.kernel_size // 2))
            if self.use_batch_normalization:
                layers.append(nn.BatchNorm2d(num_filters))
            layers.append(nn.ReLU())
            if self.use_pooling:
                layers.append(nn.MaxPool2d(2, 2))
            in_channels = num_filters
            num_filters = num_filters * 2  # Increase filters with each layer

        self.conv_layers = nn.Sequential(*layers)

        # Dynamically determine the size of the flattened features after conv layers
        self._to_linear = None

        # Dummy forward pass to calculate the output size of conv_layers
        def _determine_flattened_size(m, x):
            with torch.no_grad():
                y = m(x)
                return int(np.prod(y.size()[1:]))

        # Register a hook on the last conv layer if it's not empty
        if layers:
            dummy_input = torch.randn(1, input_channels, 28, 28)
            self._to_linear = _determine_flattened_size(self.conv_layers, dummy_input)

        dense_layers = []
        in_features = self._to_linear if self._to_linear is not None else input_channels * 28 * 28  # Handle cases with no conv layers
        num_units = self.num_units_dense_1

        for i in range(self.num_dense_layers - 1):
            dense_layers.append(nn.Linear(in_features, num_units))
            dense_layers.append(nn.ReLU())
            dense_layers.append(nn.Dropout(self.dropout_rate))
            in_features = num_units
            num_units = num_units // 2  # reduce units each layer

        dense_layers.append(nn.Linear(in_features, num_classes))

        self.dense_layers = nn.Sequential(*dense_layers)

    def forward(self, x):
        if self.conv_layers:
            x = self.conv_layers(x)
            x = torch.flatten(x, 1)  # Flatten before dense layers
        else:
            x = torch.flatten(x, 1)

        x = self.dense_layers(x)
        return x


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a CNN model on the given dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dataset dictionary with 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: The training loss.
    """
    torch.manual_seed(seed)

    # Extract hyperparameters
    batch_size = cfg.get('batch_size')
    epochs = cfg.get('epochs')
    learning_rate = cfg.get('learning_rate')
    optimizer_name = cfg.get('optimizer')

    if not isinstance(batch_size, int) or batch_size <= 0:
        raise ValueError(f"batch_size should be a positive integer value, but got batch_size={batch_size}")

    # Data preparation
    X = dataset['X']
    y = dataset['y']

    # Handle different input data shapes
    if isinstance(X, np.ndarray):
        X = np.array(X)  # Ensure X is a NumPy array
        if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            side = int(math.sqrt(n_features))
            if side * side != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, side, side)  # Reshape to (N, 1, H, W)
        elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    elif isinstance(X, torch.Tensor):
        if len(X.shape) == 2:
            n_features = X.shape[1]
            side = int(math.sqrt(n_features))
            if side * side != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, side, side)
        elif len(X.shape) == 3:
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])
    elif isinstance(X, pd.DataFrame):
        X = X.values
        X = np.array(X)  # convert dataframe to numpy
        if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            side = int(math.sqrt(n_features))
            if side * side != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, side, side)  # Reshape to (N, 1, H, W)
        elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    else:
        raise TypeError("X must be a numpy array, torch tensor, or Pandas DataFrame.")

    # Normalize data
    normalization_strategy = cfg.get('normalization_strategy')
    if normalization_strategy == 'scale':
        X = X / 255.0
    elif normalization_strategy == 'standardize':
        mean = X.mean()
        std = X.std()
        X = (X - mean) / (std + 1e-8)  # Add a small constant to avoid division by zero
    # else: 'none' - no normalization

    # Convert data to PyTorch tensors
    if not isinstance(X, torch.Tensor):
        X = torch.tensor(X, dtype=torch.float32)

    if not isinstance(y, torch.Tensor):
        y = torch.tensor(y, dtype=torch.long)

    # Create DataLoader
    dataset = torch.utils.data.TensorDataset(X, y)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Model initialization
    input_channels = X.shape[1]
    num_classes = len(torch.unique(y))
    model = CNN(cfg, input_channels=input_channels, num_classes=num_classes)

    # Optimizer selection
    if optimizer_name == 'Adam':
        beta1 = cfg.get('beta1')
        beta2 = cfg.get('beta2')
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2))
    elif optimizer_name == 'SGD':
        momentum = cfg.get('momentum')
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    elif optimizer_name == 'RMSprop':
        rho = cfg.get('rho')
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, alpha=rho)
    else:
        raise ValueError(f"Unknown optimizer: {optimizer_name}")

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in dataloader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

    # Calculate average loss
    loss = running_loss / len(dataloader)
    return float(loss)
```
--------------------------------------------------------------------------------
[2025-06-15 18:29:03] [Metadata: {'component': 'train_function', 'action': 'run'}] import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from ConfigSpace import Configuration
from typing import Any
import math
import pandas as pd


class CNN(nn.Module):
    def __init__(self, cfg: Configuration, input_channels: int = 1, num_classes: int = 10):
        super(CNN, self).__init__()

        self.num_conv_layers = cfg.get('num_conv_layers')
        self.num_filters_layer_1 = cfg.get('num_filters_layer_1')
        self.kernel_size = cfg.get('kernel_size')
        self.use_pooling = cfg.get('use_pooling')
        self.dropout_rate = cfg.get('dropout_rate')
        self.num_dense_layers = cfg.get('num_dense_layers')
        self.num_units_dense_1 = cfg.get('num_units_dense_1')
        self.use_batch_normalization = cfg.get('use_batch_normalization')

        layers = []
        in_channels = input_channels
        num_filters = self.num_filters_layer_1

        for i in range(self.num_conv_layers):
            layers.append(nn.Conv2d(in_channels, num_filters, kernel_size=self.kernel_size, padding=self.kernel_size // 2))
            if self.use_batch_normalization:
                layers.append(nn.BatchNorm2d(num_filters))
            layers.append(nn.ReLU())
            if self.use_pooling:
                layers.append(nn.MaxPool2d(2, 2))
            in_channels = num_filters
            num_filters = num_filters * 2  # Increase filters with each layer

        self.conv_layers = nn.Sequential(*layers)

        # Dynamically determine the size of the flattened features after conv layers
        self._to_linear = None

        # Dummy forward pass to calculate the output size of conv_layers
        def _determine_flattened_size(m, x):
            with torch.no_grad():
                y = m(x)
                return int(np.prod(y.size()[1:]))

        # Register a hook on the last conv layer if it's not empty
        if layers:
            dummy_input = torch.randn(1, input_channels, 28, 28)
            self._to_linear = _determine_flattened_size(self.conv_layers, dummy_input)

        dense_layers = []
        in_features = self._to_linear if self._to_linear is not None else input_channels * 28 * 28  # Handle cases with no conv layers
        num_units = self.num_units_dense_1

        for i in range(self.num_dense_layers - 1):
            dense_layers.append(nn.Linear(in_features, num_units))
            dense_layers.append(nn.ReLU())
            dense_layers.append(nn.Dropout(self.dropout_rate))
            in_features = num_units
            num_units = num_units // 2  # reduce units each layer

        dense_layers.append(nn.Linear(in_features, num_classes))

        self.dense_layers = nn.Sequential(*dense_layers)

    def forward(self, x):
        if self.conv_layers:
            x = self.conv_layers(x)
            x = torch.flatten(x, 1)  # Flatten before dense layers
        else:
            x = torch.flatten(x, 1)

        x = self.dense_layers(x)
        return x


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a CNN model on the given dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dataset dictionary with 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: The training loss.
    """
    torch.manual_seed(seed)

    # Extract hyperparameters
    batch_size = cfg.get('batch_size')
    epochs = cfg.get('epochs')
    learning_rate = cfg.get('learning_rate')
    optimizer_name = cfg.get('optimizer')

    if not isinstance(batch_size, int) or batch_size <= 0:
        raise ValueError(f"batch_size should be a positive integer value, but got batch_size={batch_size}")

    # Data preparation
    X = dataset['X']
    y = dataset['y']

    # Handle different input data shapes
    if isinstance(X, np.ndarray):
        X = np.array(X)  # Ensure X is a NumPy array
        if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            side = int(math.sqrt(n_features))
            if side * side != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, side, side)  # Reshape to (N, 1, H, W)
        elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    elif isinstance(X, torch.Tensor):
        if len(X.shape) == 2:
            n_features = X.shape[1]
            side = int(math.sqrt(n_features))
            if side * side != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, side, side)
        elif len(X.shape) == 3:
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])
    elif isinstance(X, pd.DataFrame):
        X = X.values
        X = np.array(X)  # convert dataframe to numpy
        if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            side = int(math.sqrt(n_features))
            if side * side != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, side, side)  # Reshape to (N, 1, H, W)
        elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    else:
        raise TypeError("X must be a numpy array, torch tensor, or Pandas DataFrame.")

    # Normalize data
    normalization_strategy = cfg.get('normalization_strategy')
    if normalization_strategy == 'scale':
        X = X / 255.0
    elif normalization_strategy == 'standardize':
        mean = X.mean()
        std = X.std()
        X = (X - mean) / (std + 1e-8)  # Add a small constant to avoid division by zero
    # else: 'none' - no normalization

    # Convert data to PyTorch tensors
    if not isinstance(X, torch.Tensor):
        X = torch.tensor(X, dtype=torch.float32)

    if not isinstance(y, torch.Tensor):
        y = torch.tensor(y, dtype=torch.long)

    # Create DataLoader
    dataset = torch.utils.data.TensorDataset(X, y)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Model initialization
    input_channels = X.shape[1]
    num_classes = len(torch.unique(y))
    model = CNN(cfg, input_channels=input_channels, num_classes=num_classes)

    # Optimizer selection
    if optimizer_name == 'Adam':
        beta1 = cfg.get('beta1')
        beta2 = cfg.get('beta2')
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2))
    elif optimizer_name == 'SGD':
        momentum = cfg.get('momentum')
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    elif optimizer_name == 'RMSprop':
        rho = cfg.get('rho')
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, alpha=rho)
    else:
        raise ValueError(f"Unknown optimizer: {optimizer_name}")

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in dataloader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

    # Calculate average loss
    loss = running_loss / len(dataloader)
    return float(loss)

--------------------------------------------------------------------------------
[2025-06-15 18:29:03] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#8): batch_size should be a positive integer value, but got batch_size=128
--------------------------------------------------------------------------------
[2025-06-15 18:29:13] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from ConfigSpace import Configuration
from typing import Any
import math
import pandas as pd


class CNN(nn.Module):
    def __init__(self, cfg: Configuration, input_channels: int = 1, num_classes: int = 10):
        super(CNN, self).__init__()

        self.num_conv_layers = cfg.get('num_conv_layers')
        self.num_filters_layer_1 = cfg.get('num_filters_layer_1')
        self.kernel_size = cfg.get('kernel_size')
        self.use_pooling = cfg.get('use_pooling')
        self.dropout_rate = cfg.get('dropout_rate')
        self.num_dense_layers = cfg.get('num_dense_layers')
        self.num_units_dense_1 = cfg.get('num_units_dense_1')
        self.use_batch_normalization = cfg.get('use_batch_normalization')

        layers = []
        in_channels = input_channels
        num_filters = self.num_filters_layer_1

        for i in range(self.num_conv_layers):
            layers.append(nn.Conv2d(in_channels, num_filters, kernel_size=self.kernel_size, padding=self.kernel_size // 2))
            if self.use_batch_normalization:
                layers.append(nn.BatchNorm2d(num_filters))
            layers.append(nn.ReLU())
            if self.use_pooling:
                layers.append(nn.MaxPool2d(2, 2))
            in_channels = num_filters
            num_filters = num_filters * 2  # Increase filters with each layer

        self.conv_layers = nn.Sequential(*layers)

        # Dynamically determine the size of the flattened features after conv layers
        self._to_linear = None

        # Dummy forward pass to calculate the output size of conv_layers
        def _determine_flattened_size(m, x):
            with torch.no_grad():
                y = m(x)
                return int(np.prod(y.size()[1:]))

        # Register a hook on the last conv layer if it's not empty
        if layers:
            dummy_input = torch.randn(1, input_channels, 28, 28)
            self._to_linear = _determine_flattened_size(self.conv_layers, dummy_input)

        dense_layers = []
        in_features = self._to_linear if self._to_linear is not None else input_channels * 28 * 28  # Handle cases with no conv layers
        num_units = self.num_units_dense_1

        for i in range(self.num_dense_layers - 1):
            dense_layers.append(nn.Linear(in_features, num_units))
            dense_layers.append(nn.ReLU())
            dense_layers.append(nn.Dropout(self.dropout_rate))
            in_features = num_units
            num_units = num_units // 2  # reduce units each layer

        dense_layers.append(nn.Linear(in_features, num_classes))

        self.dense_layers = nn.Sequential(*dense_layers)

    def forward(self, x):
        if self.conv_layers:
            x = self.conv_layers(x)
            x = torch.flatten(x, 1)  # Flatten before dense layers
        else:
            x = torch.flatten(x, 1)

        x = self.dense_layers(x)
        return x


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a CNN model on the given dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dataset dictionary with 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: The training loss.
    """
    torch.manual_seed(seed)

    # Extract hyperparameters
    batch_size = cfg.get('batch_size')
    epochs = cfg.get('epochs')
    learning_rate = cfg.get('learning_rate')
    optimizer_name = cfg.get('optimizer')

    if not isinstance(batch_size, int) or batch_size <= 0:
        raise ValueError(f"batch_size should be a positive integer value, but got batch_size={batch_size}")

    # Data preparation
    X = dataset['X']
    y = dataset['y']

    # Handle different input data shapes
    if isinstance(X, np.ndarray):
        X = np.array(X)  # Ensure X is a NumPy array
        if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            side = int(math.sqrt(n_features))
            if side * side != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, side, side)  # Reshape to (N, 1, H, W)
        elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    elif isinstance(X, torch.Tensor):
        if len(X.shape) == 2:
            n_features = X.shape[1]
            side = int(math.sqrt(n_features))
            if side * side != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, side, side)
        elif len(X.shape) == 3:
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])
    elif isinstance(X, pd.DataFrame):
        X = X.values
        X = np.array(X)  # convert dataframe to numpy
        if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            side = int(math.sqrt(n_features))
            if side * side != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, side, side)  # Reshape to (N, 1, H, W)
        elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    else:
        raise TypeError("X must be a numpy array, torch tensor, or Pandas DataFrame.")

    # Normalize data
    normalization_strategy = cfg.get('normalization_strategy')
    if normalization_strategy == 'scale':
        X = X / 255.0
    elif normalization_strategy == 'standardize':
        mean = X.mean()
        std = X.std()
        X = (X - mean) / (std + 1e-8)  # Add a small constant to avoid division by zero
    # else: 'none' - no normalization

    # Convert data to PyTorch tensors
    if not isinstance(X, torch.Tensor):
        X = torch.tensor(X, dtype=torch.float32)

    if not isinstance(y, torch.Tensor):
        y = torch.tensor(y, dtype=torch.long)

    # Create DataLoader
    dataset = torch.utils.data.TensorDataset(X, y)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Model initialization
    input_channels = X.shape[1] if len(X.shape) == 4 else 1
    num_classes = len(torch.unique(y))
    model = CNN(cfg, input_channels=input_channels, num_classes=num_classes)

    # Optimizer selection
    if optimizer_name == 'Adam':
        beta1 = cfg.get('beta1')
        beta2 = cfg.get('beta2')
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2))
    elif optimizer_name == 'SGD':
        momentum = cfg.get('momentum')
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    elif optimizer_name == 'RMSprop':
        rho = cfg.get('rho')
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, alpha=rho)
    else:
        raise ValueError(f"Unknown optimizer: {optimizer_name}")

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in dataloader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

    # Calculate average loss
    loss = running_loss / len(dataloader)
    return float(loss)
```
--------------------------------------------------------------------------------
[2025-06-15 18:29:13] [Metadata: {'component': 'train_function', 'action': 'run'}] import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from ConfigSpace import Configuration
from typing import Any
import math
import pandas as pd


class CNN(nn.Module):
    def __init__(self, cfg: Configuration, input_channels: int = 1, num_classes: int = 10):
        super(CNN, self).__init__()

        self.num_conv_layers = cfg.get('num_conv_layers')
        self.num_filters_layer_1 = cfg.get('num_filters_layer_1')
        self.kernel_size = cfg.get('kernel_size')
        self.use_pooling = cfg.get('use_pooling')
        self.dropout_rate = cfg.get('dropout_rate')
        self.num_dense_layers = cfg.get('num_dense_layers')
        self.num_units_dense_1 = cfg.get('num_units_dense_1')
        self.use_batch_normalization = cfg.get('use_batch_normalization')

        layers = []
        in_channels = input_channels
        num_filters = self.num_filters_layer_1

        for i in range(self.num_conv_layers):
            layers.append(nn.Conv2d(in_channels, num_filters, kernel_size=self.kernel_size, padding=self.kernel_size // 2))
            if self.use_batch_normalization:
                layers.append(nn.BatchNorm2d(num_filters))
            layers.append(nn.ReLU())
            if self.use_pooling:
                layers.append(nn.MaxPool2d(2, 2))
            in_channels = num_filters
            num_filters = num_filters * 2  # Increase filters with each layer

        self.conv_layers = nn.Sequential(*layers)

        # Dynamically determine the size of the flattened features after conv layers
        self._to_linear = None

        # Dummy forward pass to calculate the output size of conv_layers
        def _determine_flattened_size(m, x):
            with torch.no_grad():
                y = m(x)
                return int(np.prod(y.size()[1:]))

        # Register a hook on the last conv layer if it's not empty
        if layers:
            dummy_input = torch.randn(1, input_channels, 28, 28)
            self._to_linear = _determine_flattened_size(self.conv_layers, dummy_input)

        dense_layers = []
        in_features = self._to_linear if self._to_linear is not None else input_channels * 28 * 28  # Handle cases with no conv layers
        num_units = self.num_units_dense_1

        for i in range(self.num_dense_layers - 1):
            dense_layers.append(nn.Linear(in_features, num_units))
            dense_layers.append(nn.ReLU())
            dense_layers.append(nn.Dropout(self.dropout_rate))
            in_features = num_units
            num_units = num_units // 2  # reduce units each layer

        dense_layers.append(nn.Linear(in_features, num_classes))

        self.dense_layers = nn.Sequential(*dense_layers)

    def forward(self, x):
        if self.conv_layers:
            x = self.conv_layers(x)
            x = torch.flatten(x, 1)  # Flatten before dense layers
        else:
            x = torch.flatten(x, 1)

        x = self.dense_layers(x)
        return x


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a CNN model on the given dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dataset dictionary with 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: The training loss.
    """
    torch.manual_seed(seed)

    # Extract hyperparameters
    batch_size = cfg.get('batch_size')
    epochs = cfg.get('epochs')
    learning_rate = cfg.get('learning_rate')
    optimizer_name = cfg.get('optimizer')

    if not isinstance(batch_size, int) or batch_size <= 0:
        raise ValueError(f"batch_size should be a positive integer value, but got batch_size={batch_size}")

    # Data preparation
    X = dataset['X']
    y = dataset['y']

    # Handle different input data shapes
    if isinstance(X, np.ndarray):
        X = np.array(X)  # Ensure X is a NumPy array
        if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            side = int(math.sqrt(n_features))
            if side * side != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, side, side)  # Reshape to (N, 1, H, W)
        elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    elif isinstance(X, torch.Tensor):
        if len(X.shape) == 2:
            n_features = X.shape[1]
            side = int(math.sqrt(n_features))
            if side * side != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, side, side)
        elif len(X.shape) == 3:
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])
    elif isinstance(X, pd.DataFrame):
        X = X.values
        X = np.array(X)  # convert dataframe to numpy
        if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            side = int(math.sqrt(n_features))
            if side * side != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, side, side)  # Reshape to (N, 1, H, W)
        elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    else:
        raise TypeError("X must be a numpy array, torch tensor, or Pandas DataFrame.")

    # Normalize data
    normalization_strategy = cfg.get('normalization_strategy')
    if normalization_strategy == 'scale':
        X = X / 255.0
    elif normalization_strategy == 'standardize':
        mean = X.mean()
        std = X.std()
        X = (X - mean) / (std + 1e-8)  # Add a small constant to avoid division by zero
    # else: 'none' - no normalization

    # Convert data to PyTorch tensors
    if not isinstance(X, torch.Tensor):
        X = torch.tensor(X, dtype=torch.float32)

    if not isinstance(y, torch.Tensor):
        y = torch.tensor(y, dtype=torch.long)

    # Create DataLoader
    dataset = torch.utils.data.TensorDataset(X, y)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Model initialization
    input_channels = X.shape[1] if len(X.shape) == 4 else 1
    num_classes = len(torch.unique(y))
    model = CNN(cfg, input_channels=input_channels, num_classes=num_classes)

    # Optimizer selection
    if optimizer_name == 'Adam':
        beta1 = cfg.get('beta1')
        beta2 = cfg.get('beta2')
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2))
    elif optimizer_name == 'SGD':
        momentum = cfg.get('momentum')
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    elif optimizer_name == 'RMSprop':
        rho = cfg.get('rho')
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, alpha=rho)
    else:
        raise ValueError(f"Unknown optimizer: {optimizer_name}")

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in dataloader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

    # Calculate average loss
    loss = running_loss / len(dataloader)
    return float(loss)

--------------------------------------------------------------------------------
[2025-06-15 18:29:13] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#9): batch_size should be a positive integer value, but got batch_size=32
--------------------------------------------------------------------------------
[2025-06-15 18:29:23] [Metadata: {'component': 'train_function', 'action': 'fix'}] ```python
import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from ConfigSpace import Configuration
from typing import Any
import math
import pandas as pd


class CNN(nn.Module):
    def __init__(self, cfg: Configuration, input_channels: int = 1, num_classes: int = 10):
        super(CNN, self).__init__()

        self.num_conv_layers = cfg.get('num_conv_layers')
        self.num_filters_layer_1 = cfg.get('num_filters_layer_1')
        self.kernel_size = cfg.get('kernel_size')
        self.use_pooling = cfg.get('use_pooling')
        self.dropout_rate = cfg.get('dropout_rate')
        self.num_dense_layers = cfg.get('num_dense_layers')
        self.num_units_dense_1 = cfg.get('num_units_dense_1')
        self.use_batch_normalization = cfg.get('use_batch_normalization')

        layers = []
        in_channels = input_channels
        num_filters = self.num_filters_layer_1

        for i in range(self.num_conv_layers):
            layers.append(nn.Conv2d(in_channels, num_filters, kernel_size=self.kernel_size, padding=self.kernel_size // 2))
            if self.use_batch_normalization:
                layers.append(nn.BatchNorm2d(num_filters))
            layers.append(nn.ReLU())
            if self.use_pooling:
                layers.append(nn.MaxPool2d(2, 2))
            in_channels = num_filters
            num_filters = num_filters * 2  # Increase filters with each layer

        self.conv_layers = nn.Sequential(*layers)

        # Dynamically determine the size of the flattened features after conv layers
        self._to_linear = None

        # Dummy forward pass to calculate the output size of conv_layers
        def _determine_flattened_size(m, x):
            with torch.no_grad():
                y = m(x)
                return int(np.prod(y.size()[1:]))

        # Register a hook on the last conv layer if it's not empty
        if layers:
            dummy_input = torch.randn(1, input_channels, 28, 28)
            self._to_linear = _determine_flattened_size(self.conv_layers, dummy_input)

        dense_layers = []
        in_features = self._to_linear if self._to_linear is not None else input_channels * 28 * 28  # Handle cases with no conv layers
        num_units = self.num_units_dense_1

        for i in range(self.num_dense_layers - 1):
            dense_layers.append(nn.Linear(in_features, num_units))
            dense_layers.append(nn.ReLU())
            dense_layers.append(nn.Dropout(self.dropout_rate))
            in_features = num_units
            num_units = num_units // 2  # reduce units each layer

        dense_layers.append(nn.Linear(in_features, num_classes))

        self.dense_layers = nn.Sequential(*dense_layers)

    def forward(self, x):
        if self.conv_layers:
            x = self.conv_layers(x)
            x = torch.flatten(x, 1)  # Flatten before dense layers
        else:
            x = torch.flatten(x, 1)

        x = self.dense_layers(x)
        return x


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a CNN model on the given dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dataset dictionary with 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: The training loss.
    """
    torch.manual_seed(seed)

    # Extract hyperparameters
    batch_size = cfg.get('batch_size')
    epochs = cfg.get('epochs')
    learning_rate = cfg.get('learning_rate')
    optimizer_name = cfg.get('optimizer')

    if not isinstance(batch_size, int) or batch_size <= 0:
        raise ValueError(f"batch_size should be a positive integer value, but got batch_size={batch_size}")

    # Data preparation
    X = dataset['X']
    y = dataset['y']

    # Handle different input data shapes
    if isinstance(X, np.ndarray):
        X = np.array(X)  # Ensure X is a NumPy array
        if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            side = int(math.sqrt(n_features))
            if side * side != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, side, side)  # Reshape to (N, 1, H, W)
        elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    elif isinstance(X, torch.Tensor):
        if len(X.shape) == 2:
            n_features = X.shape[1]
            side = int(math.sqrt(n_features))
            if side * side != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, side, side)
        elif len(X.shape) == 3:
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])
    elif isinstance(X, pd.DataFrame):
        X = X.values
        X = np.array(X)  # convert dataframe to numpy
        if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            side = int(math.sqrt(n_features))
            if side * side != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, side, side)  # Reshape to (N, 1, H, W)
        elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    else:
        raise TypeError("X must be a numpy array, torch tensor, or Pandas DataFrame.")

    # Normalize data
    normalization_strategy = cfg.get('normalization_strategy')
    if normalization_strategy == 'scale':
        X = X / 255.0
    elif normalization_strategy == 'standardize':
        mean = X.mean()
        std = X.std()
        X = (X - mean) / (std + 1e-8)  # Add a small constant to avoid division by zero
    # else: 'none' - no normalization

    # Convert data to PyTorch tensors
    if not isinstance(X, torch.Tensor):
        X = torch.tensor(X, dtype=torch.float32)

    if not isinstance(y, torch.Tensor):
        y = torch.tensor(y, dtype=torch.long)

    # Create DataLoader
    dataset = torch.utils.data.TensorDataset(X, y)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Model initialization
    input_channels = X.shape[1] if len(X.shape) == 4 else 1
    num_classes = len(torch.unique(y))
    model = CNN(cfg, input_channels=input_channels, num_classes=num_classes)

    # Optimizer selection
    if optimizer_name == 'Adam':
        beta1 = cfg.get('beta1')
        beta2 = cfg.get('beta2')
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2))
    elif optimizer_name == 'SGD':
        momentum = cfg.get('momentum')
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    elif optimizer_name == 'RMSprop':
        rho = cfg.get('rho')
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, alpha=rho)
    else:
        raise ValueError(f"Unknown optimizer: {optimizer_name}")

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in dataloader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

    # Calculate average loss
    loss = running_loss / len(dataloader)
    return float(loss)
```
--------------------------------------------------------------------------------
[2025-06-15 18:29:23] [Metadata: {'component': 'train_function', 'action': 'run'}] import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
from ConfigSpace import Configuration
from typing import Any
import math
import pandas as pd


class CNN(nn.Module):
    def __init__(self, cfg: Configuration, input_channels: int = 1, num_classes: int = 10):
        super(CNN, self).__init__()

        self.num_conv_layers = cfg.get('num_conv_layers')
        self.num_filters_layer_1 = cfg.get('num_filters_layer_1')
        self.kernel_size = cfg.get('kernel_size')
        self.use_pooling = cfg.get('use_pooling')
        self.dropout_rate = cfg.get('dropout_rate')
        self.num_dense_layers = cfg.get('num_dense_layers')
        self.num_units_dense_1 = cfg.get('num_units_dense_1')
        self.use_batch_normalization = cfg.get('use_batch_normalization')

        layers = []
        in_channels = input_channels
        num_filters = self.num_filters_layer_1

        for i in range(self.num_conv_layers):
            layers.append(nn.Conv2d(in_channels, num_filters, kernel_size=self.kernel_size, padding=self.kernel_size // 2))
            if self.use_batch_normalization:
                layers.append(nn.BatchNorm2d(num_filters))
            layers.append(nn.ReLU())
            if self.use_pooling:
                layers.append(nn.MaxPool2d(2, 2))
            in_channels = num_filters
            num_filters = num_filters * 2  # Increase filters with each layer

        self.conv_layers = nn.Sequential(*layers)

        # Dynamically determine the size of the flattened features after conv layers
        self._to_linear = None

        # Dummy forward pass to calculate the output size of conv_layers
        def _determine_flattened_size(m, x):
            with torch.no_grad():
                y = m(x)
                return int(np.prod(y.size()[1:]))

        # Register a hook on the last conv layer if it's not empty
        if layers:
            dummy_input = torch.randn(1, input_channels, 28, 28)
            self._to_linear = _determine_flattened_size(self.conv_layers, dummy_input)

        dense_layers = []
        in_features = self._to_linear if self._to_linear is not None else input_channels * 28 * 28  # Handle cases with no conv layers
        num_units = self.num_units_dense_1

        for i in range(self.num_dense_layers - 1):
            dense_layers.append(nn.Linear(in_features, num_units))
            dense_layers.append(nn.ReLU())
            dense_layers.append(nn.Dropout(self.dropout_rate))
            in_features = num_units
            num_units = num_units // 2  # reduce units each layer

        dense_layers.append(nn.Linear(in_features, num_classes))

        self.dense_layers = nn.Sequential(*dense_layers)

    def forward(self, x):
        if self.conv_layers:
            x = self.conv_layers(x)
            x = torch.flatten(x, 1)  # Flatten before dense layers
        else:
            x = torch.flatten(x, 1)

        x = self.dense_layers(x)
        return x


def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """
    Trains a CNN model on the given dataset.

    Args:
        cfg (Configuration): Configuration object containing hyperparameters.
        dataset (Any): Dataset dictionary with 'X' (features) and 'y' (labels).
        seed (int): Random seed for reproducibility.

    Returns:
        float: The training loss.
    """
    torch.manual_seed(seed)

    # Extract hyperparameters
    batch_size = cfg.get('batch_size')
    epochs = cfg.get('epochs')
    learning_rate = cfg.get('learning_rate')
    optimizer_name = cfg.get('optimizer')

    if not isinstance(batch_size, int) or batch_size <= 0:
        raise ValueError(f"batch_size should be a positive integer value, but got batch_size={batch_size}")

    # Data preparation
    X = dataset['X']
    y = dataset['y']

    # Handle different input data shapes
    if isinstance(X, np.ndarray):
        X = np.array(X)  # Ensure X is a NumPy array
        if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            side = int(math.sqrt(n_features))
            if side * side != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, side, side)  # Reshape to (N, 1, H, W)
        elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    elif isinstance(X, torch.Tensor):
        if len(X.shape) == 2:
            n_features = X.shape[1]
            side = int(math.sqrt(n_features))
            if side * side != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, side, side)
        elif len(X.shape) == 3:
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])
    elif isinstance(X, pd.DataFrame):
        X = X.values
        X = np.array(X)  # convert dataframe to numpy
        if len(X.shape) == 2:  # Flattened input
            n_features = X.shape[1]
            side = int(math.sqrt(n_features))
            if side * side != n_features:
                raise ValueError("Input features are not a perfect square.")
            X = X.reshape(-1, 1, side, side)  # Reshape to (N, 1, H, W)
        elif len(X.shape) == 3:  # 3D input (N, H, W)
            X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # Reshape to (N, 1, H, W)
    else:
        raise TypeError("X must be a numpy array, torch tensor, or Pandas DataFrame.")

    # Normalize data
    normalization_strategy = cfg.get('normalization_strategy')
    if normalization_strategy == 'scale':
        X = X / 255.0
    elif normalization_strategy == 'standardize':
        mean = X.mean()
        std = X.std()
        X = (X - mean) / (std + 1e-8)  # Add a small constant to avoid division by zero
    # else: 'none' - no normalization

    # Convert data to PyTorch tensors
    if not isinstance(X, torch.Tensor):
        X = torch.tensor(X, dtype=torch.float32)

    if not isinstance(y, torch.Tensor):
        y = torch.tensor(y, dtype=torch.long)

    # Create DataLoader
    dataset = torch.utils.data.TensorDataset(X, y)
    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)

    # Model initialization
    input_channels = X.shape[1] if len(X.shape) == 4 else 1
    num_classes = len(torch.unique(y))
    model = CNN(cfg, input_channels=input_channels, num_classes=num_classes)

    # Optimizer selection
    if optimizer_name == 'Adam':
        beta1 = cfg.get('beta1')
        beta2 = cfg.get('beta2')
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2))
    elif optimizer_name == 'SGD':
        momentum = cfg.get('momentum')
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    elif optimizer_name == 'RMSprop':
        rho = cfg.get('rho')
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, alpha=rho)
    else:
        raise ValueError(f"Unknown optimizer: {optimizer_name}")

    # Loss function
    criterion = nn.CrossEntropyLoss()

    # Training loop
    model.train()
    for epoch in range(epochs):
        running_loss = 0.0
        for inputs, labels in dataloader:
            optimizer.zero_grad()
            outputs = model(inputs)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()
            running_loss += loss.item()

    # Calculate average loss
    loss = running_loss / len(dataloader)
    return float(loss)

--------------------------------------------------------------------------------
[2025-06-15 18:29:23] [Metadata: {'error_type': 'TRAIN_FUNCTION_ERROR'}] [TRAIN_FUNCTION_ERROR] Error in train_function (#10): batch_size should be a positive integer value, but got batch_size=256
--------------------------------------------------------------------------------
[2025-06-15 18:29:31] [Metadata: {'component': 'train_function', 'action': 'run'}] import torch
import torch.nn as nn
import torch.optim as optim
from ConfigSpace import Configuration
from typing import Any
import numpy as np
import math

def train(cfg: Configuration, dataset: Any, seed: int) -> float:
    """Trains a CNN model on the given dataset according to the provided configuration."""
    torch.manual_seed(seed)

    # Data Preparation
    X, y = dataset['X'], dataset['y']
    X = np.asarray(X) # Ensure X is a NumPy array
    y = np.asarray(y) # Ensure y is a NumPy array

    # Data Type and Shape Validation
    if not isinstance(X, np.ndarray) or not isinstance(y, np.ndarray):
        raise ValueError("X and y must be NumPy arrays.")

    if len(X.shape) == 2:
        n_features = X.shape[1]
        height = width = int(math.sqrt(n_features))
        if height * width != n_features:
            raise ValueError("Input features cannot be reshaped into a square image.")
        X = X.reshape(-1, 1, height, width)  # NCHW
    elif len(X.shape) == 3:
        X = X.reshape(-1, 1, X.shape[1], X.shape[2])  # NCHW
    elif len(X.shape) == 4:
        pass # Assume correct format NCHW
    else:
        raise ValueError("Unsupported input data shape.")

    # Normalization
    normalization_strategy = cfg.get("normalization_strategy")
    if normalization_strategy == "scale":
        X = X / 255.0
    elif normalization_strategy == "standardize":
        X = (X - np.mean(X)) / np.std(X)

    X = torch.tensor(X, dtype=torch.float32)
    y = torch.tensor(y, dtype=torch.long)

    # Device Configuration
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Model Definition
    class CNN(nn.Module):
        def __init__(self, cfg: Configuration):
            super(CNN, self).__init__()

            self.num_conv_layers = cfg.get("num_conv_layers")
            self.num_filters_layer_1 = cfg.get("num_filters_layer_1")
            self.kernel_size = cfg.get("kernel_size")
            self.use_pooling = cfg.get("use_pooling")
            self.dropout_rate = cfg.get("dropout_rate")
            self.num_dense_layers = cfg.get("num_dense_layers")
            self.num_units_dense_1 = cfg.get("num_units_dense_1")
            self.use_batch_normalization = cfg.get("use_batch_normalization")

            layers = []
            in_channels = 1  # Assuming grayscale images

            for i in range(self.num_conv_layers):
                if i == 0:
                    layers.append(nn.Conv2d(in_channels, self.num_filters_layer_1, self.kernel_size))
                    out_channels = self.num_filters_layer_1
                else:
                    layers.append(nn.Conv2d(out_channels, out_channels * 2, self.kernel_size))
                    out_channels *= 2
                if self.use_batch_normalization:
                    layers.append(nn.BatchNorm2d(out_channels))
                layers.append(nn.ReLU())
                if self.use_pooling:
                    layers.append(nn.MaxPool2d(2, 2))
                layers.append(nn.Dropout(self.dropout_rate))

            self.conv_layers = nn.Sequential(*layers)

            # Calculate the output size after convolutional layers dynamically
            with torch.no_grad():
                dummy_input = torch.zeros(1, 1, 28, 28)
                x = self.conv_layers(dummy_input)
                conv_output_size = x.view(1, -1).size(1)


            dense_layers = []
            in_features = conv_output_size
            num_classes = 10 # Fixed number of classes

            for i in range(self.num_dense_layers):
                if i == 0:
                    dense_layers.append(nn.Linear(in_features, self.num_units_dense_1))
                    in_features = self.num_units_dense_1
                else:
                    dense_layers.append(nn.Linear(in_features, in_features // 2))
                    in_features = in_features // 2
                dense_layers.append(nn.ReLU())
                dense_layers.append(nn.Dropout(self.dropout_rate))

            dense_layers.append(nn.Linear(in_features, num_classes))
            self.dense_layers = nn.Sequential(*dense_layers)

        def forward(self, x):
            x = self.conv_layers(x)
            x = x.view(x.size(0), -1)  # Flatten
            x = self.dense_layers(x)
            return x

    model = CNN(cfg).to(device)

    # Optimizer
    optimizer_name = cfg.get("optimizer")
    learning_rate = cfg.get("learning_rate")
    if optimizer_name == "Adam":
        beta1 = cfg.get("beta1")
        beta2 = cfg.get("beta2")
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, betas=(beta1, beta2))
    elif optimizer_name == "SGD":
        momentum = cfg.get("momentum")
        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)
    elif optimizer_name == "RMSprop":
        rho = cfg.get("rho")
        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, alpha=rho)
    else:
        raise ValueError(f"Unsupported optimizer: {optimizer_name}")

    # Loss Function
    criterion = nn.CrossEntropyLoss()

    # Training Loop
    batch_size = cfg.get("batch_size")
    epochs = cfg.get("epochs")
    model.train()
    total_loss = 0.0
    for epoch in range(epochs):
        for i in range(0, len(X), batch_size):
            X_batch = X[i:i + batch_size].to(device)
            y_batch = y[i:i + batch_size].to(device)

            optimizer.zero_grad()
            outputs = model(X_batch)
            loss = criterion(outputs, y_batch)
            loss.backward()
            optimizer.step()
            total_loss += loss.item()

    return float(total_loss / epochs)

--------------------------------------------------------------------------------
