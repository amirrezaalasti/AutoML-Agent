**Generate a production-grade Python training function for machine learning with the following STRICT requirements:**

---

### ** CRITICAL CODE GENERATION RULE FOR HYPERPARAMETER OPTIMIZATION:**
**GENERATE ROBUST CODE THAT DOESN'T NEED ERROR HANDLING!**
* **Why**: In hyperparameter optimization, the function will be called with diverse configurations
* **HPO Context**: The training function must handle all configurations without crashing
* **Generate Robust Code**: Write code that naturally handles all valid configurations
* **Let Errors Propagate**: Don't hide errors - let them show so they can be fixed
* **This is a HARD REQUIREMENT**: Generate inherently robust code, don't mask problems

---

### ** CRITICAL HPO ROBUSTNESS RULE:**
**GENERATE CODE THAT NATURALLY HANDLES MULTI-ALGORITHM CONFIGURATIONS!**
* **Context**: Configuration space may contain hyperparameters for multiple algorithm families
* **Algorithm Selection**: Implement algorithms that match the configuration space definition
* **Parameter Extraction**: Extract and use all relevant parameters from the configuration
* **Safe Type Conversion**: Convert parameters to appropriate types with bounds checking
* **Default Values**: Use sensible defaults for all hyperparameters when not specified
* **Validate Ranges**: Ensure all parameters are within valid ranges before use
* **Multi-Algorithm Support**: If config space supports multiple algorithms, implement appropriate logic
* **Parameter Name Matching**: Use EXACT parameter names from ConfigSpace definition
* **Algorithm Detection**: Determine algorithm choice based on configuration parameters
* **This is a HARD REQUIREMENT**: Generate naturally robust code that handles configurations correctly

---

### ** CRITICAL GENERALIZATION RULE:**
**ABSOLUTELY NO HARDCODED DIMENSIONS OR SHAPES!**
* **Why**: The function must be reusable for different datasets. Hardcoding values makes the function brittle and useless.
* **Instead**: All dimensions **must** be inferred dynamically from the input `dataset` object.
* **Examples**:
    * **Get number of features from `X.shape[1]`**.
    * **Get number of samples from `X.shape[0]`**.
    * **Get number of classes from the unique values in `y`**.
* **This is a HARD REQUIREMENT**: Any code with hardcoded shapes will be rejected.

---
* **Selected Facade**: `{suggested_facade}`
* Recommended steps to write the train function based on the planner:
  * `{train_function_plan}`
---

### **Function signature** must be:

```python
from ConfigSpace import Configuration
from typing import Any, Dict, Union
def train(cfg: Configuration, dataset: Any, seed: int, budget: int = None, model_dir: str = None) -> Dict[str, Union[float, int]]:
```

**CRITICAL**: The function signature **MUST** include the `budget` parameter even if not used by all facades:
- For standard optimization facades: `budget` parameter will be `None` and should be ignored
- For multi-fidelity facades: `budget` parameter will contain the fidelity budget (e.g., iterations, components)
- **Always include the budget parameter** in the signature for compatibility with all facades
- Use conditional logic to handle the budget parameter appropriately based on the facade type

**Budget Parameter Handling Examples:**
```python
# For iterative algorithms
if budget is not None:
    max_iterations = budget
else:
    max_iterations = default_iterations

# For ensemble methods
if budget is not None:
    n_components = budget
else:
    n_components = default_components

# For data-based fidelity
if budget is not None:
    data_fraction = min(budget, 1.0)  # Ensure valid fraction
else:
    data_fraction = 1.0  # Use full dataset
```

---

### **Function Behavior Requirements:**

* The function **must** handle the dataset properly:
  * Dataset Description: `{dataset_description}`
  * ConfigSpace Definition (**very important for choosing the algorithm**): `{config_space}`
  * SMAC Scenario: `{scenario}`

* **CRITICAL**: The function **must** use the EXACT parameter names from the ConfigSpace definition above
  * **INSPECT** the ConfigSpace definition carefully and extract only those parameters that exist

* The function **must** accept a `dataset` dictionary with:
  * `dataset['X']`: feature matrix or input tensor
  * `dataset['y']`: label vector or label tensor
  * `dataset['X_test']`: (optional) test feature matrix - only present when separate test set is provided
  * `dataset['y_test']`: (optional) test label vector - only present when separate test set is provided

* **CRITICAL**: Conditional train/test split behavior
  * **When separate test set is provided** (`dataset['X_test']` and `dataset['y_test']` exist):
    * Use the entire `dataset['X']` and `dataset['y']` for training
    * Do NOT perform internal train/test splits
    * The test set is already provided separately
  * **When no separate test set is provided** (`dataset['X_test']` and `dataset['y_test']` are missing):
    * Perform internal train/test split on the provided dataset for validation
    * Use appropriate split ratio (e.g., 80/20 or 70/30)
    * Use the split for validation during training

* The function **must** handle the configuration properly:
  * Access primitive values using `cfg.get('key')`
  * Handle all hyperparameters defined in the configuration space
  * Apply proper type conversion and validation
  * Handle conditional hyperparameters correctly
  * Parameter types may include numpy types (numpy.int64, numpy.float64, etc.)

* **Algorithm Selection Requirements:**
  * Choose algorithms based on dataset characteristics and configuration space parameters
  * Implement algorithms that are appropriate for the given task type and data properties
  * Consider computational constraints and evaluation time requirements
  * Select frameworks and methods that match the dataset size and complexity
  * **Framework Selection**: Choose the most appropriate framework based on:
    * Dataset size and computational requirements
    * Algorithm type and implementation availability
    * Memory constraints and processing efficiency

* **Training Requirements:**
  * Implement proper training procedures for the selected algorithm
  * Handle batch processing efficiently when needed
  * Apply appropriate optimization strategies
  * Implement early stopping if beneficial for the algorithm
  * Handle validation appropriately
  * **Prevent Overfitting**: Use proper validation, regularization, early stopping
  * **Ensure Generalization**: Avoid fitting too closely to training data
  * Return a dictionary with loss and evaluation metrics

* **Model Saving Requirements:**
  * **STRONGLY RECOMMENDED**: Save the trained model if `model_dir` is provided
  * This enables proper model evaluation with comprehensive metrics
  * Create the directory if it doesn't exist
  * Save model in appropriate format for the chosen framework
  * Generate unique model filename using timestamp or configuration hash
  * **CRITICAL**: The function must return a dictionary with loss and metrics
    * **REASON**: Loss is used for hyperparameter optimization, metrics provide comprehensive evaluation

* **Return Format Requirements:**
  * **CRITICAL**: Return a dictionary with 'loss' key and evaluation metrics
  * **Metric Calculation**: Use validation data if available, otherwise use training data
  * **Consistent Format**: Always return the same dictionary keys for the task type

* **Performance Optimization Requirements:**
  * Minimize memory usage and allocations
  * Use vectorized operations where possible
  * Avoid unnecessary data copying
  * Optimize data loading and preprocessing
  * Use efficient data structures
  * Implement efficient processing for the chosen algorithm
  * **Resource Management**: Use computational resources efficiently based on dataset size

* **Code Optimization Requirements:**
  * Keep code minimal and focused
  * Avoid redundant computations
  * Use efficient algorithms
  * Minimize function calls
  * Optimize loops and iterations
  * Use appropriate data types
  * Avoid unnecessary object creation
  * The train function should be computationally efficient

* **Best Practices:**
  * Implement proper logging for progress tracking
  * Handle edge cases gracefully
  * Ensure reproducibility with proper random seed usage
  * Optimize performance for the dataset characteristics
  * Follow framework-specific best practices for the chosen implementation

---

### **Framework Selection Guidelines:**

Choose the most appropriate framework based on dataset characteristics and algorithm requirements:
* **Traditional ML algorithms**: Use efficient implementations for the dataset size
* **Deep learning approaches**: Select frameworks suitable for the architecture and data type
* **Ensemble methods**: Choose implementations that scale well with the dataset
* **Consider computational constraints and available resources**

---

### **Output Format:**

* Return **only** the `train()` function
* Include necessary imports
* No example usage or additional code
* The function must be self-contained and executable
* Code must be minimal and optimized for performance

---

### **Code Generation Best Practices for HPO:**

* **VALIDATE ALL INPUTS EXPLICITLY** - Check types, shapes, ranges before use
* **HANDLE MISSING PARAMETERS** - Use sensible defaults for all hyperparameters
* **CHECK DATA TYPES AND SHAPES** - Validate and convert before processing  
* **CONVERT TYPES SAFELY** - Convert budget and other parameters to appropriate types
* **BOUND ALL PARAMETERS** - Ensure parameters are within valid ranges
* **USE SENSIBLE DEFAULTS** - Provide reasonable defaults for all hyperparameters
* **IMPLEMENT MATCHING ALGORITHMS** - Implement algorithms that match the configuration space
* **USE EXACT PARAMETER NAMES** - Extract parameters using exact names from ConfigSpace definition
* **PREVENT OVERFITTING** - Use appropriate validation, regularization, and early stopping
* **HANDLE NUMPY TYPES** - Convert numpy.int64, numpy.float64 to native Python types
* **DYNAMIC DIMENSIONS** - Infer all dimensions from data, never hardcode shapes

---

### **General Structure Principles:**

**Your generated function should follow these principles:**

1. **Set random seed** for reproducibility
2. **Extract hyperparameters** from cfg with appropriate defaults and type conversion
3. **Validate and bound** all parameters to ensure they're within valid ranges
4. **Prepare data** efficiently, inferring dimensions dynamically
5. **Select and implement algorithm** based on configuration space and dataset characteristics
6. **Handle budget parameter** appropriately for the facade type
7. **Train the model** with the validated parameters
8. **Save the model** if model_dir is provided
9. **Return dictionary with loss and metrics**

**Do NOT copy specific implementation patterns** - generate code appropriate for the given configuration space and dataset requirements.

---

### **Example Structure (NO try/except):**

```python
def train(cfg: Configuration, dataset: Any, seed: int, budget: int = None, model_dir: str = None) -> Dict[str, Union[float, int]]:
    # Set random seed for reproducibility
    
    # Extract hyperparameters efficiently based on ConfigSpace definition
    
    # Prepare data efficiently - conditional train/test split behavior
    X, y = dataset['X'], dataset['y']
    
    # Check if separate test set is provided
    if 'X_test' in dataset and 'y_test' in dataset:
        # Use entire training set - test set provided separately
        X_train, y_train = X, y
        X_test, y_test = dataset['X_test'], dataset['y_test']
    else:
        # Perform internal train/test split for validation
        from sklearn.model_selection import train_test_split
        X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=seed)
        X_test, y_test = X_val, y_val
    
    # Select and initialize algorithm based on configuration and dataset characteristics
    
    # Handle budget parameter for fidelity control
    
    # Implement optimized training procedure
    
    # Save model if directory is provided (recommended for evaluation)
    if model_dir:
        os.makedirs(model_dir, exist_ok=True)
        model_path = os.path.join(model_dir, f"model_{{seed}}.pkl")
        # Save using appropriate method for chosen framework
    
    # Calculate evaluation metrics on validation/test data
    predictions = model.predict(X_test)  # Adapt for chosen framework
    
    # Return dictionary with loss and metrics
    return metrics
```

---

** FINAL REMINDER - CRITICAL REQUIREMENTS:**
* Valid `import` statements
* A single `train()` function that returns **a dictionary with loss and metrics**
* No additional code or explanations
* Code must be optimized for performance and minimal in size
* For accuracy metrics, use positive values (e.g. accuracy: 0.85)
* For error metrics, use raw error values (e.g. mse: 0.15)
* Ensure consistent sign convention across all metrics
* **STRONGLY RECOMMENDED**: Save the trained model if `model_dir` parameter is provided
* **MUST handle multi-algorithm configuration spaces** by implementing appropriate algorithms
* **MUST use EXACT parameter names** from ConfigSpace definition
* **MUST select algorithms** based on dataset characteristics and configuration parameters
* **MUST convert budget parameter** to appropriate type and validate range
* **MUST use sensible defaults** for all hyperparameters with proper type conversion
* **MUST validate parameter ranges** to ensure they're within acceptable bounds
* **MUST prevent overfitting** with proper validation and regularization
* **NO try/except blocks** - let errors propagate so they can be fixed
* **The train function MUST be naturally robust** for hyperparameter optimization scenarios
* **CRITICAL**: **Handle train/test split conditionally**
  * **When `X_test` and `y_test` are in dataset**: Use entire training set, do NOT split
  * **When `X_test` and `y_test` are NOT in dataset**: Perform internal train/test split for validation
* **Dataset Usage**: Check for separate test set availability and adapt accordingly
* **Metric Calculation**: Always calculate metrics on validation/test data, not training data
* **Return Format**: Dictionary with consistent keys based on task type (classification/regression)
* **CRITICAL**: Dictionary must always include `loss` key for hyperparameter optimization
* **Algorithm Selection**: Base algorithm choice on dataset characteristics, not predefined preferences