**Generate a production-grade Python training function for machine learning with the following STRICT requirements:**

---

## **CRITICAL CODE GENERATION RULE FOR HYPERPARAMETER OPTIMIZATION:**
**GENERATE ROBUST CODE THAT DOESN'T NEED ERROR HANDLING!**
* **Why**: In hyperparameter optimization, the function will be called with diverse configurations
* **HPO Context**: The training function must handle all configurations without crashing
* **Generate Robust Code**: Write code that naturally handles all valid configurations
* **Let Errors Propagate**: Don't hide errors - let them show so they can be fixed
* **This is a HARD REQUIREMENT**: Generate inherently robust code, don't mask problems

---

## **CRITICAL HPO ROBUSTNESS RULE:**
**GENERATE CODE THAT NATURALLY HANDLES MULTI-ALGORITHM CONFIGURATIONS!**
* **Context**: Configuration space may contain hyperparameters for multiple algorithm families
* **Algorithm Selection**: Implement algorithms that match the configuration space definition
* **Parameter Extraction**: Extract and use all relevant parameters from the configuration
* **Safe Type Conversion**: Convert parameters to appropriate types with bounds checking
* **Default Values**: Use sensible defaults for all hyperparameters when not specified
* **Validate Ranges**: Ensure all parameters are within valid ranges before use
* **Multi-Algorithm Support**: If config space supports multiple algorithms, implement appropriate logic
* **Parameter Name Matching**: Use EXACT parameter names from ConfigSpace definition
* **Algorithm Detection**: Determine algorithm choice based on configuration parameters
* **This is a HARD REQUIREMENT**: Generate naturally robust code that handles configurations correctly

---

## **CRITICAL GENERALIZATION RULE:**
**ABSOLUTELY NO HARDCODED DIMENSIONS OR SHAPES!**
* **Why**: The function must be reusable for different datasets. Hardcoding values makes the function brittle and useless.
* **Instead**: All dimensions **must** be inferred dynamically from the input `dataset` object.
* **Examples**:
    * **Get number of features from `X.shape[1]`**.
    * **Get number of samples from `X.shape[0]`**.
    * **Get number of classes from the unique values in `y`**.
* **This is a HARD REQUIREMENT**: Any code with hardcoded shapes will be rejected.

---

## **Context Information:**
* **Selected Facade**: `{suggested_facade}`
* **Recommended Implementation Plan**: `{train_function_plan}`
* **Dataset Description**: `{dataset_description}`
* **ConfigSpace Definition**: `{config_space}`
* **SMAC Scenario**: `{scenario}`

---

## **Function Signature** (Must be exact):

```python
from ConfigSpace import Configuration
from typing import Any, Dict, Union

def train(cfg: Configuration, dataset: Any, seed: int, budget: int = None, model_dir: str = None) -> Dict[str, Union[float, int]]:
```

## **ESSENTIAL IMPORTS** (Include these at the top):

**CRITICAL**: Always include essential imports based on your implementation. Missing imports will cause immediate failures.

### **Core Libraries** (ALWAYS Include These):
```python
import numpy as np
import os  # CRITICAL: Required for model saving, directory operations
from ConfigSpace import Configuration
from typing import Dict, Union, Any
```

### **Framework-Specific Imports** (Include based on your choice):
```python
# For sklearn: 
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import accuracy_score, f1_score, etc.

# For XGBoost: 
import xgboost as xgb

# For LightGBM:
import lightgbm as lgb

# For CatBoost:
import catboost as cb

# For PyTorch: 
import torch
import torch.nn as nn
import torch.optim as optim

# For TensorFlow: 
import tensorflow as tf
from tensorflow import keras
```

### **Framework Selection Guidelines:**

**CRITICAL**: Choose the most appropriate framework based on dataset characteristics, not predefined preferences:

### **Available Frameworks** (choose based on dataset needs):

* **Scikit-learn**: Traditional ML algorithms, good baseline, fast for small-medium datasets
* **XGBoost**: Excellent for tabular data, handles missing values, often top performer
* **LightGBM**: Fast gradient boosting, good for large datasets, memory efficient
* **CatBoost**: Handles categorical features natively, robust gradient boosting
* **PyTorch**: Deep learning, neural networks, when data complexity requires it
* **TensorFlow/Keras**: Deep learning, neural networks, established for complex patterns
* **Other specialized libraries**: Use when they provide clear advantages for the specific problem

### **Framework Selection Criteria:**

* **Dataset Size**: Small (<10K) → sklearn/XGBoost; Large (>100K) → LightGBM/neural networks
* **Data Complexity**: Simple → traditional ML; Complex patterns → neural networks
* **Feature Types**: Mixed types → CatBoost; Categorical heavy → specialized handling
* **Performance Requirements**: High accuracy needed → ensemble methods, deep learning
* **Training Time**: Fast needed → LightGBM, sklearn; Time available → neural networks
* **Interpretability**: High → sklearn linear models; Low → any high-performance method

### **CRITICAL Import Issues to Avoid:**
* **Missing `import os`**: ALWAYS include if you save models or create directories
* **Missing `import numpy as np`**: ALWAYS include if you use any `np.` functions  
* **Missing framework imports**: Import ALL libraries you actually use in the implementation
* **Inconsistent preprocessing**: Match preprocessing to your chosen framework's expectations
* **Missing utility imports**: Include `import time`, `import pickle`, etc. as needed

### **MANDATORY Import Checklist:**
Before generating code, ensure you include:
- ✅ `import numpy as np` (if using numpy functions)
- ✅ `import os` (if saving models or creating directories) 
- ✅ Framework-specific imports (sklearn, xgboost, torch, etc.)
- ✅ Preprocessing imports for your chosen approach
- ✅ Metric calculation imports for evaluation

**CRITICAL**: The function signature **MUST** include the `budget` parameter even if not used by all facades:
- For standard optimization facades: `budget` parameter will be `None` and should be ignored
- For multi-fidelity facades: `budget` parameter will contain the fidelity budget (e.g., iterations, components)
- **Always include the budget parameter** in the signature for compatibility with all facades
- Use conditional logic to handle the budget parameter appropriately based on the facade type

---

## **Function Behavior Requirements:**

### **Data Handling**
* The function **must** handle the dataset properly:
* **CRITICAL**: The function **must** use the EXACT parameter names from the ConfigSpace definition above
  * **INSPECT** the ConfigSpace definition carefully and extract only those parameters that exist

* The function **must** accept a `dataset` dictionary with:
  * `dataset['X']`: feature matrix or input tensor
  * `dataset['y']`: label vector or label tensor
  * `dataset['X_test']`: (optional) test feature matrix - only present when separate test set is provided
  * `dataset['y_test']`: (optional) test label vector - only present when separate test set is provided

### **CRITICAL Data Access Pattern**
```python
# Safe data extraction (handles both numpy arrays and pandas DataFrames)
X = dataset['X']
y = dataset['y']

# Convert to numpy arrays if needed (avoid .values for numpy arrays)
if hasattr(X, 'values'):  # pandas DataFrame
    X = X.values
if hasattr(y, 'values'):  # pandas Series
    y = y.values

# Ensure proper numpy array format
X = np.asarray(X)
y = np.asarray(y)
```

### **Conditional Train/Test Split Behavior**
* **When separate test set is provided** (`dataset['X_test']` and `dataset['y_test']` exist):
  * Use the entire `dataset['X']` and `dataset['y']` for training
  * Do NOT perform internal train/test splits
  * The test set is already provided separately
* **When no separate test set is provided** (`dataset['X_test']` and `dataset['y_test']` are missing):
  * Perform internal train/test split on the provided dataset for validation
  * Use appropriate split ratio (e.g., 80/20 or 70/30)
  * Use the split for validation during training

### **CRITICAL Data Preprocessing Requirements**

**MANDATORY**: Always include data preprocessing. Raw data often causes algorithm failures!

#### **Essential Preprocessing Steps:**

1. **Handle Missing Values** (if present):
   * Check for NaN values with `np.isnan(X).any()`
   * Apply appropriate imputation strategy based on data characteristics

2. **Feature Scaling** (highly recommended for most algorithms):
   * **StandardScaler**: For normally distributed features (mean=0, std=1)
   * **MinMaxScaler**: For bounded features (range 0-1)
   * **RobustScaler**: For data with outliers
   * Choose based on dataset characteristics and algorithm requirements

3. **Categorical Encoding** (if needed):
   * Handle categorical features appropriately for your chosen algorithm
   * Consider algorithm's native support for categorical data

#### **Preprocessing Implementation Pattern:**
```python
# Extract and prepare data
X, y = dataset['X'], dataset['y']
# ... data access pattern from above ...

# Handle train/test split first
if 'X_test' in dataset and 'y_test' in dataset:
    X_train, y_train = X, y
    X_test = np.asarray(dataset['X_test'])
    y_test = np.asarray(dataset['y_test'])
    if hasattr(dataset['X_test'], 'values'):
        X_test = dataset['X_test'].values
    if hasattr(dataset['y_test'], 'values'):
        y_test = dataset['y_test'].values
else:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=seed, stratify=y
    )

# Apply preprocessing (CRITICAL: fit only on training data!)
# Choose scaler based on configuration or algorithm requirements
scaler_type = cfg.get('preprocessing', 'standard')  # or determine based on algorithm
if scaler_type == 'standard':
    scaler = StandardScaler()
elif scaler_type == 'minmax':
    scaler = MinMaxScaler()
# Add more preprocessing options as needed

# Fit on training data, transform both train and test
X_train_processed = scaler.fit_transform(X_train)
X_test_processed = scaler.transform(X_test)
```

### **Configuration Handling**
* The function **must** handle the configuration properly:
  * Access primitive values using `cfg.get('key')`
  * Handle all hyperparameters defined in the configuration space
  * Apply proper type conversion and validation
  * Handle conditional hyperparameters correctly
  * Parameter types may include numpy types (numpy.int64, numpy.float64, etc.)

### **Algorithm Selection Requirements**
* **Data-Driven Selection**: Choose algorithms based on dataset characteristics, not bias towards specific libraries
* **Performance-Oriented**: Prioritize methods that are likely to perform best on the given dataset type
* **Framework Agnostic**: Consider all available frameworks and choose the most appropriate
* **Consider Data Properties**: 
  * Small datasets → traditional ML may be sufficient
  * Large datasets → gradient boosting or neural networks may excel
  * High-dimensional → specialized techniques may be needed
  * Complex patterns → deep learning may be justified
* **Computational Constraints**: Balance performance potential with available resources
* **Task-Specific**: Choose methods proven effective for the specific task type (classification, regression, etc.)

### **Training Requirements**
* Implement proper training procedures for the selected algorithm
* Handle batch processing efficiently when needed
* Apply appropriate optimization strategies
* Implement early stopping if beneficial for the algorithm
* Handle validation appropriately
* **Prevent Overfitting**: Use proper validation, regularization, early stopping
* **Ensure Generalization**: Avoid fitting too closely to training data

### **Neural Network Training Requirements**
* **CRITICAL EPOCH HANDLING**: For PyTorch/TensorFlow neural networks:
  * **NEVER** hardcode epochs to 10 or any fixed value
  * **MUST** extract epochs from configuration: `epochs = int(cfg.get('epochs', 100))`
  * **MUST** apply budget constraints when available: `epochs = min(int(budget), epochs)`
  * **MUST** make epochs a tunable hyperparameter from ConfigSpace
  * **NEVER** use `epochs = 10` or similar hardcoded values
  * **BUDGET INTELLIGENT DECISION**: The LLM should decide how to use budget based on context:
  * **If budget seems like epochs**: Use `epochs = int(budget)` (common in multi-fidelity)
  * **If budget seems like iterations**: Use `max_iter = int(budget)` (for iterative algorithms)
  * **If budget seems like ensemble size**: Use `n_estimators = int(budget)` (for ensemble methods)
  * **If budget seems like data fraction**: Use budget to sample training data
  * **CRITICAL**: Make intelligent decision based on algorithm type and context
* **Device Handling**: Always use appropriate device (CPU/GPU) detection
* **Early Stopping**: Implement early stopping with configurable patience
* **Validation**: Use validation data for early stopping and model selection

### **Model Saving Requirements**
* **STRONGLY RECOMMENDED**: Save the trained model if `model_dir` is provided
* This enables proper model evaluation with comprehensive metrics
* Create the directory if it doesn't exist
* Save model in appropriate format for the chosen framework
* Generate unique model filename using timestamp or configuration hash
* Put the save model code section in a try/except to prevent it from possible errors

### **Return Format Requirements**
* **CRITICAL**: Return a dictionary with 'loss' key and evaluation metrics
* **REASON**: Loss is used for hyperparameter optimization, metrics provide comprehensive evaluation
* **Metric Calculation**: Use validation data if available, otherwise use training data
* **Consistent Format**: Always return the same dictionary keys for the task type
* **CRITICAL**: Dictionary must always include `loss` key for hyperparameter optimization

---

## **Performance and Code Quality Requirements:**

### **Performance Optimization**
* Minimize memory usage and allocations
* Use vectorized operations where possible
* Avoid unnecessary data copying
* Optimize data loading and preprocessing
* Use efficient data structures
* Implement efficient processing for the chosen algorithm
* **Resource Management**: Use computational resources efficiently based on dataset size

### **Code Quality**
* Keep code minimal and focused
* Avoid redundant computations
* Use efficient algorithms
* Minimize function calls
* Optimize loops and iterations
* Use appropriate data types
* Avoid unnecessary object creation
* The train function should be computationally efficient

### **Best Practices**
* Implement proper logging for progress tracking
* Handle edge cases gracefully
* Ensure reproducibility with proper random seed usage
* Optimize performance for the dataset characteristics
* Follow framework-specific best practices for the chosen implementation

---

## **Framework Selection Guidelines:**

**CRITICAL**: Choose frameworks based on what will achieve the best performance for your specific dataset:

### **Dataset-Driven Framework Selection:**
* **Small Datasets (< 10K samples)**: Traditional ML (sklearn) often sufficient, but consider XGBoost for better performance
* **Medium Datasets (10K-100K)**: XGBoost, LightGBM, CatBoost often excel; neural networks if complexity warrants
* **Large Datasets (> 100K)**: LightGBM, neural networks (PyTorch/TensorFlow), distributed methods
* **High-Dimensional Data**: Specialized techniques, potentially neural networks with proper regularization
* **Categorical-Heavy Data**: CatBoost, XGBoost with categorical handling, or appropriate preprocessing + any framework

### **Performance-First Approach:**
* **Prioritize proven high-performers**: XGBoost, LightGBM often outperform sklearn on tabular data
* **Consider neural networks**: For complex patterns, large datasets, or when traditional ML plateaus
* **Don't default to sklearn**: Use it when appropriate, not as a default choice
* **Leverage specialized libraries**: CatBoost for categorical data, PyTorch for custom architectures

### **Implementation Considerations:**
* **Available compute**: for PyTorch/TensorFlow use GPU if available
* **Memory constraints**: Large datasets may need memory-efficient approaches (LightGBM, streaming)
* **Training time**: Fast needed → LightGBM; time available → explore neural networks
* **Deployment requirements**: Consider inference speed and model size for production

---

## **Code Generation Best Practices:**

### **Parameter Handling**
* **VALIDATE ALL INPUTS EXPLICITLY** - Check types, shapes, ranges before use
* **HANDLE MISSING PARAMETERS** - Use sensible defaults for all hyperparameters
* **CHECK DATA TYPES AND SHAPES** - Validate and convert before processing  
* **CONVERT TYPES SAFELY** - Convert budget and other parameters to appropriate types
* **BOUND ALL PARAMETERS** - Ensure parameters are within valid ranges
* **USE SENSIBLE DEFAULTS** - Provide reasonable defaults for all hyperparameters
* **HANDLE NUMPY TYPES** - Convert numpy.int64, numpy.float64 to native Python types using str() or int() conversion
* **CRITICAL EPOCH PARAMETER**: For neural networks, NEVER hardcode epochs to 10
  * **MUST** extract from config: `epochs = int(cfg.get('epochs', 100))`
  * **MUST** apply budget constraints when available
  * **NEVER** use `epochs = 10` or any fixed value
  * **BUDGET INTELLIGENT DECISION**: The LLM should decide how to use budget based on context:
  * **If budget seems like epochs**: Use `epochs = int(budget)` (common in multi-fidelity)
  * **If budget seems like iterations**: Use `max_iter = int(budget)` (for iterative algorithms)
  * **If budget seems like ensemble size**: Use `n_estimators = int(budget)` (for ensemble methods)
  * **If budget seems like data fraction**: Use budget to sample training data
  * **CRITICAL**: Make intelligent decision based on algorithm type and context

### **Algorithm Implementation**
* **IMPLEMENT MATCHING ALGORITHMS** - Implement algorithms that match the configuration space
* **USE EXACT PARAMETER NAMES** - Extract parameters using exact names from ConfigSpace definition
* **PREVENT OVERFITTING** - Use appropriate validation, regularization, and early stopping
* **DYNAMIC DIMENSIONS** - Infer all dimensions from data, never hardcode shapes
* **ALGORITHM SELECTION** - Base algorithm choice on dataset characteristics, not predefined preferences

### **Error Prevention**
* **Convert numpy string types**: Use `str(cfg.get('param'))` to convert np.str_ to native string
* **Validate categorical parameters** against allowed values before using them
* **Use parameter mapping** when framework names differ from configuration names
* **Handle multi-class problems** appropriately for the chosen algorithms
* **Validate solver and activation parameters** for neural network algorithms

### **CRITICAL ERROR PREVENTION PATTERNS:**

**MUST implement these patterns to prevent common HPO failures:**

#### **Safe Parameter Extraction Pattern:**
```python
# Always convert numpy types to native Python types
def safe_extract(cfg, param_name, default_value, target_type=None):
    value = cfg.get(param_name, default_value)
    
    # Handle numpy types
    if hasattr(value, 'dtype'):
        if 'str' in str(value.dtype):
            value = str(value)
        elif 'int' in str(value.dtype):
            value = int(value)
        elif 'float' in str(value.dtype):
            value = float(value)
    
    # Apply type conversion if specified
    if target_type is not None:
        value = target_type(value)
    
    return value
```

#### **CRITICAL Parameter Type Conversion Patterns:**

**MANDATORY**: Use these patterns to prevent type-related errors:

```python
# For INTEGER parameters (max_depth, n_estimators, etc.)
max_depth = cfg.get('max_depth')
if max_depth is not None:
    max_depth = int(max_depth)  # Convert float to int

# For FLOAT parameters (learning_rate, alpha, etc.)  
learning_rate = float(cfg.get('learning_rate', 0.01))

# For STRING parameters (algorithm, solver, etc.)
algorithm = str(cfg.get('algorithm', 'random_forest'))
solver = str(cfg.get('solver', 'lbfgs'))

# For BOOLEAN parameters
fit_intercept = bool(cfg.get('fit_intercept', True))

# For CATEGORICAL parameters with validation
activation = str(cfg.get('activation', 'relu'))
valid_activations = ['relu', 'tanh', 'logistic', 'identity']
if activation not in valid_activations:
    activation = 'relu'  # Fallback to default

# For OPTIONAL INTEGER parameters (can be None)
max_depth = cfg.get('max_depth')
if max_depth is not None:
    max_depth = int(max_depth)  # Only convert if not None
# max_depth can now be either an int or None, which is correct for sklearn

# CRITICAL: For neural network epochs (NEVER hardcode to 10!)
epochs = int(cfg.get('epochs', 100))  # Extract from config, use reasonable default
if budget is not None:
    # The LLM should intelligently decide how to use budget:
    # - If using neural networks: epochs = int(budget)  # Budget controls training epochs
    # - If using ensemble methods: n_estimators = int(budget)  # Budget controls ensemble size
    # - If using iterative methods: max_iter = int(budget)  # Budget controls iterations
    # - If using data sampling: sample_size = int(budget * len(X_train))  # Budget controls data fraction
    epochs = int(budget)  # Example for neural networks - adapt based on algorithm choice
```

#### **Parameter Validation Principles:**
* **Multi-class Classification**: Always specify multi-class handling strategy when required
* **Categorical Parameters**: Validate against allowed values and provide mappings for invalid values
* **Numeric Parameters**: Ensure they are within valid ranges for the algorithm
* **String Parameters**: Convert all string parameters to native Python strings

#### **Robust Metric Calculation Pattern:**
```python
# Handle multi-class metrics safely
try:
    # Adapt metric calculation based on problem type and model capabilities
    # Use appropriate averaging strategies
    # Handle edge cases with fallback values
except:
    # Provide sensible fallback values for failed metric calculations
    pass
```

---

## **Budget Parameter Handling Principles:**

**CRITICAL**: The budget parameter controls fidelity for multi-fidelity facades. Handle it appropriately!

### **Budget Parameter Guidelines:**

* **For Multi-Fidelity Facades**: Budget parameter contains a numeric value that should control algorithm fidelity
* **For Full Evaluation Facades**: Budget parameter will be `None` and should be ignored
* **Budget Application**: Use budget to override fidelity-related hyperparameters in the chosen algorithm
* **Type Safety**: Always convert budget to appropriate type (usually `int`) before use

### **Budget Handling by Facade Type:**

* **Multi-Fidelity Facades** (e.g., Hyperband, BOHB):
  * **Budget controls fidelity**: Higher budget = more computational resources
  * **Budget can be epochs**: `epochs = int(budget)` (common for neural networks)
  * **Budget can be ensemble size**: `n_estimators = int(budget)` (for ensemble methods)
  * **Budget can be iterations**: `max_iter = int(budget)` (for iterative algorithms)
  * **Budget can be data fraction**: `sample_size = int(budget * len(X_train))` (for data sampling)

* **Standard Optimization Facades** (e.g., SMAC, Random Search):
  * **Budget may be None**: Use configuration parameters when budget is None
  * **Budget may constrain parameters**: Use budget as upper bound when provided
  * **Budget may control fidelity**: Use budget to control computational cost

* **CRITICAL**: The LLM should intelligently decide how to use budget based on:
  * Algorithm type (neural networks, ensemble methods, iterative methods)
  * Dataset characteristics (size, complexity)
  * Configuration space parameters
  * Context clues from the optimization scenario

### **Common Fidelity Parameters by Algorithm Type:**

* **Ensemble Algorithms**: Use budget for number of estimators/components
* **Iterative Algorithms**: Use budget for maximum iterations/epochs  
* **Neural Networks**: Use budget for training epochs or iterations
  * **CRITICAL**: NEVER hardcode epochs to 10 or any fixed value
  * **MUST** extract epochs from configuration and apply budget constraints
  * **Example**: `epochs = min(int(budget), int(cfg.get('epochs', 100)))` if budget available
  * **BUDGET INTELLIGENT DECISION**: The LLM should decide how to use budget based on context:
  * **If budget seems like epochs**: Use `epochs = int(budget)` (common in multi-fidelity)
  * **If budget seems like iterations**: Use `max_iter = int(budget)` (for iterative algorithms)
  * **If budget seems like ensemble size**: Use `n_estimators = int(budget)` (for ensemble methods)
  * **If budget seems like data fraction**: Use budget to sample training data
  * **CRITICAL**: Make intelligent decision based on algorithm type and context
* **Data-Based Fidelity**: Use budget for fraction of training data

### **Intelligent Budget Decision Making:**

**CRITICAL**: The LLM should intelligently decide how to use the budget parameter based on context:

### **Budget Decision Framework:**

**MANDATORY**: Analyze the context and make intelligent decisions about budget usage:

1. **Algorithm Type Analysis**:
   * **Neural Networks**: Budget likely controls epochs (`epochs = int(budget)`)
   * **Ensemble Methods**: Budget likely controls ensemble size (`n_estimators = int(budget)`)
   * **Iterative Algorithms**: Budget likely controls iterations (`max_iter = int(budget)`)
   * **Data Sampling**: Budget likely controls data fraction (`sample_size = int(budget * len(X_train))`)

2. **Context Clues**:
   * **Multi-fidelity optimization**: Budget directly controls fidelity parameter
   * **Standard optimization**: Budget may constrain or control parameters
   * **Dataset size**: Large datasets may suggest data sampling
   * **Configuration space**: Parameters in config suggest appropriate budget usage

3. **Intelligent Decision Making**:
   * **Choose the most appropriate fidelity parameter** for the selected algorithm
   * **Consider computational cost** - what parameter controls training time?
   * **Consider algorithm characteristics** - what parameter affects performance most?
   * **Adapt to the specific context** - don't assume budget is always epochs

### **General Budget Application Principles:**

**CRITICAL**: The budget parameter controls algorithm fidelity. Apply it based on what parameter controls computational cost in your chosen algorithm:

* **Identify the fidelity parameter**: Determine which hyperparameter controls computational cost/training time
* **Apply budget appropriately**: Use budget to limit or scale that parameter
* **Ensure reasonable bounds**: Don't let budget create invalid parameter values
* **Scale budget intelligently**: Different algorithms may need different scaling factors

### **Budget Parameter Categories:**

* **Ensemble Size Control**: For algorithms that build multiple models, use budget to control ensemble size
* **Iteration Control**: For iterative algorithms, use budget to control maximum iterations or epochs  
* **Early Stopping Control**: For algorithms with early stopping, use budget to control patience or validation rounds
* **Component Control**: For algorithms with multiple components, use budget to control number of components

### **CRITICAL Budget Implementation Pattern:**

```python
def train(cfg: Configuration, dataset: Any, seed: int, budget: int = None, model_dir: str = None) -> Dict[str, Union[float, int]]:
    # ... data extraction and algorithm selection based on dataset characteristics ...
    
    # CRITICAL: Apply budget BEFORE creating the model
    if budget is not None:
        budget = int(budget)  # Ensure integer type
        
        # Identify the appropriate fidelity parameter for your chosen algorithm
        # This depends on what algorithm you selected based on dataset characteristics
        
        # Apply budget to the parameter that controls computational cost:
        # - If your algorithm uses n_estimators: limit it with budget
        # - If your algorithm uses max_iter: scale budget appropriately  
        # - If your algorithm uses epochs: use budget to control training length
        # - Ensure the budget application makes sense for your specific algorithm
        
        # Example patterns (adapt to your chosen algorithm):
        # The LLM should intelligently decide how to use budget based on context:
        # For ensemble methods: n_estimators = int(budget)  # Budget controls ensemble size
        # For neural networks: epochs = int(budget)  # Budget controls training epochs
        # For iterative methods: max_iter = int(budget)  # Budget controls iterations
        # For data sampling: sample_size = int(budget * len(X_train))  # Budget controls data fraction
        # CRITICAL: Make intelligent decision based on algorithm type and context
    
    # Create and train your model with budget-adjusted parameters
    # ... rest of training logic ...
```

### **CRITICAL Preprocessing Pattern (Prevents Scaler Errors):**

**MANDATORY**: Always initialize scaler to prevent "referenced before assignment" errors:

```python
# CRITICAL: Initialize scaler variable BEFORE conditional blocks
scaler = None

# Extract and prepare data
X, y = dataset['X'], dataset['y']
# ... safe data access pattern ...

# Handle train/test split
if 'X_test' in dataset and 'y_test' in dataset:
    X_train, y_train = X, y
    X_test = np.asarray(dataset['X_test'])
    y_test = np.asarray(dataset['y_test'])
    # ... handle test data conversion ...
else:
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=seed, stratify=y
    )

# CRITICAL: Always initialize and apply appropriate preprocessing for your chosen framework
preprocessing_scaler = str(cfg.get('preprocessing_scaler', 'standard'))

if preprocessing_scaler == 'standard':
    scaler = StandardScaler()
elif preprocessing_scaler == 'minmax':
    scaler = MinMaxScaler()
elif preprocessing_scaler == 'robust':
    from sklearn.preprocessing import RobustScaler
    scaler = RobustScaler()
else:
    # Choose appropriate default based on your framework and algorithm
    scaler = StandardScaler()

# Apply preprocessing (adapt to your chosen framework's requirements)
X_train_processed = scaler.fit_transform(X_train)
X_test_processed = scaler.transform(X_test)
```

---

## **General Structure Principles:**

**Your generated function should follow these principles:**

1. **Set random seed** for reproducibility
2. **Extract hyperparameters** from cfg with appropriate defaults and type conversion
3. **Validate and bound** all parameters to ensure they're within valid ranges
4. **Prepare data** efficiently, inferring dimensions dynamically
5. **Select and implement algorithm** based on configuration space and dataset characteristics
6. **Handle budget parameter** appropriately for the facade type
7. **Train the model** with the validated parameters
8. **Save the model** if model_dir is provided
9. **Return dictionary with loss and metrics**

**Do NOT copy specific implementation patterns** - generate code appropriate for the given configuration space and dataset requirements.

---

## **Output Format:**

* Return **only** the `train()` function
* Include necessary imports
* No example usage or additional code
* The function must be self-contained and executable
* Code must be minimal and optimized for performance

---

## **FINAL REMINDER - CRITICAL REQUIREMENTS:**
* **COMPLETE `import` statements** - Include ALL libraries you use (numpy, os, framework-specific, etc.)
* A single `train()` function that returns **a dictionary with loss and metrics**
* No additional code or explanations
* Code must be optimized for performance and minimal in size
* For accuracy metrics, use positive values (e.g. accuracy: 0.85)
* For error metrics, use raw error values (e.g. mse: 0.15)
* Ensure consistent sign convention across all metrics
* **STRONGLY RECOMMENDED**: Save the trained model if `model_dir` parameter is provided
* **MUST handle multi-algorithm configuration spaces** by implementing appropriate algorithms
* **MUST use EXACT parameter names** from ConfigSpace definition
* **MUST select algorithms** based on dataset characteristics and configuration parameters
* **MUST convert budget parameter** to appropriate type and validate range
* **MUST use sensible defaults** for all hyperparameters with proper type conversion
* **MUST validate parameter ranges** to ensure they're within acceptable bounds
* **MUST prevent overfitting** with proper validation and regularization
* **NO try/except blocks** - let errors propagate so they can be fixed
* **The train function MUST be naturally robust** for hyperparameter optimization scenarios
* **CRITICAL**: **Handle train/test split conditionally**
  * **When `X_test` and `y_test` are in dataset**: Use entire training set, do NOT split
  * **When `X_test` and `y_test` are NOT in dataset**: Perform internal train/test split for validation
* **Dataset Usage**: Check for separate test set availability and adapt accordingly
* **Metric Calculation**: Always calculate metrics on validation/test data, not training data
* **Return Format**: Dictionary with consistent keys based on task type (classification/regression)
* **CRITICAL**: Dictionary must always include `loss` key for hyperparameter optimization
* **Framework and Algorithm Selection**: Choose best framework and algorithms for dataset characteristics, not library bias
* **CRITICAL** When you use pytorch or tensorflow it is very important to use device and cuda
* **CRITICAL EPOCH HANDLING**: For neural networks, NEVER hardcode epochs to a fixed value like 10
  * **MUST** extract epochs from configuration: `epochs = int(cfg.get('epochs', default_value))`
  * **MUST** use budget parameter when available: `epochs = min(int(budget), int(cfg.get('epochs', default_value)))`
  * **MUST** allow epochs to be tunable hyperparameter from ConfigSpace
  * **NEVER** use `epochs = 10` or any other hardcoded value
  * **NEVER** set epochs constant - it must be configurable for hyperparameter optimization
  * **BUDGET INTELLIGENT DECISION**: The LLM should decide how to use budget based on context:
  * **If budget seems like epochs**: Use `epochs = int(budget)` (common in multi-fidelity)
  * **If budget seems like iterations**: Use `max_iter = int(budget)` (for iterative algorithms)
  * **If budget seems like ensemble size**: Use `n_estimators = int(budget)` (for ensemble methods)
  * **If budget seems like data fraction**: Use budget to sample training data
  * **CRITICAL**: Make intelligent decision based on algorithm type and context
* Only generate the train function nothing else