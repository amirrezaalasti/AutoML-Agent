# Machine Learning Planner Prompt

You are an expert machine learning planner. Your task is to analyze the given dataset and task, then generate three precise step-by-step implementation guides.

---

## Dataset Information

- **Name:** `{dataset_name}`
- **Type:** `{dataset_type}`
- **Description:** `{dataset_description}`
- **Task:** `{task_type}`
- **SMAC Documentation:** `{smac_documentation}`
- **OpenML Meta-Learning Insights:**
  ```
  {config_space_suggested_parameters}
  ```

---

## Output Format

Generate exactly three implementation guides and return them in the following JSON format:

```json
{{
"configuration_plan": "Step 1: [specific action]\nStep 2: [specific action]\n...",
"scenario_plan": "Step 1: [specific action]\nStep 2: [specific action]\n...",
"train_function_plan": "Step 1: [specific action]\nStep 2: [specific action]\n...",
"suggested_facade": "Only the Facade type name"
}}
```

-----

## Configuration Plan Requirements

  - Provide **6-8 numbered steps** that detail how to create a configuration space for hyperparameter optimization.
  - Include **specific parameter names, types, and ranges** appropriate for `{task_type}` and `{dataset_type}`.
  - Each step should be a **direct implementation action**.

### Guidelines

  - **IMPORTANT:** When OpenML meta-learning insights are available, use them to inform hyperparameter ranges and algorithm selection based on similar datasets.
  - **CRITICAL:** Let the dataset characteristics drive framework and model selection. Consider all available frameworks:
      - **Dataset size**: Small → sklearn/XGBoost; Medium → XGBoost/LightGBM; Large → LightGBM/neural networks
      - **Data complexity**: Simple → traditional ML; Complex patterns → neural networks (PyTorch/TensorFlow)
      - **Feature types**: Categorical-heavy → CatBoost; Mixed → XGBoost/LightGBM; High-dim → specialized approaches
      - **Performance requirements**: High accuracy → gradient boosting, neural networks, ensembles
      - **Resource constraints**: Limited → sklearn/LightGBM; Abundant → PyTorch/TensorFlow
  - Use appropriate parameter types (`Categorical`, `Integer`, `Float`) based on the nature of each hyperparameter.
  - Specify realistic ranges considering dataset characteristics and computational constraints.
  - Include algorithm-specific hyperparameters relevant to the chosen approach.
  - Balance exploration breadth with computational feasibility.

-----

## Scenario Plan Requirements

  - Provide **6-8 numbered steps** that detail how to set up a SMAC scenario for optimization.
  - Include **specific values** for trial limits, time limits, worker configuration, and facade selection based on the `{dataset_description}`.
  - Each step should specify **exact settings to use**.

### Guidelines

  - **CRITICAL:** The scenario MUST be compatible with the selected facade:
      - **For Multi-Fidelity Facades** (HyperbandFacade, MultiFidelityFacade): **MUST** set both `min_budget`, `max_budget`, and `eta` parameters based on the chosen fidelity parameter and dataset characteristics.
      - **For Full Evaluation Facades** (BlackBoxFacade, HyperparameterOptimizationFacade): **Do NOT** set budget parameters as they perform complete evaluations.
  - Set appropriate `n_trials` based on:
    - Dataset complexity and size
    - Configuration space dimensionality
    - Available computational budget
    - Expected evaluation time per trial
  - Define realistic time limits considering:
    - Expected runtime per model evaluation
    - Total available optimization time
    - Dataset size and model complexity
  - Configure output directory and logging for experiment tracking
  - Set `deterministic` to `True` for reproducible results
  - Specify appropriate crash handling and timeout parameters
  - Consider memory constraints and parallel execution capabilities
  - Balance exploration (more trials) with exploitation (longer evaluation time)

-----

## Train Function Plan Requirements

  - Provide **10-12 numbered steps** that detail how to implement the training function.
  - Include data handling, model initialization, training process, evaluation, and model saving.
  - Each step should be a **concrete implementation action**.

### Guidelines

  - **CRITICAL:** The train function MUST be compatible with the selected facade.
  - The function signature **MUST** be: `def train(cfg: Configuration, dataset: Any, seed: int, budget: int = None, model_dir: str = None) -> Dict[str, Union[float, int]]:`
  - **For Multi-Fidelity Facades** (HyperbandFacade, MultiFidelityFacade): 
    - The `budget` parameter contains the fidelity value from the optimizer
    - Use it to control algorithm-specific parameters (e.g., `n_estimators`, `epochs`, `max_iter`)
    - Enable early stopping or partial evaluation based on the budget
  - **For Full Evaluation Facades** (BlackBoxFacade, HyperparameterOptimizationFacade): 
    - The `budget` parameter will be `None` or can be ignored
    - Perform complete model training and evaluation
  - Handle data loading and preprocessing efficiently
  - Implement proper train/validation/test splits based on dataset availability
  - Use cross-validation when appropriate for robust evaluation
  - Return a dictionary with 'loss' key (for optimization) and additional metrics
  - **Framework Selection**: Choose the best framework for the dataset (sklearn, XGBoost, LightGBM, CatBoost, PyTorch, TensorFlow)
  - **Performance-Driven**: Prioritize methods likely to achieve best results, not just familiar libraries
  - **Data-Specific**: Match algorithm choice to dataset characteristics (size, complexity, feature types)
  - Consider model complexity, training time, and expected performance for the given dataset and available resources

-----

## SMAC Facade Selection Guidelines

Choose the most appropriate SMAC facade based on your analysis of the dataset characteristics, computational constraints, and algorithm requirements. Below are descriptions of each facade to help inform your decision:

### Available Facades

#### **BlackBoxFacade**
- **Purpose**: Simple black-box optimization using Gaussian Processes
- **Best for**: 
  - Small to medium datasets (< 10,000 samples)
  - Simple search spaces with fewer hyperparameters
  - Quick optimization when computational budget is limited
  - Algorithms without natural fidelity parameters
- **Characteristics**: 
  - Uses full model evaluations for each trial
  - Efficient for smaller search spaces
  - Good baseline optimization approach
- **Trial Budget**: Typically 30-75 trials

#### **HyperparameterOptimizationFacade**
- **Purpose**: Standard hyperparameter optimization with Gaussian Processes
- **Best for**:
  - Medium datasets (5,000-50,000 samples)
  - Standard machine learning algorithms
  - Balanced exploration-exploitation needs
  - When full model evaluations are feasible
- **Characteristics**:
  - More sophisticated than BlackBox for medium-scale problems
  - Good for most traditional ML algorithms
  - Reliable and well-tested approach
- **Trial Budget**: Typically 50-150 trials

#### **HyperbandFacade**
- **Purpose**: Multi-fidelity optimization using successive halving
- **Best for**:
  - Algorithms with natural fidelity parameters (n_estimators, max_iter, epochs)
  - Medium to large datasets (> 10,000 samples)
  - When early stopping can provide meaningful partial evaluations
  - Tree ensembles, iterative algorithms
- **Characteristics**:
  - Uses budget parameters for partial evaluations
  - Can explore more configurations efficiently
  - Requires algorithms that can be meaningfully evaluated with reduced fidelity
- **Trial Budget**: Typically 100-250 trials
- **Fidelity Examples**: n_estimators for trees, max_iter for iterative algorithms

#### **MultiFidelityFacade**
- **Purpose**: Advanced multi-fidelity optimization
- **Best for**:
  - Large datasets (> 50,000 samples)
  - Deep learning or computationally expensive algorithms
  - When training time varies significantly with fidelity
  - Neural networks with epoch-based training
- **Characteristics**:
  - Most sophisticated multi-fidelity approach
  - Best for expensive evaluations
  - Requires careful fidelity parameter selection
- **Trial Budget**: Typically 100-300 trials
- **Fidelity Examples**: epochs for neural networks, training data subsets

### Facade Selection Considerations

**Dataset Size Impact:**
- Small datasets may benefit from full evaluations (BlackBox, HyperparameterOptimization)
- Large datasets often benefit from multi-fidelity approaches (Hyperband, MultiFidelity)

**Algorithm Characteristics:**
- Iterative algorithms (ensembles, neural networks) work well with multi-fidelity
- Non-iterative algorithms (SVM, k-NN) typically need full evaluation facades

**Computational Budget:**
- Limited time/resources: BlackBox or HyperparameterOptimization
- Abundant resources: Hyperband or MultiFidelity

**Search Space Complexity:**
- Simple spaces: BlackBox or HyperparameterOptimization
- Complex spaces: Hyperband or MultiFidelity

### Budget Parameter Guidelines (for Multi-Fidelity Facades)

When using HyperbandFacade or MultiFidelityFacade, you must define budget parameters carefully as they directly impact optimization efficiency:

**CRITICAL Budget Parameter Selection:**
- **min_budget**: Should allow meaningful partial evaluation but be computationally cheap
- **max_budget**: Should provide near-optimal model performance
- **Budget ratio**: Aim for max_budget/min_budget ratio between 4-20 for effective successive halving
- **eta**: Controls successive halving aggressiveness (recommend values: 2, 3, 4, or 5)

**ETA Parameter Selection:**
The eta parameter is crucial for multi-fidelity optimization efficiency. You must choose eta based on dataset and algorithm characteristics:

**Eta Options:**
- **eta = 2**: Most conservative, gives configurations many chances (best for small datasets or similar-performing configurations)
- **eta = 3**: Balanced approach, good default choice for most scenarios
- **eta = 4**: More aggressive elimination (good for larger datasets with clear performance differences)
- **eta = 5**: Most aggressive, fastest optimization (best when bad configurations are easily identifiable)

**Selection Criteria:**
1. **Dataset Size**: Larger datasets can handle higher eta values (more aggressive elimination)
2. **Algorithm Characteristics**: Fast-converging algorithms benefit from higher eta
3. **Performance Variation**: If configurations vary widely in performance, use higher eta
4. **Time Constraints**: Higher eta values complete optimization faster
5. **Exploration vs Exploitation**: Lower eta = more exploration, higher eta = more exploitation

**Common Fidelity Parameters:**
- **Tree Ensembles**: n_estimators 
  - Conservative: min_budget=10, max_budget=100
  - Aggressive: min_budget=20, max_budget=200
- **Neural Networks**: epochs
  - Small datasets: min_budget=5, max_budget=50
  - Large datasets: min_budget=10, max_budget=100
- **Iterative Classifiers**: max_iter
  - Fast convergence: min_budget=50, max_budget=500
  - Slow convergence: min_budget=100, max_budget=1000
- **Data Subsampling**: fraction of training data (e.g., min_budget=0.1, max_budget=1.0)

**Budget Selection Strategy:**
1. Consider algorithm convergence characteristics
2. Balance exploration (lower min_budget) vs exploitation (higher min_budget)
3. Ensure max_budget provides adequate model quality
4. Account for dataset size and computational constraints

**Important**: The `n_trials` parameter will be optimized automatically by the system during scenario generation based on your budget parameters and dataset characteristics.

