**Generate a production-grade Python configuration space for machine learning hyperparameter optimization with the following STRICT requirements:**

---

### **Function signature** must be:

```python
from ConfigSpace import ConfigurationSpace, UniformFloatHyperparameter, UniformIntegerHyperparameter, CategoricalHyperparameter
def get_configspace() -> ConfigurationSpace:
```

---

### **Configuration Space Requirements:**

* The configuration space **must** be appropriate for the dataset type and characteristics:
  * Dataset Description: `{dataset_description}`

* Recommended Configuration based on the planner:
  * `{recommended_configuration}`

* The configuration space **must** include:
  * Hyperparameter ranges that match the dataset characteristics and computational constraints
  * Reasonable default values based on dataset properties
  * Appropriate hyperparameter types (continuous, discrete, categorical)
  * Conditional hyperparameters when algorithm dependencies exist
  * Proper bounds and constraints that ensure valid configurations

* **Data-Driven Design Principles:**
  * Analyze dataset size, dimensionality, and complexity to determine appropriate algorithms
  * Consider feature types (numerical, categorical, text) when selecting preprocessing and algorithms
  * Factor in computational constraints and time requirements
  * Use dataset characteristics to inform parameter ranges and defaults
  * Ensure the configuration space enables exploration of promising algorithm families

* **Configuration Space Best Practices:**
  * Use meaningful and descriptive hyperparameter names
  * Include comprehensive documentation for each hyperparameter
  * Set parameter ranges that balance exploration with computational feasibility
  * Ensure ranges are neither too narrow (limiting exploration) nor too wide (inefficient search)
  * Add proper conditional dependencies between related hyperparameters
  * Consider parameter interactions and their impact on model performance

* **Algorithmic Considerations:**
  * Select algorithms based on dataset characteristics rather than predefined preferences
  * Include preprocessing options relevant to the data types present
  * Consider ensemble methods when dataset complexity warrants them
  * Include regularization options appropriate for the dataset size and dimensionality
  * Ensure scalability considerations match dataset size and computational resources

---

### **Output Format:**

* Return **only** the `get_configspace()` function
* Include necessary imports
* No example usage or additional code
* The function must be self-contained and executable

---

### **Error Prevention:**

* Ensure all hyperparameter names are valid Python identifiers
* Verify that all ranges and bounds are mathematically valid
* Check that conditional hyperparameters are properly defined with correct parent-child relationships
* Validate that default values fall within the specified ranges
* Ensure parameter combinations will not result in invalid configurations

---

### **CRITICAL Parameter Naming Conventions:**

**MANDATORY**: Use consistent, clear parameter names that match exactly between config space and train function:

* **Use underscores for multi-word parameters**: `max_depth`, `min_samples_split`, `learning_rate`
* **Use framework-appropriate parameter names**: Match exactly with chosen framework's parameter names (sklearn, XGBoost, PyTorch, etc.)
* **Avoid ambiguous abbreviations**: Use `preprocessing_type` instead of `prep_type`
* **Be explicit about algorithm selection**: Use `algorithm` or `classifier_type` for algorithm selection
* **Example good names**:
  - `preprocessing_scaler` (not `prep`, `scaler_type`, or `scaling`)
  - `max_depth` (not `maxdepth` or `max_d`)
  - `n_estimators` (not `nestimators` or `num_trees`)
  - `learning_rate` (not `lr` or `learn_rate`)

**REMEMBER**: The train function must use the EXACT same parameter names you define here!

---

### **Example Structure:**

```python
def get_configspace() -> ConfigurationSpace:
    cs = ConfigurationSpace()
    
    # Add framework selection based on dataset characteristics
    # Consider: sklearn, XGBoost, LightGBM, CatBoost, PyTorch, TensorFlow
    
    # Add framework-specific hyperparameters with conditional dependencies
    
    # Add preprocessing hyperparameters relevant to chosen framework and data types
    
    # Add optimization hyperparameters appropriate for selected methods
    
    return cs
```

---

**Reminder:** The output must be limited to:
* Valid `import` statements
* A single `get_configspace()` function that returns a properly configured `ConfigurationSpace` object
* No additional code or explanations
* **Base framework and algorithm selection on dataset characteristics, not library bias**
* **Consider all available frameworks (sklearn, XGBoost, LightGBM, CatBoost, PyTorch, TensorFlow) based on data needs**
* **Design parameter ranges that are appropriate for the specific dataset properties and chosen methods**